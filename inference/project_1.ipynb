{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacktahon project 1: Query categorisation\n",
    "\n",
    "In this project you will use a transformer model in order to classify queries from the UK IKEA website. For this purpose we have provided a list with the 10 000 most common queries, a transformer model as well as some sample code.\n",
    "\n",
    "As you experiment with the model and do the classification you will quickly find that how the classification is done will be very dependent on _what_ you actually ask the transformer to do and _how_ you ask it to do so. \n",
    "\n",
    "Finding the best way of prompting the transformer will be the main tasks in this project, but as the queries are unlabeled actually defning what 'best' means will also play a large part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Moved model to GPUs\n"
     ]
    }
   ],
   "source": [
    "#load model/tokenizer into memory and move to GPU\n",
    "model_name = \"/home/transformers\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Model and tokenizer loaded\")\n",
    "\n",
    "model.parallelize()\n",
    "print(\"Moved model to GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          searchKeyword   output\n",
      "0              wardrobe     item\n",
      "1                  desk     item\n",
      "2                kallax     item\n",
      "3                mirror     item\n",
      "4      chest of drawers     item\n",
      "5                  malm     item\n",
      "6                  sofa     item\n",
      "7         bedside table     item\n",
      "8               shelves     item\n",
      "9               drawers     item\n",
      "10           kid's room     room\n",
      "11             delivery  service\n",
      "12   assemble furniture  service\n",
      "13     customer support  service\n",
      "14              returns  service\n",
      "15          dining room     room\n",
      "16               closet     room\n",
      "17             bathroom     room\n",
      "18                porch     room\n",
      "19              balcony     room\n"
     ]
    }
   ],
   "source": [
    "hfb = \"\"\"Customer support, Living room seating, Store and organise furniture, Workspaces, \n",
    "Bedroom furniture, Beds & Mattresses, Bathroom, Kitchen, Dining, ChildrenÂ´s IKEA, \n",
    "Lighting, Bed and bath textiles, Home textiles, Rugs, Cooking, Eating, \n",
    "Decoration, Outdoor & Secondary storage, Home organisation, Other business opportunities, \n",
    "Home electronics, Home Appliances\"\"\"\n",
    "\n",
    "# simple example on how to run the inference pipeline, on a toy data set using a very basic prompt\n",
    "categories = \"\"\"item, room, service, other\"\"\"\n",
    "\n",
    "df = pd.read_csv('example.csv')\n",
    "df['output'] = 0\n",
    "i = 0\n",
    "while i < 20:\n",
    "    inp = [f\"query: {search}. which category does it belong to?\" +\n",
    "           categories for search in df.iloc[i:i+5, 1]]\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        inp, return_tensors='pt', padding=True)\n",
    "    inputs = inputs.to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        for j in range(len(inp)):\n",
    "            #print(i+j)\n",
    "            outputs = model.generate(inputs['input_ids'])\n",
    "            df.iloc[i+j,\n",
    "                        2] = tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
    "    i += 5\n",
    "print(df[['searchKeyword', 'output']])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}