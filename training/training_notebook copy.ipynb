{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from lib2to3.pgen2 import token\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from train import DataCollatorForMultipleChoice\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"dataset_name\"] = \"miniprompts002.parquet.gzip\"\n",
    "args[\"eval_name\"] = \"miniprompts002_eval.parquet.gzip\"\n",
    "args[\"model_name_or_path\"] = \"bigscience/T0_3B\"\n",
    "args[\"output_dir\"] = \"/home/gikok/output\"\n",
    "args[\"num_train_epochs\"] = 1\n",
    "args[\"per_device_train_batch_size\"] = 16\n",
    "args[\"per_device_eval_batch_size\"] = 16\n",
    "args[\"freeze_encoder\"] = True\n",
    "args[\"learning_rate\"] = 1e30\n",
    "args[\"parallelize\"] = False\n",
    "args[\"seed\"] = 42\n",
    "args[\"pad_to_max_length\"] = False\n",
    "args[\"input_eos\"] = False\n",
    "args[\"target_max_length\"] = 256\n",
    "args[\"max_length\"] = 512\n",
    "args[\"num_warmup_steps\"] = 0\n",
    "args[\"debug\"] = False\n",
    "args[\"lr_scheduler_type\"] = \"linear\"\n",
    "args[\"num_shots\"] = None\n",
    "args[\"weight_decay\"] = 0.0\n",
    "args[\"gradient_checkpoint\"] = False\n",
    "args[\"gradient_accumulation_steps\"] = 1\n",
    "args[\"max_train_steps\"] = None\n",
    "\n",
    "\n",
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(2048):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new, i], 10000)\n",
    "        pdf = val / sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:] + bin[:-1]) / 2\n",
    "        new_tensor[-n_new:, i] = torch.tensor(\n",
    "            np.interp(np.random.random(n_new), cdf, b)\n",
    "        )\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:, :] = new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 13:29:16 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "10/18/2022 13:29:16 - WARNING - datasets.builder - Using custom data configuration data-19542286d8d74b47\n",
      "10/18/2022 13:29:16 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-19542286d8d74b47/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "10/18/2022 13:29:16 - WARNING - datasets.builder - Using custom data configuration data-19542286d8d74b47\n",
      "10/18/2022 13:29:16 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-19542286d8d74b47/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /home/gikok/.cache/huggingface/transformers/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /home/gikok/.cache/huggingface/transformers/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /home/gikok/.cache/huggingface/transformers/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /home/gikok/.cache/huggingface/transformers/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "100%|██████████| 26/26 [00:05<00:00,  4.97ba/s]\n",
      "100%|██████████| 26/26 [00:01<00:00, 19.65ba/s]\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:348: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "10/18/2022 13:30:05 - INFO - __main__ - ***** Running training *****\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Num training = 25600\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Num Epochs = 1\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "10/18/2022 13:30:05 - INFO - __main__ -   Total optimization steps = 1600\n",
      "  0%|          | 0/1600 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "set_seed(args[\"seed\"])\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Handle the output directory creation\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args[\"dataset_name\"] is not None:\n",
    "    data_files = {\"train\": args[\"dataset_name\"], \"test\": args[\"eval_name\"]}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "    raw_eval_dataset = load_dataset(\"data\", data_files=data_files, split=\"test\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Please specify `args['dataset_name`.\")\n",
    "\n",
    "# Trim a number of evaluation training\n",
    "if args[\"debug\"]:\n",
    "    raw_train_dataset = raw_train_dataset.select(\n",
    "        range(min(100, len(raw_train_dataset)))\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args[\"model_name_or_path\"]).to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name_or_path\"])\n",
    "\n",
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "# tokenizer.add_tokens(items)\n",
    "\n",
    "# # then resize embeddings\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # resample shared embedding and lm_head layer\n",
    "# resample(model, 0, len(items))\n",
    "# resample(model, -1, len(items))\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args[\"pad_to_max_length\"] else False\n",
    "\n",
    "\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args[\"input_eos\"],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "column_names = raw_eval_dataset.column_names\n",
    "\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "    answer_choices_texts = examples[\"options\"]\n",
    "    bs = len(examples[column_names[0]])\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = [\n",
    "        tokenizer(\n",
    "            ans_choi,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "        )\n",
    "        for ans_choi in answer_choices_texts\n",
    "    ]\n",
    "\n",
    "    features = {\n",
    "        k: [\n",
    "            [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "            for idx, elem in enumerate(v)\n",
    "        ]\n",
    "        for k, v in tokenized_inputs.items()\n",
    "    }\n",
    "\n",
    "    features[\"labels\"] = [tokenized_targets[idx][\"input_ids\"] for idx in range(bs)]\n",
    "    features[\"labels_attention_mask\"] = [\n",
    "        tokenized_targets[idx][\"attention_mask\"] for idx in range(bs)\n",
    "    ]\n",
    "    features[\"targets\"] = [\n",
    "        answer_choices_texts[idx].index(t) for idx, t in enumerate(target_texts)\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = raw_eval_dataset.map(\n",
    "        preprocess_eval, batched=True, remove_columns=column_names\n",
    "    )\n",
    "\n",
    "    if args[\"num_shots\"] is not None:\n",
    "        sample_indices = random.sample(\n",
    "            range(0, len(raw_train_dataset)), k=args[\"num_shots\"]\n",
    "        )\n",
    "        raw_train_dataset = raw_train_dataset.select(sample_indices)\n",
    "    train_dataset = raw_train_dataset.map(tokenize_train, batched=True)\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Log a few random training:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.debug(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "# for index in random.sample(range(len(eval_dataset)), 3):\n",
    "#     logger.debug(f\"Sample {index} of the evaluation set: {eval_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args[\"per_device_train_batch_size\"],\n",
    ")\n",
    "\n",
    "if args[\"pad_to_max_length\"]:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    eval_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    eval_collator = DataCollatorForMultipleChoice(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=eval_collator,\n",
    "    batch_size=args[\"per_device_eval_batch_size\"],\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args[\"weight_decay\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args[\"learning_rate\"])\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "if args[\"max_train_steps\"] is None:\n",
    "    args[\"max_train_steps\"] = args[\"num_train_epochs\"] * num_update_steps_per_epoch\n",
    "else:\n",
    "    args[\"num_train_epochs\"] = math.ceil(\n",
    "        args[\"max_train_steps\"] / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args[\"lr_scheduler_type\"],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args[\"num_warmup_steps\"],\n",
    "    num_training_steps=args[\"max_train_steps\"],\n",
    ")\n",
    "\n",
    "if args[\"parallelize\"]:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    assert num_gpus > 1, \"You need at least 2 GPUs to use `model.parallelize()`.\"\n",
    "    model.parallelize()\n",
    "    optimizer, train_dataloader = accelerator.prepare(optimizer, train_dataloader)\n",
    "else:\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "total_batch_size = (\n",
    "    args[\"per_device_train_batch_size\"]\n",
    "    * accelerator.num_processes\n",
    "    * args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num training = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args['per_device_train_batch_size']}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {args['max_train_steps']}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(args[\"max_train_steps\"]), disable=not accelerator.is_local_main_process\n",
    ")\n",
    "global_steps = 0\n",
    "\n",
    "# how often trained model should be saved\n",
    "r = int(args[\"max_train_steps\"] / 30)\n",
    "if args[\"gradient_checkpoint\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_grad = []\n",
    "em_delta_pct = []\n",
    "lm_grad = []\n",
    "lm_delta_pct = []\n",
    "em = list(model.named_parameters())[0][1]\n",
    "lm = list(model.named_parameters())[-1][1]\n",
    "em_old = em * 1\n",
    "lm_old = lm * 1\n",
    "init_len = len(tokenizer) - len(items)\n",
    "def hook(grad):\n",
    "    grad_mask[:init_len, :] = 0\n",
    "    print(grad[len(tokenizer)-100, :].mean())\n",
    "    print(\"HERE IS HOOK\")\n",
    "    return grad*grad_mask\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        grad_mask = torch.ones_like(param)\n",
    "        grad_mask[:init_len, :] = 0\n",
    "        h = param.register_hook(hook)\n",
    "for epoch in range(1, 2):#args[\"num_train_epochs\"] + 1):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        if step % 1 == 0 or step == len(train_dataloader) - 1:\n",
    "            item_no = int(step/4)\n",
    "            ind = len(tokenizer) - 100 + item_no\n",
    "            em_grad += [\n",
    "                ((em.grad[ind, :]).mean()).cpu()*1\n",
    "            ]\n",
    "            lm_grad += [\n",
    "                ((lm.grad[ind, :]).mean()).cpu()*1\n",
    "            ]\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            loss = loss.item()\n",
    "            em = list(model.named_parameters())[0][1]\n",
    "            lm = list(model.named_parameters())[-1][1]\n",
    "            em_delta_pct += [\n",
    "                (((em[ind, :] - em_old[ind, :]) / em[ind, :]).mean()).cpu()\n",
    "            ]\n",
    "            lm_delta_pct += [\n",
    "                (((lm[ind, :] - lm_old[ind, :]) / lm[ind, :]).mean()).cpu()\n",
    "            ]\n",
    "            em_old = em * 1\n",
    "            lm_old = lm * 1\n",
    "            if accelerator.is_main_process:\n",
    "                tqdm.write(f\"epoch = {epoch}, step = {global_steps}, loss = {loss}\")\n",
    "        if step >= 25:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_len = len(tokenizer) - len(items)\n",
    "def hook(grad):\n",
    "    grad_mask[:init_len, :] = 0\n",
    "    print(grad[len(tokenizer)-100, :].mean())\n",
    "    print(\"HERE IS HOOK\")\n",
    "    return grad*grad_mask\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        grad_mask = torch.ones_like(param)\n",
    "        grad_mask[:init_len, :] = 0\n",
    "        h = param.register_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0476, -0.0623, -0.0977,  ...,  0.0262,  0.0427, -0.0295],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = list(model.named_parameters())[-1][1]\n",
    "lm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0476, -0.0623, -0.0977,  ...,  0.0262,  0.0427, -0.0295],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = list(model.named_parameters())[-1][1]\n",
    "lm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=args[\"learning_rate\"])\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plmodel = LitAutoEncoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 13:39:58 - INFO - pytorch_lightning.trainer.connectors.accelerator_connector - Auto select gpus: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 13:39:59 - INFO - pytorch_lightning.utilities.rank_zero - GPU available: True (cuda), used: True\n",
      "10/18/2022 13:39:59 - INFO - pytorch_lightning.utilities.rank_zero - TPU available: False, using: 0 TPU cores\n",
      "10/18/2022 13:39:59 - INFO - pytorch_lightning.utilities.rank_zero - IPU available: False, using: 0 IPUs\n",
      "10/18/2022 13:39:59 - INFO - pytorch_lightning.utilities.rank_zero - HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=-1, auto_select_gpus=True, auto_lr_find=True, accumulate_grad_batches=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 13:40:24 - INFO - pytorch_lightning.accelerators.cuda - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:348: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:241: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:   4%|▍         | 4/100 [00:05<02:01,  1.26s/it]\n",
      "10/18/2022 13:40:31 - INFO - pytorch_lightning.utilities.rank_zero - `Trainer.fit` stopped: `max_steps=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(0., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n",
      "tensor(1., device='cuda:0')\n",
      "HERE IS HOOK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 13:40:36 - INFO - pytorch_lightning.tuner.lr_finder - LR finder stopped early after 4 steps due to diverging loss.\n",
      "10/18/2022 13:40:36 - ERROR - pytorch_lightning.tuner.lr_finder - Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "10/18/2022 13:40:36 - INFO - pytorch_lightning.utilities.rank_zero - Restoring states from the checkpoint path at /home/gikok/t0code2/training/.lr_find_bf90d385-26fd-4e1e-b917-4e9df9ae0c01.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr_find': <pytorch_lightning.tuner.lr_finder._LRFinder at 0x7fbf5f4f9e90>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainer.fit(model=plmodel, train_dataloaders=train_dataloader)\n",
    "trainer.tune(model=plmodel, train_dataloaders=train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plmodel.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        t = torch.zeros(param.shape).to(\"cuda:0\")\n",
    "        t[-len(items) :, :] = 1\n",
    "        param.register_hook(lambda grad: grad * t)\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "global_steps += 1\n",
    "loss = loss.item()\n",
    "if accelerator.is_main_process:\n",
    "    tqdm.write(f\"epoch = {1}, step = {global_steps}, loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(model.children())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values[0][1].grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items) :, :] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l += 1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l, mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
