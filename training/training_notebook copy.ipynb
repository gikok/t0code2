{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from lib2to3.pgen2 import token\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from train import DataCollatorForMultipleChoice\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dataset_name'] = 'prompts2opts_001_sub.parquet.gzip'\n",
    "args['model_name_or_path'] = 'bigscience/T0_3B'\n",
    "args['output_dir'] = '/home/gikok/output'\n",
    "args['num_train_epochs'] = 1\n",
    "args['per_device_train_batch_size'] = 2\n",
    "args['per_device_eval_batch_size'] = 2\n",
    "args['freeze_encoder'] = True\n",
    "args['learning_rate'] = 10000\n",
    "args['parallelize'] = False\n",
    "args['seed'] = 42\n",
    "args['pad_to_max_length'] = True\n",
    "args['input_eos'] = False\n",
    "args['target_max_length'] = 256\n",
    "args['max_length'] = 512\n",
    "args['num_warmup_steps'] = 0\n",
    "args['debug'] = False\n",
    "args['lr_scheduler_type'] = 'linear'\n",
    "args['num_shots'] = None\n",
    "args['weight_decay'] = 0.01\n",
    "args['gradient_checkpoint'] = False\n",
    "args['gradient_accumulation_steps'] = 8\n",
    "args['max_train_steps'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2022 13:22:16 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "08/17/2022 13:22:16 - WARNING - datasets.builder - Using custom data configuration data-1847ec1d8d9327ad\n",
      "08/17/2022 13:22:16 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-1847ec1d8d9327ad/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /home/gikok/.cache/huggingface/transformers/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /home/gikok/.cache/huggingface/transformers/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /home/gikok/.cache/huggingface/transformers/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /home/gikok/.cache/huggingface/transformers/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "08/17/2022 13:24:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/gikok/.cache/huggingface/datasets/parquet/data-1847ec1d8d9327ad/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fb0b88ff27f6458c.arrow\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "08/17/2022 13:24:18 - INFO - __main__ - ***** Running training *****\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Num training = 96576\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Num Epochs = 1\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "08/17/2022 13:24:18 - INFO - __main__ -   Total optimization steps = 6036\n",
      "  0%|          | 0/6036 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "set_seed(args['seed'])\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(\n",
    "    logging.INFO if accelerator.is_local_main_process else logging.ERROR\n",
    ")\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Handle the output directory creation\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(args['output_dir'], exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args['dataset_name'] is not None:\n",
    "    data_files = {\"train\": args['dataset_name'], \"test\": args['dataset_name']}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "    # raw_eval_dataset = load_dataset(\"data\", data_files=data_files, split=\"test\")\n",
    "else:\n",
    "    raise ValueError(\"Please specify `args['dataset_name`.\")\n",
    "\n",
    "# Trim a number of evaluation training\n",
    "if args['debug']:\n",
    "    raw_train_dataset = raw_train_dataset.select(\n",
    "        range(min(100, len(raw_train_dataset)))\n",
    "    )\n",
    "    # raw_eval_dataset = raw_eval_dataset.select(\n",
    "    #     range(min(100, len(raw_eval_dataset)))\n",
    "    # )\n",
    "\n",
    "# column_names = raw_eval_dataset.column_names\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args['model_name_or_path']).to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'])\n",
    "\n",
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet(\"data/item_no_6k.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "tokenizer.add_tokens(items)\n",
    "\n",
    "# then resize embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args['pad_to_max_length'] else False\n",
    "\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args['input_eos'],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "    answer_choices_texts = examples[\"options\"]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = [\n",
    "        tokenizer(\n",
    "            ans_choi,\n",
    "            padding=True,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "        )\n",
    "        for ans_choi in answer_choices_texts\n",
    "    ]\n",
    "\n",
    "    features = {\n",
    "        k: [\n",
    "            [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "            for idx, elem in enumerate(v)\n",
    "        ]\n",
    "        for k, v in tokenized_inputs.items()\n",
    "    }\n",
    "\n",
    "    features[\"labels\"] = [tokenized_targets[idx][\"input_ids\"] for idx in range(bs)]\n",
    "    features[\"labels_attention_mask\"] = [\n",
    "        tokenized_targets[idx][\"attention_mask\"] for idx in range(bs)\n",
    "    ]\n",
    "    features[\"targets\"] = [\n",
    "        answer_choices_texts[idx].index(t) for idx, t in enumerate(target_texts)\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    # eval_dataset = raw_eval_dataset.map(\n",
    "    #     preprocess_eval, batched=True, remove_columns=column_names\n",
    "    # )\n",
    "\n",
    "    if args['num_shots'] is not None:\n",
    "        sample_indices = random.sample(\n",
    "            range(0, len(raw_train_dataset)), k=args['num_shots']\n",
    "        )\n",
    "        raw_train_dataset = raw_train_dataset.select(sample_indices)\n",
    "    train_dataset = raw_train_dataset.map(\n",
    "        tokenize_train, batched=True\n",
    "    )\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# Log a few random training:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.debug(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "# for index in random.sample(range(len(eval_dataset)), 3):\n",
    "#     logger.debug(f\"Sample {index} of the evaluation set: {eval_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args['per_device_train_batch_size'],\n",
    ")\n",
    "\n",
    "# if args['pad_to_max_length']:\n",
    "#     # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "#     # to tensors.\n",
    "#     eval_collator = default_data_collator\n",
    "# else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    # eval_collator = DataCollatorForMultipleChoice(\n",
    "    #     tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    # )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     eval_dataset,\n",
    "#     collate_fn=eval_collator,\n",
    "#     batch_size=args['per_device_eval_batch_size'],\n",
    "# )\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'])\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args['gradient_accumulation_steps']\n",
    ")\n",
    "if args['max_train_steps'] is None:\n",
    "    args['max_train_steps'] = args['num_train_epochs'] * num_update_steps_per_epoch\n",
    "else:\n",
    "    args['num_train_epochs'] = math.ceil(\n",
    "        args['max_train_steps'] / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args['lr_scheduler_type'],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args['num_warmup_steps'],\n",
    "    num_training_steps=args['max_train_steps'],\n",
    ")\n",
    "\n",
    "if args['parallelize']:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    assert num_gpus > 1, \"You need at least 2 GPUs to use `model.parallelize()`.\"\n",
    "    model.parallelize()\n",
    "    optimizer, train_dataloader = accelerator.prepare(\n",
    "        optimizer, train_dataloader\n",
    "    )\n",
    "else:\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "total_batch_size = (\n",
    "    args['per_device_train_batch_size']\n",
    "    * accelerator.num_processes\n",
    "    * args['gradient_accumulation_steps']\n",
    ")\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num training = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args['per_device_train_batch_size']}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {args['max_train_steps']}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(args['max_train_steps']), disable=not accelerator.is_local_main_process\n",
    ")\n",
    "global_steps = 0\n",
    "\n",
    "# how often trained model should be saved\n",
    "r = int(args['max_train_steps'] / 30)\n",
    "if args['gradient_checkpoint']:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = list(model.named_parameters())[0][1]\n",
    "lm_head = list(model.named_parameters())[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=0\n",
    "n_new = len(items)\n",
    "new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(new_tensor[:-n_new,i], 10000)\n",
    "    pdf = val/sum(val)\n",
    "    cdf = np.cumsum(pdf)\n",
    "    b = (bin[1:]+bin[:-1])/2\n",
    "    new_tensor[-n_new:,i] = torch.tensor(np.interp(np.random.random(n_new), cdf, b))\n",
    "data = list(model.named_parameters())[layer][1].data\n",
    "data[:,:] = new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  0.1436,   3.8750,   0.5352,  ...,  30.8750,   1.3281, -21.5000],\n",
       "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
       "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
       "        ...,\n",
       "        [ -0.3020,  -0.3723,   1.2172,  ...,   1.8101,   1.0143,  -0.7559],\n",
       "        [ -0.7375,   0.0380,   0.1171,  ...,   1.1145,  -0.9926,   1.0823],\n",
       "        [  0.4283,   0.9865,   0.6769,  ...,  -1.0338,  -0.2796,  -0.3376]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "    means = new_tensor[:-n_new,:].mean(axis=0)\n",
    "    stds = new_tensor[:-n_new,:].std(axis=0)\n",
    "    for i in range(2048):\n",
    "        new_tensor[-n_new:,i] = np.random.normal(means[i], stds[i], size = n_new)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "og = embeddings[:-n_new,:].detach().cpu()\n",
    "new = embeddings[-n_new:,:].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "og_vals = np.zeros((2048, 30))\n",
    "og_bins = np.zeros((2048, 30))\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(og[:,i], 30, normed=True)\n",
    "    og_vals[i] = val\n",
    "    og_bins[i] = (bin[1:]+bin[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "new_vals = np.zeros((2048, 30))\n",
    "new_bins = np.zeros((2048, 30))\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(new[:,i], 30, normed=True)\n",
    "    new_vals[i] = val\n",
    "    new_bins[i] = (bin[1:]+bin[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2048):\n",
    "    plt.plot(og_bins[i], og_vals[i], c='blue', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2048):\n",
    "    plt.plot(new_bins[i], new_vals[i], c='blue', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.90364513, -5.22604294, -2.94816726])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6036 [00:01<3:02:18,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2796, device='cuda:0')\n",
      "lm_grad tensor(6531968, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3785, device='cuda:0')\n",
      "lm_grad tensor(2733, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3813, device='cuda:0')\n",
      "lm_grad tensor(2839, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3758, device='cuda:0')\n",
      "lm_grad tensor(7540, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3770, device='cuda:0')\n",
      "lm_grad tensor(8485, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6561, device='cuda:0')\n",
      "lm_grad tensor(8699, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6561, device='cuda:0')\n",
      "lm_grad tensor(10501, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6563, device='cuda:0')\n",
      "lm_grad tensor(11384, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6036 [00:09<8:39:52,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7562, device='cuda:0')\n",
      "lm_grad tensor(12464, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1000, device='cuda:0')\n",
      "lm_grad tensor(1017, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(997, device='cuda:0')\n",
      "lm_grad tensor(1050, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2874, device='cuda:0')\n",
      "lm_grad tensor(1942, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2846, device='cuda:0')\n",
      "lm_grad tensor(1960, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2856, device='cuda:0')\n",
      "lm_grad tensor(1952, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2830, device='cuda:0')\n",
      "lm_grad tensor(1941, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2831, device='cuda:0')\n",
      "lm_grad tensor(1951, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6036 [00:16<10:27:12,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4750, device='cuda:0')\n",
      "lm_grad tensor(2837, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1029, device='cuda:0')\n",
      "lm_grad tensor(1016, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1023, device='cuda:0')\n",
      "lm_grad tensor(1887, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1055, device='cuda:0')\n",
      "lm_grad tensor(2017, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1041, device='cuda:0')\n",
      "lm_grad tensor(3112, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3858, device='cuda:0')\n",
      "lm_grad tensor(4035, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(5728, device='cuda:0')\n",
      "lm_grad tensor(4122, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(7619, device='cuda:0')\n",
      "lm_grad tensor(4146, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/6036 [00:24<11:17:44,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11318, device='cuda:0')\n",
      "lm_grad tensor(5116, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1056, device='cuda:0')\n",
      "lm_grad tensor(960, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1991, device='cuda:0')\n",
      "lm_grad tensor(1940, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2008, device='cuda:0')\n",
      "lm_grad tensor(1926, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1998, device='cuda:0')\n",
      "lm_grad tensor(1934, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2006, device='cuda:0')\n",
      "lm_grad tensor(1919, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1968, device='cuda:0')\n",
      "lm_grad tensor(1912, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1947, device='cuda:0')\n",
      "lm_grad tensor(1920, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/6036 [00:31<11:45:36,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4721, device='cuda:0')\n",
      "lm_grad tensor(2833, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = list(model.named_parameters())[0][1]\n",
    "lm_head = list(model.named_parameters())[-1][1]\n",
    "init_len = len(tokenizer)-len(items)\n",
    "for epoch in range(1, args['num_train_epochs'] + 1):\n",
    "    model.train()\n",
    "\n",
    "    # freeze encoder updates if specified\n",
    "    if args['freeze_encoder']:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "                param.requires_grad = False\n",
    "            if name.startswith('shared') or name.startswith(\"lm_head\"):\n",
    "                grad_mask = torch.ones_like(param)\n",
    "                grad_mask[:init_len,:] = 0\n",
    "                h = param.register_hook(lambda grad: grad * grad_mask)\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / 8\n",
    "        accelerator.backward(loss)\n",
    "        print(\"emb_grad\", (embeddings.grad>0).sum())\n",
    "        print(\"lm_grad\", (lm_head.grad>0).sum())\n",
    "        if (\n",
    "                step % 8 == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "        ):\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            loss = loss.item()\n",
    "\n",
    "            item_ind = int(global_steps/2) #two batches per item_id\n",
    "            item_no = items[item_ind]\n",
    "\n",
    "            # get gradients for embedding matrix and lm_head\n",
    "            emb_grad = embeddings.grad\n",
    "            lm_grad = lm_head.grad\n",
    "            print(\"INSIDE emb_grad\", (embeddings.grad>0).sum())\n",
    "            print(\"INSIDE lm_grad\", (lm_head.grad>0).sum())\n",
    "            # embedding_grad_item = torch.norm(emb_grad[init_len+item_ind,:], p=2, dim=-1)\n",
    "            # embedding_grad_rest = torch.norm(torch.cat((emb_grad[init_len:init_len+item_ind,:], \n",
    "            #                                             emb_grad[init_len+item_ind+1:,:])),\n",
    "            #                                 p=2,\n",
    "            #                                 dim=-1).mean()\n",
    "            # lm_grad_item = torch.norm(lm_grad[init_len+item_ind,:], p=2, dim=-1)\n",
    "            # lm_grad_rest = torch.norm(torch.cat((lm_grad[init_len:init_len+item_ind,:], \n",
    "            #                                             lm_grad[init_len+item_ind+1:,:])),\n",
    "            #                                 p=2,\n",
    "            #                                 dim=-1).mean()\n",
    "            # wandb.log({\"loss\": loss, 'item_no':item_no, 'embedding_grad_item':embedding_grad_item,\n",
    "            #  'embedding_grad_rest':embedding_grad_rest, 'lm_grad_item':lm_grad_item,\n",
    "            #  'lm_grad_rest':lm_grad_rest}, step=global_steps)\n",
    "            # global_steps += 1\n",
    "        if step>=32:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  0.1436,   3.8750,   0.5352,  ...,  30.8750,   1.3281, -21.5000],\n",
       "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
       "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
       "        ...,\n",
       "        [ -0.3020,  -0.3723,   1.2172,  ...,   1.8101,   1.0143,  -0.7559],\n",
       "        [ -0.7375,   0.0380,   0.1171,  ...,   1.1145,  -0.9926,   1.0823],\n",
       "        [  0.4283,   0.9865,   0.6769,  ...,  -1.0338,  -0.2796,  -0.3376]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.7607e-02, -6.2256e-02, -9.7656e-02,  ...,  2.6245e-02,\n",
       "          4.2725e-02, -2.9541e-02],\n",
       "        [-6.7383e-02, -1.2878e-02, -1.5234e-01,  ..., -1.4355e-01,\n",
       "         -2.8564e-02,  6.4453e-02],\n",
       "        [-3.1006e-02, -1.5820e-01,  2.6758e-01,  ...,  4.6143e-02,\n",
       "         -7.4707e-02,  1.6992e-01],\n",
       "        ...,\n",
       "        [ 5.1124e+05, -6.2280e+04, -7.8284e+04,  ..., -2.1716e+05,\n",
       "         -6.6607e+04, -3.2049e+04],\n",
       "        [ 7.0905e+03, -6.2312e+02, -1.0533e+03,  ..., -2.8763e+03,\n",
       "         -6.3681e+02, -4.1033e+02],\n",
       "        [ 1.4768e+03, -1.1156e+02, -2.2923e+02,  ..., -5.1655e+02,\n",
       "         -1.7873e+02, -6.2893e+01]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23757/3196268026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "v = torch.ones((emb.shape)).to('cuda:0')\n",
    "v2 = next(iter(lm._backward_hooks))(v)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lm._backward_hooks.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(grad)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb._backward_hooks[list(emb._backward_hooks.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 32128]), torch.Size([2048, 32128]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_n = torch.norm(emb, p=2, dim=-1)\n",
    "lm_n = torch.norm(lm, p=2, dim=-1 )\n",
    "emb_ = emb.T/emb_n\n",
    "lm_ = lm.T/lm_n\n",
    "lm_.shape, emb_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    heat_map = torch.matmul(emb_.cpu(), lm_.T.cpu())\n",
    "    ax = sns.heatmap(heat_map, norm=LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 125,   19,    3,    9, 9321,   58,    1]], device='cuda:0')\n",
      "torch.Size([1, 7, 2048])\n",
      "torch.Size([1, 7, 32128])\n",
      "tensor([  773, 18391, 22154,  9631,  7531, 25900, 11001], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kind Standortalbeitrésboygefordertpermalink'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tokenizer.encode('what is a cow?', return_tensors='pt').to('cuda:0')\n",
    "#l = l[0][0]\n",
    "print(l)\n",
    "l_e = embedding_module(l)\n",
    "print(l_e.shape)\n",
    "l_lm = lm_head(l_e)\n",
    "print(l_lm.shape)\n",
    "m = nn.Softmax(dim=-1)\n",
    "d = m(l_lm).max(axis=-1)[1][0]\n",
    "print(d)\n",
    "tokenizer.decode(d, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/06/2022 11:55:13 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgikok\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gikok/t0code2/training/wandb/run-20220706_115513-39wegzjz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8081/gikok/grad_test/runs/39wegzjz\" target=\"_blank\">glad-serenity-5</a></strong> to <a href=\"http://localhost:8081/gikok/grad_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"http://localhost:8081/gikok/grad_test/runs/39wegzjz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ffa7bb1b290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='grad_test', config={'a':'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/24144 [04:19<55:59:29,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "3.676435708999634"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = emb[init_len+item_ind,:]\n",
    "b = torch.cat((emb[init_len:init_len+item_ind,:], emb[init_len+item_ind+1:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_grad_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.2586, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizerFast(name_or_path='bigscience/T0_3B', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}), model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.max(\n",
       " values=tensor([ 46.5000,  65.5000,  32.0000,  58.5000,  20.7500,  61.0000,  67.5000,\n",
       "         112.0000, 122.5000,  86.5000], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " indices=tensor([ 757,  190, 1051,  711,  303,  793,  190,  843,  843,  843],\n",
       "        device='cuda:0')),\n",
       " torch.return_types.max(\n",
       " values=tensor([0.2393, 1.4375, 0.5039, 0.3613, 0.6602, 0.2559, 0.2520, 0.3145, 0.2852,\n",
       "         0.4180], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " indices=tensor([1506, 1035,  161,  351,  431, 1809,  351,  679,  710,  161],\n",
       "        device='cuda:0')))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0:10,:].max(axis=-1), lm[0:10,:].max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n",
      "lol\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/3758117431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;32m/tmp/ipykernel_20577/1684194404.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;31m#param.detach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('shared') or name.startswith(\"lm_head\"):\n",
    "        t = torch.zeros(param.shape).to('cuda:0')\n",
    "        t[-len(items):,:] = 1\n",
    "        param.register_hook(lambda grad: grad*t)\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "global_steps += 1\n",
    "loss = loss.item()\n",
    "if accelerator.is_main_process:\n",
    "    tqdm.write(f\"epoch = {1}, step = {global_steps}, loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/1934986589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'last' is not defined"
     ]
    }
   ],
   "source": [
    "last(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values[0][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items):,:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Embedding(38136, 2048)\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "38136\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l +=1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l,mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(38136, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=38136, bias=False)\n",
      ")\n",
      "************** 1\n",
      "Embedding(38136, 2048)\n",
      "************** 2\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 3\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 4\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 5\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 6\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 7\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 8\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 9\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 10\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 11\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 12\n",
      "Embedding(32, 32)\n",
      "************** 13\n",
      "T5LayerNorm()\n",
      "************** 14\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 15\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 16\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 17\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 18\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 19\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 20\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 21\n",
      "NewGELUActivation()\n",
      "************** 22\n",
      "T5LayerNorm()\n",
      "************** 23\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 24\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 25\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 26\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 27\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 28\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 29\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 30\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 31\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 32\n",
      "T5LayerNorm()\n",
      "************** 33\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 34\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 35\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 36\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 37\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 38\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 39\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 40\n",
      "T5LayerNorm()\n",
      "************** 41\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 42\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 43\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 44\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 45\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 46\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 47\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 48\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 49\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 50\n",
      "T5LayerNorm()\n",
      "************** 51\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 52\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 53\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 54\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 55\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 56\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 57\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 58\n",
      "T5LayerNorm()\n",
      "************** 59\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 60\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 61\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 62\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 63\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 64\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 65\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 66\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 67\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 68\n",
      "T5LayerNorm()\n",
      "************** 69\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 70\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 71\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 72\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 73\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 74\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 75\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 76\n",
      "T5LayerNorm()\n",
      "************** 77\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 78\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 79\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 80\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 81\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 82\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 83\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 84\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 85\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 86\n",
      "T5LayerNorm()\n",
      "************** 87\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 88\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 89\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 90\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 91\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 92\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 93\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 94\n",
      "T5LayerNorm()\n",
      "************** 95\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 96\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 97\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 98\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 99\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 100\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 101\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 102\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 103\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 104\n",
      "T5LayerNorm()\n",
      "************** 105\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 106\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 107\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 108\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 109\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 110\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 111\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 112\n",
      "T5LayerNorm()\n",
      "************** 113\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 114\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 115\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 116\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 117\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 118\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 119\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 120\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 121\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 122\n",
      "T5LayerNorm()\n",
      "************** 123\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 124\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 125\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 126\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 127\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 128\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 129\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 130\n",
      "T5LayerNorm()\n",
      "************** 131\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 132\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 133\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 134\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 135\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 136\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 137\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 138\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 139\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 140\n",
      "T5LayerNorm()\n",
      "************** 141\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 142\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 143\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 144\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 145\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 146\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 147\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 148\n",
      "T5LayerNorm()\n",
      "************** 149\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 150\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 151\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 152\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 153\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 154\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 155\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 156\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 157\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 158\n",
      "T5LayerNorm()\n",
      "************** 159\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 160\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 161\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 162\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 163\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 164\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 165\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 166\n",
      "T5LayerNorm()\n",
      "************** 167\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 168\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 169\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 170\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 171\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 172\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 173\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 174\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 175\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 176\n",
      "T5LayerNorm()\n",
      "************** 177\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 178\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 179\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 180\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 181\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 182\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 183\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 184\n",
      "T5LayerNorm()\n",
      "************** 185\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 186\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 187\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 188\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 189\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 190\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 191\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 192\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 193\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 194\n",
      "T5LayerNorm()\n",
      "************** 195\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 196\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 197\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 198\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 199\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 200\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 201\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 202\n",
      "T5LayerNorm()\n",
      "************** 203\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 204\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 205\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 206\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 207\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 208\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 209\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 210\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 211\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 212\n",
      "T5LayerNorm()\n",
      "************** 213\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 214\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 215\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 216\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 217\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 218\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 219\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 220\n",
      "T5LayerNorm()\n",
      "************** 221\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 222\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 223\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 224\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 225\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 226\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 227\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 228\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 229\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 230\n",
      "T5LayerNorm()\n",
      "************** 231\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 232\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 233\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 234\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 235\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 236\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 237\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 238\n",
      "T5LayerNorm()\n",
      "************** 239\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 240\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 241\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 242\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 243\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 244\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 245\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 246\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 247\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 248\n",
      "T5LayerNorm()\n",
      "************** 249\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 250\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 251\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 252\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 253\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 254\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 255\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 256\n",
      "T5LayerNorm()\n",
      "************** 257\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 258\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 259\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 260\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 261\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 262\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 263\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 264\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 265\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 266\n",
      "T5LayerNorm()\n",
      "************** 267\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 268\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 269\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 270\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 271\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 272\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 273\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 274\n",
      "T5LayerNorm()\n",
      "************** 275\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 276\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 277\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 278\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 279\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 280\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 281\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 282\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 283\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 284\n",
      "T5LayerNorm()\n",
      "************** 285\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 286\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 287\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 288\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 289\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 290\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 291\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 292\n",
      "T5LayerNorm()\n",
      "************** 293\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 294\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 295\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 296\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 297\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 298\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 299\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 300\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 301\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 302\n",
      "T5LayerNorm()\n",
      "************** 303\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 304\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 305\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 306\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 307\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 308\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 309\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 310\n",
      "T5LayerNorm()\n",
      "************** 311\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 312\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 313\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 314\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 315\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 316\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 317\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 318\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 319\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 320\n",
      "T5LayerNorm()\n",
      "************** 321\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 322\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 323\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 324\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 325\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 326\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 327\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 328\n",
      "T5LayerNorm()\n",
      "************** 329\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 330\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 331\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 332\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 333\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 334\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 335\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 336\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 337\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 338\n",
      "T5LayerNorm()\n",
      "************** 339\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 340\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 341\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 342\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 343\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 344\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 345\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 346\n",
      "T5LayerNorm()\n",
      "************** 347\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 348\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 349\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 350\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 351\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 352\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 353\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 354\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 355\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 356\n",
      "T5LayerNorm()\n",
      "************** 357\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 358\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 359\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 360\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 361\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 362\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 363\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 364\n",
      "T5LayerNorm()\n",
      "************** 365\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 366\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 367\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 368\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 369\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 370\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 371\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 372\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 373\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 374\n",
      "T5LayerNorm()\n",
      "************** 375\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 376\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 377\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 378\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 379\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 380\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 381\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 382\n",
      "T5LayerNorm()\n",
      "************** 383\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 384\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 385\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 386\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 387\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 388\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 389\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 390\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 391\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 392\n",
      "T5LayerNorm()\n",
      "************** 393\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 394\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 395\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 396\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 397\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 398\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 399\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 400\n",
      "T5LayerNorm()\n",
      "************** 401\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 402\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 403\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 404\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 405\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 406\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 407\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 408\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 409\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 410\n",
      "T5LayerNorm()\n",
      "************** 411\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 412\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 413\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 414\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 415\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 416\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 417\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 418\n",
      "T5LayerNorm()\n",
      "************** 419\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 420\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 421\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 422\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 423\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 424\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 425\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 426\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 427\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 428\n",
      "T5LayerNorm()\n",
      "************** 429\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 430\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 431\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 432\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 433\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 434\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 435\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 436\n",
      "T5LayerNorm()\n",
      "************** 437\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 438\n",
      "T5LayerNorm()\n",
      "************** 439\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 440\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 441\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 442\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 443\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 444\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 445\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 446\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 447\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 448\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 449\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 450\n",
      "Embedding(32, 32)\n",
      "************** 451\n",
      "T5LayerNorm()\n",
      "************** 452\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 453\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 454\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 455\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 456\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 457\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 458\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 459\n",
      "T5LayerNorm()\n",
      "************** 460\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 461\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 462\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 463\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 464\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 465\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 466\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 467\n",
      "T5LayerNorm()\n",
      "************** 468\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 469\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 470\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 471\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 472\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 473\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 474\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 475\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 476\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 477\n",
      "T5LayerNorm()\n",
      "************** 478\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 479\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 480\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 481\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 482\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 483\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 484\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 485\n",
      "T5LayerNorm()\n",
      "************** 486\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 487\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 488\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 489\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 490\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 491\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 492\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 493\n",
      "T5LayerNorm()\n",
      "************** 494\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 495\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 496\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 497\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 498\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 499\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 500\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 501\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 502\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 503\n",
      "T5LayerNorm()\n",
      "************** 504\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 505\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 506\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 507\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 508\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 509\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 510\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 511\n",
      "T5LayerNorm()\n",
      "************** 512\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 513\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 514\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 515\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 516\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 517\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 518\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 519\n",
      "T5LayerNorm()\n",
      "************** 520\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 521\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 522\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 523\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 524\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 525\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 526\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 527\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 528\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 529\n",
      "T5LayerNorm()\n",
      "************** 530\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 531\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 532\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 533\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 534\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 535\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 536\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 537\n",
      "T5LayerNorm()\n",
      "************** 538\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 539\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 540\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 541\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 542\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 543\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 544\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 545\n",
      "T5LayerNorm()\n",
      "************** 546\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 547\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 548\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 549\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 550\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 551\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 552\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 553\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 554\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 555\n",
      "T5LayerNorm()\n",
      "************** 556\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 557\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 558\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 559\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 560\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 561\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 562\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 563\n",
      "T5LayerNorm()\n",
      "************** 564\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 565\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 566\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 567\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 568\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 569\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 570\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 571\n",
      "T5LayerNorm()\n",
      "************** 572\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 573\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 574\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 575\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 576\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 577\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 578\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 579\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 580\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 581\n",
      "T5LayerNorm()\n",
      "************** 582\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 583\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 584\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 585\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 586\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 587\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 588\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 589\n",
      "T5LayerNorm()\n",
      "************** 590\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 591\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 592\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 593\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 594\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 595\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 596\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 597\n",
      "T5LayerNorm()\n",
      "************** 598\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 599\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 600\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 601\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 602\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 603\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 604\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 605\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 606\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 607\n",
      "T5LayerNorm()\n",
      "************** 608\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 609\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 610\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 611\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 612\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 613\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 614\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 615\n",
      "T5LayerNorm()\n",
      "************** 616\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 617\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 618\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 619\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 620\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 621\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 622\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 623\n",
      "T5LayerNorm()\n",
      "************** 624\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 625\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 626\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 627\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 628\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 629\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 630\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 631\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 632\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 633\n",
      "T5LayerNorm()\n",
      "************** 634\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 635\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 636\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 637\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 638\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 639\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 640\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 641\n",
      "T5LayerNorm()\n",
      "************** 642\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 643\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 644\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 645\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 646\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 647\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 648\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 649\n",
      "T5LayerNorm()\n",
      "************** 650\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 651\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 652\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 653\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 654\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 655\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 656\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 657\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 658\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 659\n",
      "T5LayerNorm()\n",
      "************** 660\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 661\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 662\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 663\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 664\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 665\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 666\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 667\n",
      "T5LayerNorm()\n",
      "************** 668\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 669\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 670\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 671\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 672\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 673\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 674\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 675\n",
      "T5LayerNorm()\n",
      "************** 676\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 677\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 678\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 679\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 680\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 681\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 682\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 683\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 684\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 685\n",
      "T5LayerNorm()\n",
      "************** 686\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 687\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 688\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 689\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 690\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 691\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 692\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 693\n",
      "T5LayerNorm()\n",
      "************** 694\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 695\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 696\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 697\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 698\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 699\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 700\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 701\n",
      "T5LayerNorm()\n",
      "************** 702\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 703\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 704\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 705\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 706\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 707\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 708\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 709\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 710\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 711\n",
      "T5LayerNorm()\n",
      "************** 712\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 713\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 714\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 715\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 716\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 717\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 718\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 719\n",
      "T5LayerNorm()\n",
      "************** 720\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 721\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 722\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 723\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 724\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 725\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 726\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 727\n",
      "T5LayerNorm()\n",
      "************** 728\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 729\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 730\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 731\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 732\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 733\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 734\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 735\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 736\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 737\n",
      "T5LayerNorm()\n",
      "************** 738\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 739\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 740\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 741\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 742\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 743\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 744\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 745\n",
      "T5LayerNorm()\n",
      "************** 746\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 747\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 748\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 749\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 750\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 751\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 752\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 753\n",
      "T5LayerNorm()\n",
      "************** 754\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 755\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 756\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 757\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 758\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 759\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 760\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 761\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 762\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 763\n",
      "T5LayerNorm()\n",
      "************** 764\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 765\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 766\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 767\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 768\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 769\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 770\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 771\n",
      "T5LayerNorm()\n",
      "************** 772\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 773\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 774\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 775\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 776\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 777\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 778\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 779\n",
      "T5LayerNorm()\n",
      "************** 780\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 781\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 782\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 783\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 784\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 785\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 786\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 787\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 788\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 789\n",
      "T5LayerNorm()\n",
      "************** 790\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 791\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 792\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 793\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 794\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 795\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 796\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 797\n",
      "T5LayerNorm()\n",
      "************** 798\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 799\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 800\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 801\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 802\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 803\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 804\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 805\n",
      "T5LayerNorm()\n",
      "************** 806\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 807\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 808\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 809\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 810\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 811\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 812\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 813\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 814\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 815\n",
      "T5LayerNorm()\n",
      "************** 816\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 817\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 818\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 819\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 820\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 821\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 822\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 823\n",
      "T5LayerNorm()\n",
      "************** 824\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 825\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 826\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 827\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 828\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 829\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 830\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 831\n",
      "T5LayerNorm()\n",
      "************** 832\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 833\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 834\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 835\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 836\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 837\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 838\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 839\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 840\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 841\n",
      "T5LayerNorm()\n",
      "************** 842\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 843\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 844\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 845\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 846\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 847\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 848\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 849\n",
      "T5LayerNorm()\n",
      "************** 850\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 851\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 852\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 853\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 854\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 855\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 856\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 857\n",
      "T5LayerNorm()\n",
      "************** 858\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 859\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 860\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 861\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 862\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 863\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 864\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 865\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 866\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 867\n",
      "T5LayerNorm()\n",
      "************** 868\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 869\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 870\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 871\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 872\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 873\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 874\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 875\n",
      "T5LayerNorm()\n",
      "************** 876\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 877\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 878\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 879\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 880\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 881\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 882\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 883\n",
      "T5LayerNorm()\n",
      "************** 884\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 885\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 886\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 887\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 888\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 889\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 890\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 891\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 892\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 893\n",
      "T5LayerNorm()\n",
      "************** 894\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 895\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 896\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 897\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 898\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 899\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 900\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 901\n",
      "T5LayerNorm()\n",
      "************** 902\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 903\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 904\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 905\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 906\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 907\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 908\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 909\n",
      "T5LayerNorm()\n",
      "************** 910\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 911\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 912\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 913\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 914\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 915\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 916\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 917\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 918\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 919\n",
      "T5LayerNorm()\n",
      "************** 920\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 921\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 922\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 923\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 924\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 925\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 926\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 927\n",
      "T5LayerNorm()\n",
      "************** 928\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 929\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 930\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 931\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 932\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 933\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 934\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 935\n",
      "T5LayerNorm()\n",
      "************** 936\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 937\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 938\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 939\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 940\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 941\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 942\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 943\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 944\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 945\n",
      "T5LayerNorm()\n",
      "************** 946\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 947\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 948\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 949\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 950\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 951\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 952\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 953\n",
      "T5LayerNorm()\n",
      "************** 954\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 955\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 956\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 957\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 958\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 959\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 960\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 961\n",
      "T5LayerNorm()\n",
      "************** 962\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 963\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 964\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 965\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 966\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 967\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 968\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 969\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 970\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 971\n",
      "T5LayerNorm()\n",
      "************** 972\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 973\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 974\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 975\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 976\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 977\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 978\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 979\n",
      "T5LayerNorm()\n",
      "************** 980\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 981\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 982\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 983\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 984\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 985\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 986\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 987\n",
      "T5LayerNorm()\n",
      "************** 988\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 989\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 990\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 991\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 992\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 993\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 994\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 995\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 996\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 997\n",
      "T5LayerNorm()\n",
      "************** 998\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 999\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1000\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1001\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1002\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1003\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1004\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1005\n",
      "T5LayerNorm()\n",
      "************** 1006\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1007\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1008\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1009\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1010\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1011\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1012\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1013\n",
      "T5LayerNorm()\n",
      "************** 1014\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1015\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1016\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1017\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1018\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1019\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1020\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1021\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1022\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1023\n",
      "T5LayerNorm()\n",
      "************** 1024\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1025\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1026\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1027\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1028\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1029\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1030\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1031\n",
      "T5LayerNorm()\n",
      "************** 1032\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1033\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1034\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1035\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1036\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1037\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1038\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1039\n",
      "T5LayerNorm()\n",
      "************** 1040\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1041\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1042\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1043\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1044\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1045\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1046\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1047\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1048\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1049\n",
      "T5LayerNorm()\n",
      "************** 1050\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1051\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1052\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1053\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1054\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1055\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1056\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1057\n",
      "T5LayerNorm()\n",
      "************** 1058\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1059\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1060\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1061\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1062\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1063\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1064\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1065\n",
      "T5LayerNorm()\n",
      "************** 1066\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1067\n",
      "T5LayerNorm()\n",
      "************** 1068\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1069\n",
      "Linear(in_features=2048, out_features=38136, bias=False)\n",
      "************** 1070\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (relative_attention_bias): Embedding(32, 32)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
