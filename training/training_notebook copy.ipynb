{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from lib2to3.pgen2 import token\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from train import DataCollatorForMultipleChoice\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dataset_name'] = 'prompts2opts_001_sub.parquet.gzip'\n",
    "args['model_name_or_path'] = 'bigscience/T0_3B'\n",
    "args['output_dir'] = '/home/gikok/output'\n",
    "args['num_train_epochs'] = 1\n",
    "args['per_device_train_batch_size'] = 2\n",
    "args['per_device_eval_batch_size'] = 2\n",
    "args['freeze_encoder'] = True\n",
    "args['learning_rate'] = 10000\n",
    "args['parallelize'] = False\n",
    "args['seed'] = 42\n",
    "args['pad_to_max_length'] = True\n",
    "args['input_eos'] = False\n",
    "args['target_max_length'] = 256\n",
    "args['max_length'] = 512\n",
    "args['num_warmup_steps'] = 0\n",
    "args['debug'] = False\n",
    "args['lr_scheduler_type'] = 'linear'\n",
    "args['num_shots'] = None\n",
    "args['weight_decay'] = 0.01\n",
    "args['gradient_checkpoint'] = False\n",
    "args['gradient_accumulation_steps'] = 8\n",
    "args['max_train_steps'] = None\n",
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(2048):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new,i], 10000)\n",
    "        pdf = val/sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:]+bin[:-1])/2\n",
    "        new_tensor[-n_new:,i] = torch.tensor(np.interp(np.random.random(n_new), cdf, b))\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:,:] = new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/18/2022 10:10:27 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "08/18/2022 10:10:27 - WARNING - datasets.builder - Using custom data configuration data-1847ec1d8d9327ad\n",
      "08/18/2022 10:10:27 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-1847ec1d8d9327ad/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /home/gikok/.cache/huggingface/transformers/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /home/gikok/.cache/huggingface/transformers/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /home/gikok/.cache/huggingface/transformers/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /home/gikok/.cache/huggingface/transformers/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "100%|██████████| 97/97 [00:47<00:00,  2.03ba/s]\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "08/18/2022 10:12:12 - INFO - __main__ - ***** Running training *****\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Num training = 96576\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Num Epochs = 1\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "08/18/2022 10:12:12 - INFO - __main__ -   Total optimization steps = 6036\n",
      "  0%|          | 0/6036 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "set_seed(args['seed'])\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(\n",
    "    logging.INFO if accelerator.is_local_main_process else logging.ERROR\n",
    ")\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Handle the output directory creation\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(args['output_dir'], exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args['dataset_name'] is not None:\n",
    "    data_files = {\"train\": args['dataset_name'], \"test\": args['dataset_name']}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "    # raw_eval_dataset = load_dataset(\"data\", data_files=data_files, split=\"test\")\n",
    "else:\n",
    "    raise ValueError(\"Please specify `args['dataset_name`.\")\n",
    "\n",
    "# Trim a number of evaluation training\n",
    "if args['debug']:\n",
    "    raw_train_dataset = raw_train_dataset.select(\n",
    "        range(min(100, len(raw_train_dataset)))\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args['model_name_or_path']).to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'])\n",
    "\n",
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet(\"data/item_no_6k.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "tokenizer.add_tokens(items)\n",
    "\n",
    "# then resize embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# resample shared embedding and lm_head layer\n",
    "resample(model, 0, len(items))\n",
    "resample(model, -1, len(items))\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args['pad_to_max_length'] else False\n",
    "\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args['input_eos'],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "    answer_choices_texts = examples[\"options\"]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = [\n",
    "        tokenizer(\n",
    "            ans_choi,\n",
    "            padding=True,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "        )\n",
    "        for ans_choi in answer_choices_texts\n",
    "    ]\n",
    "\n",
    "    features = {\n",
    "        k: [\n",
    "            [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "            for idx, elem in enumerate(v)\n",
    "        ]\n",
    "        for k, v in tokenized_inputs.items()\n",
    "    }\n",
    "\n",
    "    features[\"labels\"] = [tokenized_targets[idx][\"input_ids\"] for idx in range(bs)]\n",
    "    features[\"labels_attention_mask\"] = [\n",
    "        tokenized_targets[idx][\"attention_mask\"] for idx in range(bs)\n",
    "    ]\n",
    "    features[\"targets\"] = [\n",
    "        answer_choices_texts[idx].index(t) for idx, t in enumerate(target_texts)\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    # eval_dataset = raw_eval_dataset.map(\n",
    "    #     preprocess_eval, batched=True, remove_columns=column_names\n",
    "    # )\n",
    "\n",
    "    if args['num_shots'] is not None:\n",
    "        sample_indices = random.sample(\n",
    "            range(0, len(raw_train_dataset)), k=args['num_shots']\n",
    "        )\n",
    "        raw_train_dataset = raw_train_dataset.select(sample_indices)\n",
    "    train_dataset = raw_train_dataset.map(\n",
    "        tokenize_train, batched=True\n",
    "    )\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "# Log a few random training:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.debug(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "# for index in random.sample(range(len(eval_dataset)), 3):\n",
    "#     logger.debug(f\"Sample {index} of the evaluation set: {eval_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args['per_device_train_batch_size'],\n",
    ")\n",
    "\n",
    "# if args['pad_to_max_length']:\n",
    "#     # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "#     # to tensors.\n",
    "#     eval_collator = default_data_collator\n",
    "# else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    # eval_collator = DataCollatorForMultipleChoice(\n",
    "    #     tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    # )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     eval_dataset,\n",
    "#     collate_fn=eval_collator,\n",
    "#     batch_size=args['per_device_eval_batch_size'],\n",
    "# )\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'])\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args['gradient_accumulation_steps']\n",
    ")\n",
    "if args['max_train_steps'] is None:\n",
    "    args['max_train_steps'] = args['num_train_epochs'] * num_update_steps_per_epoch\n",
    "else:\n",
    "    args['num_train_epochs'] = math.ceil(\n",
    "        args['max_train_steps'] / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args['lr_scheduler_type'],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args['num_warmup_steps'],\n",
    "    num_training_steps=args['max_train_steps'],\n",
    ")\n",
    "\n",
    "if args['parallelize']:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    assert num_gpus > 1, \"You need at least 2 GPUs to use `model.parallelize()`.\"\n",
    "    model.parallelize()\n",
    "    optimizer, train_dataloader = accelerator.prepare(\n",
    "        optimizer, train_dataloader\n",
    "    )\n",
    "else:\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "total_batch_size = (\n",
    "    args['per_device_train_batch_size']\n",
    "    * accelerator.num_processes\n",
    "    * args['gradient_accumulation_steps']\n",
    ")\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num training = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args['per_device_train_batch_size']}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {args['max_train_steps']}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(args['max_train_steps']), disable=not accelerator.is_local_main_process\n",
    ")\n",
    "global_steps = 0\n",
    "\n",
    "# how often trained model should be saved\n",
    "r = int(args['max_train_steps'] / 30)\n",
    "if args['gradient_checkpoint']:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = list(model.named_parameters())[0][1]\n",
    "lm_head = list(model.named_parameters())[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=0\n",
    "n_new = len(items)\n",
    "new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(new_tensor[:-n_new,i], 10000)\n",
    "    pdf = val/sum(val)\n",
    "    cdf = np.cumsum(pdf)\n",
    "    b = (bin[1:]+bin[:-1])/2\n",
    "    new_tensor[-n_new:,i] = torch.tensor(np.interp(np.random.random(n_new), cdf, b))\n",
    "data = list(model.named_parameters())[layer][1].data\n",
    "data[:,:] = new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=-1\n",
    "n_new = len(items)\n",
    "new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(new_tensor[:-n_new,i], 10000)\n",
    "    pdf = val/sum(val)\n",
    "    cdf = np.cumsum(pdf)\n",
    "    b = (bin[1:]+bin[:-1])/2\n",
    "    new_tensor[-n_new:,i] = torch.tensor(np.interp(np.random.random(n_new), cdf, b))\n",
    "data = list(model.named_parameters())[layer][1].data\n",
    "data[:,:] = new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  0.1436,   3.8750,   0.5352,  ...,  30.8750,   1.3281, -21.5000],\n",
       "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
       "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
       "        ...,\n",
       "        [ -0.3020,  -0.3723,   1.2172,  ...,   1.8101,   1.0143,  -0.7559],\n",
       "        [ -0.7375,   0.0380,   0.1171,  ...,   1.1145,  -0.9926,   1.0823],\n",
       "        [  0.4283,   0.9865,   0.6769,  ...,  -1.0338,  -0.2796,  -0.3376]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(2048):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new,i], 10000)\n",
    "        pdf = val/sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:]+bin[:-1])/2\n",
    "        new_tensor[-n_new:,i] = torch.tensor(np.interp(np.random.random(n_new), cdf, b))\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:,:] = new_tensor\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=0\n",
    "n_new = len(items)\n",
    "og = embeddings[:-n_new,:].detach().cpu()\n",
    "new = embeddings[-n_new:,:].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "og_vals = np.zeros((2048, 30))\n",
    "og_bins = np.zeros((2048, 30))\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(og[:,i], 30, normed=True)\n",
    "    og_vals[i] = val\n",
    "    og_bins[i] = (bin[1:]+bin[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "new_vals = np.zeros((2048, 30))\n",
    "new_bins = np.zeros((2048, 30))\n",
    "for i in range(2048):\n",
    "    val, bin = np.histogram(new[:,i], 30, normed=True)\n",
    "    new_vals[i] = val\n",
    "    new_bins[i] = (bin[1:]+bin[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2048):\n",
    "    plt.plot(og_bins[i], og_vals[i], c='blue', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(2048):\n",
    "    plt.plot(new_bins[i], new_vals[i], c='blue', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"if item_no is 10462236, which of the following is the correct name: 'hemlagad frying pan black' or 'frasera whiskey glass'?\",\n",
       " \"if name is hemlagad frying pan black, what item_no does it refer to? '60435537', '10462236' or '49454495'?\",\n",
       " \"query:'32cm fry pan'\\nthe query above returns item_no 10462236 as a result. what is its rank? 2, 9, 5, 3 or 10\",\n",
       " \"if name is hemlagad frying pan black, what item_no does it refer to? '20365094', '39290701', '40221047' or '10462236'?\",\n",
       " \"query:'hemlagad wok'\\nthe query above returns item_no 10462236 as a result. what is its rank? 10, 9, 3 or 7\",\n",
       " \"25 year guarantee. read about the terms in the guarantee brochure.,sink in stainless steel, a hygienic, strong and durable material that's easy to keep clean.,under the sink is a sound-absorbing material which reduces resonances in the metal when using the sink, thus lowering the sound level. . is the previous sentence a description of item_no 10462236. yes or no?\",\n",
       " \"query:'32 cm frying pan '\\nthe query above returns item_no 10462236 as a result. what is its rank? 1 or 8\",\n",
       " \"if item_no is 10462236, which of the following is the correct name: 'ringhult cover panel high gloss light grey', 'undvika multi latch white' or 'hemlagad frying pan black'?\",\n",
       " \"if item_no is 10462236, which of the following is the correct name: 'kallax shelving unit oak effect', 'hemlagad frying pan black', 'metod corner wall cabinet with carousel white askersund light ash effect' or 'platsa wardrobe w 6 doors fonnes white'?\",\n",
       " \"query:'pan non stick'\\ndoes the query above return item_no 10462236 as a result. yes or no?\",\n",
       " \"works well on all types of hobs, including induction hob.,with teflon® select, a hardwearing non-stick coating that allows cooking with little fat and makes cleaning easy.,made from aluminium, which spreads heat evenly and energy efficiently, and makes it easier to regulate heat so the food does not burn and stick.,two handles make it easy to lift the pan.,the pan's low weight makes it easy to handle when filled with food.,the pan has extra thick walls and base, which distribute the heat evenly and give good cooking results.,easy grip handle makes the pan easy to lift. is the previous sentence a description of item_no 10462236. yes or no?\",\n",
       " \"if name is hemlagad frying pan black, what item_no does it refer to? '20341569', '10462236', '59331499' or '39394548'?\",\n",
       " \"query:'hemlagad 32 cm'\\ndoes the query above return item_no 10462236 as a result. yes or no?\",\n",
       " \"query:'frying pan induction'\\ndoes the query above return item_no 10462236 as a result. yes or no?\",\n",
       " \"query:'non-stick frying pan'\\nthe query above returns item_no 10462236 as a result. what is its rank? 8 or 7\",\n",
       " \"query:'mugs and glasses'\\ndoes the query above return item_no 10462236 as a result. yes or no?\",\n",
       " 'askersund door is a modern blonde ash wood effect door, with a grain pattern that you can both see and feel.,melamine is very hardwearing, resistant to moisture, staining, scratching and impacts. it is easy to clean.,you can choose to mount the door on the right or left side.,25 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 10484257. yes or no?',\n",
       " \"if item_no is 10484257, which of the following is the correct name: 'ikea 365 frying pan stainless steel non stick coating', 'bjoerksta picture with frame shore in the dawn aluminium colour', 'metod maximera base cabinet with drawer door white askersund dark brown ash effect' or 'droenjoens desk organiser white'?\",\n",
       " \"query:'ikea frying pan'\\ndoes the query above return item_no 10484257 as a result. yes or no?\",\n",
       " \"query:'365 frying pan'\\ndoes the query above return item_no 10484257 as a result. yes or no?\",\n",
       " 'works well on all types of hobs, including induction hob.,with teflon® platinum plus, a very hardwearing non-stick coating that makes the pan suitable for everyday use.,the pan has a rounded interior, which makes stirring and beating easy.,non-stick coating allows frying with less fat.,made of stainless steel, which makes the pans durable and easy to clean.,thick base with one layer of aluminium between two layers of stainless steel. gives an even and energy-efficient heat, which reduces the risk of food burning and sticking.,you can cook food on a low heat and save energy because the stainless steel in the pan absorbs heat well and retains it for a long time.,frying pan with non-stick coating reduces the risk of food burning and sticking, and is easier to clean.,easy to hang up and store by using the hole in the handle. is the previous sentence a description of item_no 10484257. yes or no?',\n",
       " 'you can customise spacing as you need, because the shelves are adjustable.,you can choose to mount the door on the right or left side.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 10484257. yes or no?',\n",
       " \"query:'ikea 365 cookware'\\nthe query above returns item_no 10484257 as a result. what is its rank? 9, 4, 7 or 1\",\n",
       " \"if name is ikea 365 frying pan stainless steel non stick coating, what item_no does it refer to? '00408792', '10484257' or '10182189'?\",\n",
       " 'wire basket with handles. is the previous sentence a summary of item_no 10484257. yes or no?',\n",
       " \"query:'frying pan 32 cm'\\ndoes the query above return item_no 10484257 as a result. yes or no?\",\n",
       " 'works well on all types of hobs, including induction hob.,with teflon® platinum plus, a very hardwearing non-stick coating that makes the pan suitable for everyday use.,the pan has a rounded interior, which makes stirring and beating easy.,non-stick coating allows frying with less fat.,made of stainless steel, which makes the pans durable and easy to clean.,thick base with one layer of aluminium between two layers of stainless steel. gives an even and energy-efficient heat, which reduces the risk of food burning and sticking.,you can cook food on a low heat and save energy because the stainless steel in the pan absorbs heat well and retains it for a long time.,frying pan with non-stick coating reduces the risk of food burning and sticking, and is easier to clean.,easy to hang up and store by using the hole in the handle. is the previous sentence a description of item_no 10484257. yes or no?',\n",
       " \"if item_no is 10484257, which of the following is the correct name: 'ikea 365 frying pan stainless steel non stick coating', 'voxtorp drawer front dark grey', 'skydrag tradfri lighting kit white' or 'samla lid for box 55 130 l transparent'?\",\n",
       " 'frying pan. is the previous sentence a summary of item_no 10484257. yes or no?',\n",
       " \"if item_no is 10484257, which of the following is the correct name: 'ikea 365 frying pan stainless steel non stick coating', 'flekke day bed w 2 drawers 2 mattresses white vannareid firm', 'metod wall cabinet with shelves 2 doors white axstad dark grey' or 'voxtorp drawer front matt white'?\",\n",
       " \"query:'frying pans ikea 365 '\\ndoes the query above return item_no 10484257 as a result. yes or no?\",\n",
       " \"query:'frying pan 32'\\ndoes the query above return item_no 10484257 as a result. yes or no?\",\n",
       " \"query:'frying pan '\\ndoes the query above return item_no 00267706 as a result. yes or no?\",\n",
       " \"if name is kavalkad frying pan black, what item_no does it refer to? '20448952' or '00267706'?\",\n",
       " \"query:'pawn'\\ndoes the query above return item_no 00267706 as a result. yes or no?\",\n",
       " \"10 year guarantee. read about the terms in the guarantee brochure.,the clothes rail allows you to hang clothes forward-facing in a shallow, 35 cm wardrobe frame.,don't forget your hangers! we have all types and style options covered whether you are hanging your finest dress or your heaviest winter parka. is the previous sentence a description of item_no 00267706. yes or no?\",\n",
       " \"query:'non stick frying pans'\\nthe query above returns item_no 00267706 as a result. what is its rank? 2, 7 or 5\",\n",
       " \"query:'cavalkad'\\ndoes the query above return item_no 00267706 as a result. yes or no?\",\n",
       " 'frying pan. is the previous sentence a summary of item_no 00267706. yes or no?',\n",
       " \"query:'nonstick pans'\\ndoes the query above return item_no 00267706 as a result. yes or no?\",\n",
       " \"query:'pan non stick'\\ndoes the query above return item_no 00267706 as a result. yes or no?\",\n",
       " \"if item_no is 00267706, which of the following is the correct name: 'kavalkad frying pan black', 'kungsbacka door anthracite' or 'komplement pull out tray with divider white light grey'?\",\n",
       " \"query:'non-stick'\\nthe query above returns item_no 00267706 as a result. what is its rank? 3, 1, 3 or 7\",\n",
       " \"if name is kavalkad frying pan black, what item_no does it refer to? '70226053', '00267706', '99330214' or '99930937'?\",\n",
       " \"query:'fyring pan'\\nthe query above returns item_no 00267706 as a result. what is its rank? 3, 4, 1, 4 or 7\",\n",
       " \"if name is kavalkad frying pan black, what item_no does it refer to? '00267706', '80431897' or '79424299'?\",\n",
       " 'the deep plate’s simple, functional design is easy to coordinate with other colours, shapes or different types of glazing – and makes färgklar the perfect base for many occasions. is the previous sentence a description of item_no 00267706. yes or no?',\n",
       " \"if name is kavalkad frying pan black, what item_no does it refer to? '90219829', '00267706' or '19278253'?\",\n",
       " \"if name is klockren pan lid glass, what item_no does it refer to? '49457719', '30264301', '50402033' or '00459022'?\",\n",
       " 'fits frying pans with diameter 24 cm and 8 litre pots.,the glass lid allows you to monitor the contents of the pan during the cooking process. is the previous sentence a description of item_no 00459022. yes or no?',\n",
       " 'fits frying pans with diameter 24 cm and 8 litre pots.,the glass lid allows you to monitor the contents of the pan during the cooking process. is the previous sentence a description of item_no 00459022. yes or no?',\n",
       " \"if item_no is 00459022, which of the following is the correct name: 'jonaxel wardrobe combination white', 'klockren pan lid glass' or 'metod high cabinet with shelves white enkoeping white wood effect'?\",\n",
       " \"if name is klockren pan lid glass, what item_no does it refer to? '00459022' or '80308675'?\",\n",
       " 'keep track of important papers, letters and newspapers by sorting them on the inside of the cabinet door.,the cabinet can be used either with the included legs or be placed on the floor.,helps you keep track of small items like chargers, keys and wallets, or more bulky items like handbags and toys. is the previous sentence a description of item_no 00459022. yes or no?',\n",
       " 'fits frying pans with diameter 24 cm and 8 litre pots.,the glass lid allows you to monitor the contents of the pan during the cooking process. is the previous sentence a description of item_no 00459022. yes or no?',\n",
       " \"query:'fry'\\ndoes the query above return item_no 00459022 as a result. yes or no?\",\n",
       " \"query:'lid glass'\\ndoes the query above return item_no 00459022 as a result. yes or no?\",\n",
       " 'parasol with base. is the previous sentence a summary of item_no 00459022. yes or no?',\n",
       " 'fits frying pans with diameter 24 cm and 8 litre pots.,the glass lid allows you to monitor the contents of the pan during the cooking process. is the previous sentence a description of item_no 00459022. yes or no?',\n",
       " \"query:'pan lid, glass '\\nthe query above returns item_no 00459022 as a result. what is its rank? 4, 10 or 2\",\n",
       " 'pan lid. is the previous sentence a summary of item_no 00459022. yes or no?',\n",
       " \"query:'lids for 24 cm pan'\\ndoes the query above return item_no 00459022 as a result. yes or no?\",\n",
       " \"query:'frying pan and lid'\\ndoes the query above return item_no 00459022 as a result. yes or no?\",\n",
       " \"query:'pan lid 24'\\nthe query above returns item_no 00459022 as a result. what is its rank? 1, 4 or 7\",\n",
       " \"if name is tolerant wok black, what item_no does it refer to? '20440951', '70247319' or '09323675'?\",\n",
       " \"if item_no is 70247319, which of the following is the correct name: 'enhet mirror cabinet with 2 doors grey', 'maximera drawer high white', 'tolerant wok black' or 'kungsbacka cover panel matt white'?\",\n",
       " \"query:'non stick wok'\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " 'container. is the previous sentence a summary of item_no 70247319. yes or no?',\n",
       " \"if name is tolerant wok black, what item_no does it refer to? '00397307' or '70247319'?\",\n",
       " \"query:'cookware non stick'\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " \"query:'pan\\\\'\\nthe query above returns item_no 70247319 as a result. what is its rank? 8, 8 or 1\",\n",
       " \"query:'induction wok'\\nthe query above returns item_no 70247319 as a result. what is its rank? 4, 8 or 4\",\n",
       " \"query:'induction kadai '\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " \"query:'wok tolerant'\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " \"if name is tolerant wok black, what item_no does it refer to? '39454523', '70247319' or '89221767'?\",\n",
       " 'wok. is the previous sentence a summary of item_no 70247319. yes or no?',\n",
       " \"query:'tolerant - wok'\\nthe query above returns item_no 70247319 as a result. what is its rank? 2, 7, 3, 1 or 4\",\n",
       " \"query:'woke burner gas hob '\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " \"query:'b91 2au'\\ndoes the query above return item_no 70247319 as a result. yes or no?\",\n",
       " 'a fresh scent of mint and green leaves with hints of cedar. ,suitable when you want to create an energetic atmosphere at home with inspiration from wind and water. ,the candles have the same beautiful colour and pleasant scent during their entire burning time, as they are coloured through and scented through.,at least 50% of the wax in this product is renewable plant-based wax. is the previous sentence a description of item_no 70247319. yes or no?',\n",
       " 'you can customise spacing as you need, because the shelf is adjustable.,you can choose to mount the door on the right or left side.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 50451052. yes or no?',\n",
       " \"query:'pan stainless'\\ndoes the query above return item_no 50451052 as a result. yes or no?\",\n",
       " \"query:'copper pans'\\ndoes the query above return item_no 50451052 as a result. yes or no?\",\n",
       " \"if name is oumbaerlig frying pan copper color, what item_no does it refer to? '69175974' or '50451052'?\",\n",
       " \"query:'oumbärlig 28cm'\\ndoes the query above return item_no 50451052 as a result. yes or no?\",\n",
       " \"query:'pan induction hob'\\ndoes the query above return item_no 50451052 as a result. yes or no?\",\n",
       " \"query:'copper based pots'\\nthe query above returns item_no 50451052 as a result. what is its rank? 4, 7 or 1\",\n",
       " \"query:'frying pan oumbärlig'\\nthe query above returns item_no 50451052 as a result. what is its rank? 4, 1 or 7\",\n",
       " \"if item_no is 50451052, which of the following is the correct name: 'metod base cabinet frame white' or 'oumbaerlig frying pan copper color'?\",\n",
       " \"query:'fry pan non stick'\\nthe query above returns item_no 50451052 as a result. what is its rank? 6, 10, 1 or 7\",\n",
       " \"query:'oumb%c3%a4rlig '\\nthe query above returns item_no 50451052 as a result. what is its rank? 6, 2 or 3\",\n",
       " \"query:'28 pan lid'\\nthe query above returns item_no 50451052 as a result. what is its rank? 6, 2 or 6\",\n",
       " 'a simple unit can be enough storage for a limited space or the foundation for a larger storage solution if your needs change. is the previous sentence a description of item_no 50451052. yes or no?',\n",
       " \"if item_no is 50451052, which of the following is the correct name: 'vinter 2021 tealight holder white' or 'oumbaerlig frying pan copper color'?\",\n",
       " \"query:'fries'\\ndoes the query above return item_no 50451052 as a result. yes or no?\",\n",
       " \"query:'frying pan induction hob'\\nthe query above returns item_no 50451052 as a result. what is its rank? 4, 3 or 1\",\n",
       " \"query:'pans and pots 25 cm'\\ndoes the query above return item_no 40513102 as a result. yes or no?\",\n",
       " 'fabric. is the previous sentence a summary of item_no 40513102. yes or no?',\n",
       " \"query:'frying pan 28cms'\\nthe query above returns item_no 40513102 as a result. what is its rank? 4, 1, 10 or 3\",\n",
       " 'frying pan with teflon® professional, our best non-stick coating; for cooks that place high demands on your pans.,the pots and pan are suitable for all types of cooktops, including induction cooktop.,non-stick lining allows frying with less fat. is the previous sentence a description of item_no 40513102. yes or no?',\n",
       " 'frying pan with teflon® professional, our best non-stick coating; for cooks that place high demands on your pans.,the pots and pan are suitable for all types of cooktops, including induction cooktop.,non-stick lining allows frying with less fat. is the previous sentence a description of item_no 40513102. yes or no?',\n",
       " 'frying pan. is the previous sentence a summary of item_no 40513102. yes or no?',\n",
       " \"query:'28cm frying pan'\\ndoes the query above return item_no 40513102 as a result. yes or no?\",\n",
       " \"if name is hemkomst frying pan stainless steel non stick coating, what item_no does it refer to? '10399886' or '40513102'?\",\n",
       " \"query:'fryng pan'\\ndoes the query above return item_no 40513102 as a result. yes or no?\",\n",
       " 'frame. is the previous sentence a summary of item_no 40513102. yes or no?',\n",
       " \"query:'frying up and'\\ndoes the query above return item_no 40513102 as a result. yes or no?\",\n",
       " \"query:'hemkost'\\nthe query above returns item_no 40513102 as a result. what is its rank? 10, 5 or 3\",\n",
       " \"query:'fryng pan'\\nthe query above returns item_no 40513102 as a result. what is its rank? 2, 7 or 4\",\n",
       " 'frying pan with teflon® professional, our best non-stick coating; for cooks that place high demands on your pans.,the pots and pan are suitable for all types of cooktops, including induction cooktop.,non-stick lining allows frying with less fat. is the previous sentence a description of item_no 40513102. yes or no?',\n",
       " \"query:'pan 28cm'\\nthe query above returns item_no 40513102 as a result. what is its rank? 10, 3, 6 or 6\",\n",
       " \"query:'40513102'\\ndoes the query above return item_no 40513102 as a result. yes or no?\",\n",
       " \"if name is vonsbaek rug low pile green, what item_no does it refer to? '60450033' or '00291626'?\",\n",
       " \"query:'locker'\\nthe query above returns item_no 60450033 as a result. what is its rank? 4, 10, 10, 3 or 6\",\n",
       " \"if item_no is 60450033, which of the following is the correct name: 'eket wall mounted shelving unit red brown', 'vonsbaek rug low pile green' or 'identitet 16 piece cutlery set stainless steel'?\",\n",
       " \"if item_no is 60450033, which of the following is the correct name: 'espevaer slatted mattress base with legs dark grey', 'vonsbaek rug low pile green', 'enhet bc w shlf door white' or 'lerhyttan front for dishwasher light grey'?\",\n",
       " \"if name is vonsbaek rug low pile green, what item_no does it refer to? '79395739', '09388110', '60450033' or '40245411'?\",\n",
       " \"query:'170 x 230'\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " \"query:'teal rugs large'\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " \"if name is vonsbaek rug low pile green, what item_no does it refer to? '90466155' or '60450033'?\",\n",
       " \"query:'romdrup '\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " \"query:'604.500.33 '\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " 'the pattern looks faded and worn, which gives the rug a vintage look.,the vintage expression and colours fit in both modern and traditional furniture.,chairs are easy to move and won’t leave marks thanks to the very low pile. this makes it a suitable choice for your dining or living room.,a perfect companion for all types of flooring, even those with underfloor heating. ,durable and will not shed since the rug is made of polypropylene. ,the rug will last a long time since it is stain-resistant and easy to care for. is the previous sentence a description of item_no 60450033. yes or no?',\n",
       " \"query:'rug low patterned'\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 60450033. yes or no?',\n",
       " \"query:'pattern rug'\\nthe query above returns item_no 60450033 as a result. what is its rank? 7, 4, 4, 2 or 5\",\n",
       " \"query:'vonsbäk rug low pile'\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " \"query:'pattern carpets 170x230'\\ndoes the query above return item_no 60450033 as a result. yes or no?\",\n",
       " 'the pattern looks faded and worn, which gives the rug a vintage look.,the vintage expression and colours fit in both modern and traditional furniture.,chairs are easy to move and won’t leave marks thanks to the very low pile. this makes it a suitable choice for your dining or living room.,a perfect companion for all types of flooring, even those with underfloor heating. ,durable and will not shed since the rug is made of polypropylene. ,the rug will last a long time since it is stain-resistant and easy to care for. is the previous sentence a description of item_no 30449456. yes or no?',\n",
       " \"query:'covoare'\\nthe query above returns item_no 30449456 as a result. what is its rank? 7, 9, 8, 1 or 2\",\n",
       " \"query:'multi colour rugs'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"if item_no is 30449456, which of the following is the correct name: 'vonsbaek rug low pile green' or 'besta storage combination with drawers white stained oak effect selsviken stubbarp high gloss beige'?\",\n",
       " \"query:'green large rugs'\\nthe query above returns item_no 30449456 as a result. what is its rank? 6, 4, 1 or 6\",\n",
       " \"query:'room carpet'\\nthe query above returns item_no 30449456 as a result. what is its rank? 6, 8, 6, 7 or 6\",\n",
       " \"query:'133x195 '\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'rugs green'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"if name is vonsbaek rug low pile green, what item_no does it refer to? '30449456' or '79418463'?\",\n",
       " \"query:'pina coffee table'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'big jute rugs'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"if name is vonsbaek rug low pile green, what item_no does it refer to? '30449456' or '69047646'?\",\n",
       " \"query:'large dining table'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'traditional carpet'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'toilet bowl rug'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'hanging tie storage'\\ndoes the query above return item_no 30449456 as a result. yes or no?\",\n",
       " \"query:'rug under mats'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'carpet offwhite'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'off qhite rug handmade'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'trug'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'large '\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " 'rug. is the previous sentence a summary of item_no 80442527. yes or no?',\n",
       " \"query:'large wool rug'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'wool rug'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'underlay for rug'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"query:'wool rug large'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " \"if name is hjortsvang rug handmade off white, what item_no does it refer to? '80442527' or '09437436'?\",\n",
       " \"query:'wool carpet'\\nthe query above returns item_no 80442527 as a result. what is its rank? 1, 10 or 10\",\n",
       " \"query:'jortsvang'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " 'vacuum flask. is the previous sentence a summary of item_no 80442527. yes or no?',\n",
       " 'rug. is the previous sentence a summary of item_no 80442527. yes or no?',\n",
       " \"query:'rug under mats'\\ndoes the query above return item_no 80442527 as a result. yes or no?\",\n",
       " 'the backing keeps the door mat firmly in place and reduces the risk of slipping.,easy to keep clean - just vacuum or shake the rug. is the previous sentence a description of item_no 90507924. yes or no?',\n",
       " \"query:'rug natural'\\nthe query above returns item_no 90507924 as a result. what is its rank? 8, 9, 2, 3 or 10\",\n",
       " \"the beauty is in the uniqueness - this rug is handwoven making it one of a kind.,the soft woven pebbles create a bold, chunky character.,each rug is unique with a variety of neutral-coloured yarns that have been added by hand into the weave.,this rug was made by skilled craftspeople with good working conditions and fair wages at organised weaving centres in india and bangladesh.,the rug is made of wool so it's naturally soil-repellent and very durable. ,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 90507924. yes or no?\",\n",
       " \"if item_no is 90507924, which of the following is the correct name: 'kallax shelving unit with doors white stained oak effect', 'gunva cushion cover light blue', 'raecka curtain rod white' or 'svaerdborg rug flatwoven handmade off white multicolour'?\",\n",
       " \"query:'rugs large flatwoven'\\ndoes the query above return item_no 90507924 as a result. yes or no?\",\n",
       " \"query:'rugs large flatwoven'\\nthe query above returns item_no 90507924 as a result. what is its rank? 3, 4, 2, 3 or 6\",\n",
       " \"query:'rugs large flat'\\ndoes the query above return item_no 90507924 as a result. yes or no?\",\n",
       " \"if item_no is 90507924, which of the following is the correct name: 'besta storage combination with doors white glassvik stubbarp white clear glass', 'svaerdborg rug flatwoven handmade off white multicolour' or 'beraekna vase light grey'?\",\n",
       " \"query:'large white rugs'\\nthe query above returns item_no 90507924 as a result. what is its rank? 4, 5 or 1\",\n",
       " \"query:'skarasso'\\ndoes the query above return item_no 90507924 as a result. yes or no?\",\n",
       " \"query:'large rugs flatweave'\\ndoes the query above return item_no 90507924 as a result. yes or no?\",\n",
       " 'corner wall cab w shelves/glass dr. is the previous sentence a summary of item_no 90507924. yes or no?',\n",
       " \"if item_no is 90507924, which of the following is the correct name: 'besta storage combination w doors drawers white lappviken stubbarp white', 'besta storage combination with drawers oak effect selsviken high gloss white' or 'svaerdborg rug flatwoven handmade off white multicolour'?\",\n",
       " 'tv storage combination/glass doors. is the previous sentence a summary of item_no 90507924. yes or no?',\n",
       " \"query:'wool carpet'\\ndoes the query above return item_no 90507924 as a result. yes or no?\",\n",
       " \"if item_no is 90507924, which of the following is the correct name: 'istad resealable bag red', 'bekant desk combination black stained ash veneer black' or 'svaerdborg rug flatwoven handmade off white multicolour'?\",\n",
       " 'artificial flower. is the previous sentence a summary of item_no 90372331. yes or no?',\n",
       " \"query:'nödebo rug low pile'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"query:'ottoman wooden bed frames'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " 'you can always adapt or complete this open storage solution as needed. maybe the combination we’ve suggested is perfect for you, or you can easily create your own.,you can combine open and closed storage - shelves for your favorite things and drawers for the things you want to store away.,adjustable shelves and clothes rails make it easy for you to customize the space according to your needs.,you choose if you want to place the open storage solution against a wall or use it as a room divider since the post attaches to the ceiling.,drawers with integrated dampers close slowly, silently and softly.,height adjustable 87⅜\"-137¾\"- allows you to utilize the entire ceiling height. is the previous sentence a description of item_no 90372331. yes or no?',\n",
       " 'handwoven by skilled craftspeople, each one is unique. made in india in organized weaving centers with good working conditions and fair wages.,the rug is made of wool so it’s naturally soil-repellent and very durable.,the dense, thick pile dampens sound and provides a soft surface to walk on. is the previous sentence a description of item_no 90372331. yes or no?',\n",
       " \"query:'large teal rug'\\nthe query above returns item_no 90372331 as a result. what is its rank? 4, 6, 6 or 1\",\n",
       " \"query:'wool rugs large'\\nthe query above returns item_no 90372331 as a result. what is its rank? 8, 6, 7 or 3\",\n",
       " \"query:'nödebo rugs'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"query:'medium rugs'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"query:'carpets green '\\nthe query above returns item_no 90372331 as a result. what is its rank? 8, 1 or 10\",\n",
       " \"query:'green rugs '\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"query:'large low pile rug'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"query:'n%c3%b6dbeo'\\ndoes the query above return item_no 90372331 as a result. yes or no?\",\n",
       " \"if item_no is 90372331, which of the following is the correct name: 'noedebo rug low pile handmade green', 'pax forsand wardrobe combination white stained oak effect', 'aepplaroe table 2 folding chairs outdoor brown stained froesoen duvholmen beige' or 'tretur block out roller blind white'?\",\n",
       " \"query:'green rug low pile'\\nthe query above returns item_no 90372331 as a result. what is its rank? 7, 2, 5 or 1\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90372331. yes or no?',\n",
       " \"query:'orange'\\ndoes the query above return item_no 30487575 as a result. yes or no?\",\n",
       " \"query:'table carpet'\\nthe query above returns item_no 30487575 as a result. what is its rank? 1, 9, 6, 8 or 2\",\n",
       " \"if item_no is 30487575, which of the following is the correct name: 'eket cabinet combination with feet white light grey' or 'morum rug flatwoven in outdoor rust'?\",\n",
       " \"query:'orange rugs '\\nthe query above returns item_no 30487575 as a result. what is its rank? 10, 2, 4, 3 or 8\",\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,the rug is perfect for outdoor use because it’s water resistant and made for easy care.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30487575. yes or no?',\n",
       " \"query:'200 300 rug'\\nthe query above returns item_no 30487575 as a result. what is its rank? 4 or 8\",\n",
       " \"query:'large rug 200 x 300'\\ndoes the query above return item_no 30487575 as a result. yes or no?\",\n",
       " \"query:'indoor outdoor flatwoven'\\ndoes the query above return item_no 30487575 as a result. yes or no?\",\n",
       " \"query:'rugs 200x300'\\ndoes the query above return item_no 30487575 as a result. yes or no?\",\n",
       " \"query:'orange rugs'\\nthe query above returns item_no 30487575 as a result. what is its rank? 6, 1, 4, 10 or 5\",\n",
       " \"distribute charcoal or briquettes in the grill to create a hot zone for direct cooking or a cooler zone for heat retention – perfect when you want to grill food which has different cooking times.,heat-insulated handles in stainless steel.,the ash tray can be pulled out, so it’s easy to empty the ashes once you have finished grilling.,the built-in thermometer on the hood helps you check the temperature during grilling – without having to lift the hood.,the cooking grate is durable and rustproof, as it is made of high-grade stainless steel.,on the shelves there is plenty of space for plates, glasses, utensils, oils and other things you need for the barbeque party.,the surface of the sink unit is made of stainless steel – a hygienic, hardwearing and durable material that is resistant to rust and easy to clean.,if you want running water outdoors, just connect the sink unit to a garden hose.,the movable shelf is made of steel and has a durable, powder-coated surface that is easy to wipe dry.,this kitchen island gives you extra storage space and a good work surface where you can prepare food before it's time to put it on the grill. is the previous sentence a description of item_no 30487575. yes or no?\",\n",
       " \"query:'rust colour rug'\\ndoes the query above return item_no 30487575 as a result. yes or no?\",\n",
       " 'door. is the previous sentence a summary of item_no 30487575. yes or no?',\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,the rug is perfect for outdoor use because it’s water resistant and made for easy care.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30487575. yes or no?',\n",
       " \"query:'orange rugs'\\nthe query above returns item_no 30487575 as a result. what is its rank? 1 or 6\",\n",
       " \"if name is morum rug flatwoven in outdoor rust, what item_no does it refer to? '30487575', '99424156' or '20452294'?\",\n",
       " \"if item_no is 20487571, which of the following is the correct name: 'fantastisk paper napkin dark red', 'kungsbacka door anthracite' or 'morum rug flatwoven in outdoor light blue'?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 20487571. yes or no?',\n",
       " \"query:'medium rugs in blue'\\nthe query above returns item_no 20487571 as a result. what is its rank? 1, 2, 10, 5 or 2\",\n",
       " \"if name is morum rug flatwoven in outdoor light blue, what item_no does it refer to? '89471756', '20487571', '29443182' or '60403881'?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 20487571. yes or no?',\n",
       " \"query:'rugs outdoor'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " \"query:'teal rug '\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 20487571. yes or no?',\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,the rug is perfect for outdoor use because it’s water resistant and made for easy care.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 20487571. yes or no?',\n",
       " \"query:'diningroom'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " \"query:'200 x 300 large rugs'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " \"query:'blue rug flatwoven'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " \"query:'rugs in/outdoor'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " \"query:'putdoor rug'\\ndoes the query above return item_no 20487571 as a result. yes or no?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 20487571. yes or no?',\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 20487571. yes or no?',\n",
       " \"query:'morum in/outdoor rug flat woven rug'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " '10 year guarantee. read about the terms in the guarantee brochure.,if you want to organise inside you can complement with interior organisers from the komplement series.,adjustable feet make it possible to compensate any irregularities in the floor. is the previous sentence a description of item_no 80198295. yes or no?',\n",
       " 'drawer front. is the previous sentence a summary of item_no 80198295. yes or no?',\n",
       " \"query:'karpet'\\nthe query above returns item_no 80198295 as a result. what is its rank? 2 or 4\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 80198295. yes or no?',\n",
       " \"query:'xl rugs'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " 'it is easy to carry the bag to and from the washing/drying room using the short or long handles. perfect for taking the laundry to laundromats or shared laundry rooms.,the plastic coating on the inside protects against moisture.,place the bag freestanding in a room, store it in a wardrobe or hang it on a hook.,soft and easy to fold and store when not in use.,drawstring closure hides the laundry and keeps dust out. it also makes it possible to fill just a little more wash at the top of the bag.,the laundry airs during storage thanks to the small gap that is created when the bag is closed.,it is easy to carry the bag to and from the washing machine using the short or long handles.,the plastic interior makes the bag easy to wipe dry and also more stable. is the previous sentence a description of item_no 80198295. yes or no?',\n",
       " \"if item_no is 80198295, which of the following is the correct name: 'morum rug flatwoven in outdoor beige', 'metod base cabinet f sink w 2 doors front black axstad dark grey', 'boaxel lagkapten shelving unit with table top white' or 'enhet wall cb w 2 shlvs door grey oak effect'?\",\n",
       " \"if item_no is 80198295, which of the following is the correct name: 'haelsa steel vacuum flask beige' or 'morum rug flatwoven in outdoor beige'?\",\n",
       " \"query:'large indoor%2foutdoor rugs'\\nthe query above returns item_no 80198295 as a result. what is its rank? 1, 6, 2, 4 or 4\",\n",
       " \"query:'rig'\\nthe query above returns item_no 80198295 as a result. what is its rank? 2, 7, 5 or 6\",\n",
       " \"query:'sorop rug'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " \"query:'160x230 rug'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " \"query:'area rug'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " \"10 year guarantee. read about the terms in the guarantee brochure.,you can easily adapt this ready-made pax/komplement combination to suit your needs and taste using the pax planning tool.,adjustable feet make it possible to compensate any irregularities in the floor.,with the pax planner, you can easily complete your combination with integrated lighting. after choosing the light source, the planner will work out which accessories you need to complete the solution.,thanks to the smart divider, there's space for both long and short clothes on a hanger, without loosing the storage space underneath the short ones.,the mesh basket allows air to circulate and is perfect for storing your folded clothes, socks or accessories. is the previous sentence a description of item_no 80198295. yes or no?\",\n",
       " \"query:'flat rug'\\ndoes the query above return item_no 80198295 as a result. yes or no?\",\n",
       " 'there’s always something new to notice and treasure thanks to the detailed pattern.,the pattern and colours are easy to coordinate with many different styles.,the dense, thick pile dampens sound and provides a soft surface to walk on.,the rug will last a long time since it is stain-resistant and easy to care for. is the previous sentence a description of item_no 50449455. yes or no?',\n",
       " \"query:'persoan rugs'\\ndoes the query above return item_no 50449455 as a result. yes or no?\",\n",
       " \"query:'dining rustic'\\ndoes the query above return item_no 50449455 as a result. yes or no?\",\n",
       " \"query:'red rugs'\\ndoes the query above return item_no 50449455 as a result. yes or no?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '30064849', '09457844', '50449455' or '60361517'?\",\n",
       " \"if item_no is 50449455, which of the following is the correct name: 'metod high cabinet for fridge freezer white bodarp grey green' or 'vedbaek rug low pile multicolour'?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '90371765', '10285328' or '50449455'?\",\n",
       " 'there’s always something new to notice and treasure thanks to the detailed pattern.,the pattern and colours are easy to coordinate with many different styles.,the dense, thick pile dampens sound and provides a soft surface to walk on.,the rug will last a long time since it is stain-resistant and easy to care for. is the previous sentence a description of item_no 50449455. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 50449455. yes or no?',\n",
       " \"query:'rug vedbäk'\\ndoes the query above return item_no 50449455 as a result. yes or no?\",\n",
       " \"query:'rug patterned'\\nthe query above returns item_no 50449455 as a result. what is its rank? 8, 2, 4, 5 or 9\",\n",
       " \"query:'rugs '\\ndoes the query above return item_no 50449455 as a result. yes or no?\",\n",
       " \"if item_no is 50449455, which of the following is the correct name: 'vedbaek rug low pile multicolour' or 'droemsk plant pot white'?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '50478023' or '50449455'?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '40365762' or '50449455'?\",\n",
       " \"query:'200x300 cm rugs'\\nthe query above returns item_no 50449455 as a result. what is its rank? 7, 3, 5, 2 or 1\",\n",
       " \"query:'cendelin '\\ndoes the query above return item_no 80450046 as a result. yes or no?\",\n",
       " 'the fabric is made of recycled polyester. using waste as a resource takes us one step closer to a more sustainable future. ,helps you organise socks, belts and jewellery in your wardrobe or chest of drawers.,it’s easy to pull out no matter how you position it, because the box has handles on both its short and long side.,when you don’t need the box and want to save space, simply open the zipper in the bottom and fold it flat. is the previous sentence a description of item_no 80450046. yes or no?',\n",
       " \"query:'200 by 300 blue rugs'\\ndoes the query above return item_no 80450046 as a result. yes or no?\",\n",
       " \"if item_no is 80450046, which of the following is the correct name: 'vedbaek rug low pile multicolour' or 'billy oxberg bookcase oak veneer'?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '80450046', '20486784' or '00457711'?\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '50221023', '80450046' or '20339284'?\",\n",
       " \"query:'pop up wardrobe'\\nthe query above returns item_no 80450046 as a result. what is its rank? 8, 8, 1, 4 or 4\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '80450046', '09216595', '00405213' or '59388061'?\",\n",
       " \"query:'rugs large 200 x 300'\\ndoes the query above return item_no 80450046 as a result. yes or no?\",\n",
       " \"query:'red rugs'\\nthe query above returns item_no 80450046 as a result. what is its rank? 5, 1 or 2\",\n",
       " \"if name is vedbaek rug low pile multicolour, what item_no does it refer to? '09156624' or '80450046'?\",\n",
       " \"query:'children%e2%80%99s rug'\\ndoes the query above return item_no 80450046 as a result. yes or no?\",\n",
       " \"query:'rugs 170x200'\\nthe query above returns item_no 80450046 as a result. what is its rank? 7, 2, 10, 10 or 1\",\n",
       " \"query:'red rug'\\nthe query above returns item_no 80450046 as a result. what is its rank? 8, 5, 8 or 1\",\n",
       " \"every table is unique, with varying grain pattern and natural colour shifts that are part of the charm of wood.,oak is an exceedingly strong and durable hardwood with a prominent grain. it darkens beautifully with age acquiring a golden-brown undertone.,table with a top layer of solid wood, a hardwearing natural material that can be sanded and surface treated when required.,the table has a full plank design that gives it an authentic plank expression with a genuine wood feeling.,the plank expression is enhanced by the design on the edges.,this table has been tested against our strictest standards for stability, durability and safety to withstand everyday use in your home for years.,you sit comfortably thanks to the restful flexibility of the scooped seat and shaped back.,the self-adjusting plastic feet add stability to the chair.,you can stack the chairs, so they take less space when you're not using them. is the previous sentence a description of item_no 80450046. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 80450046. yes or no?',\n",
       " \"query:'rugs shaggy'\\nthe query above returns item_no 20487566 as a result. what is its rank? 2, 6 or 8\",\n",
       " 'wall cab horizo 2 doors w push-open. is the previous sentence a summary of item_no 20487566. yes or no?',\n",
       " \"query:'200 x 300 cm rug'\\ndoes the query above return item_no 20487566 as a result. yes or no?\",\n",
       " \"query:'pink rugs large'\\nthe query above returns item_no 20487566 as a result. what is its rank? 9, 6, 2, 8 or 1\",\n",
       " \"if name is vollerslev rug high pile pale pink, what item_no does it refer to? '40299244', '49139939', '30221019' or '20487566'?\",\n",
       " \"query:'pink products'\\ndoes the query above return item_no 20487566 as a result. yes or no?\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 20487566. yes or no?',\n",
       " \"query:'vollerslev 129'\\nthe query above returns item_no 20487566 as a result. what is its rank? 1 or 5\",\n",
       " 'extra thick and comfy under your feet.,different yarn thicknesses create a varied texture.,the high pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 20487566. yes or no?',\n",
       " \"query:'extra large rugs'\\ndoes the query above return item_no 20487566 as a result. yes or no?\",\n",
       " \"if name is vollerslev rug high pile pale pink, what item_no does it refer to? '20487566', '09400522', '30435548' or '69468142'?\",\n",
       " 'you can customise spacing as you need, because the shelf is adjustable.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 20487566. yes or no?',\n",
       " \"if item_no is 20487566, which of the following is the correct name: 'pinnarp custom made worktop walnut veneer' or 'vollerslev rug high pile pale pink'?\",\n",
       " \"if item_no is 20487566, which of the following is the correct name: 'storjorm mirror with integrated lighting white', 'vollerslev rug high pile pale pink' or 'voxtorp drawer front matt white'?\",\n",
       " \"query:'200cm 300cm rugs'\\nthe query above returns item_no 20487566 as a result. what is its rank? 2, 8, 9 or 3\",\n",
       " \"if name is vollerslev rug high pile pale pink, what item_no does it refer to? '00499160' or '20487566'?\",\n",
       " 'extra thick and comfy under your feet.,different yarn thicknesses create a varied texture.,the high pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30492572. yes or no?',\n",
       " \"query:'medium size rugs'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"the drawers close slowly, quietly and softly thanks to the built-in dampers.,you can view and access what's inside, as the drawers can be pulled out all the way .,smooth-running drawers with stop.,drawers are self-closing the last few centimetres.,sturdy frame construction, 18 mm thick. is the previous sentence a description of item_no 30492572. yes or no?\",\n",
       " 'extra thick and comfy under your feet.,different yarn thicknesses create a varied texture.,the high pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30492572. yes or no?',\n",
       " \"query:'rug 133*195cm'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'rug size 200x140 '\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 30492572. yes or no?',\n",
       " \"query:'soft fluffy rugs '\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'cosy rug'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'shaggy white rug'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'rug shaggy large'\\nthe query above returns item_no 30492572 as a result. what is its rank? 8 or 1\",\n",
       " \"if name is vollerslev rug high pile white, what item_no does it refer to? '20483988', '89334043' or '30492572'?\",\n",
       " \"if name is vollerslev rug high pile white, what item_no does it refer to? '50464610', '30492572' or '50527603'?\",\n",
       " \"query:'rugs large shaggy'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'high pile rug large'\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'vollerslev '\\ndoes the query above return item_no 30492572 as a result. yes or no?\",\n",
       " \"query:'rugs large thick pile'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " \"query:'sofa blue colour'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " \"query:'pink high pile rug'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " 'food container with lid. is the previous sentence a summary of item_no 90492550. yes or no?',\n",
       " \"query:'rug pink '\\nthe query above returns item_no 90492550 as a result. what is its rank? 3, 5, 6 or 7\",\n",
       " \"query:'rugs pink high pile'\\nthe query above returns item_no 90492550 as a result. what is its rank? 1, 4, 9, 4 or 8\",\n",
       " \"query:'sofa pink'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " \"query:'large pink rug'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " 'chest of 3 drawers. is the previous sentence a summary of item_no 90492550. yes or no?',\n",
       " \"query:'pink ikea rug'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " 'adjustable shelves; adapt space between shelves according to your needs.,a simple unit can be enough storage for a limited space or the foundation for a larger storage solution if your needs change.,surface made from natural wood veneer. is the previous sentence a description of item_no 90492550. yes or no?',\n",
       " 'extra thick and comfy under your feet.,different yarn thicknesses create a varied texture.,the high pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 90492550. yes or no?',\n",
       " \"query:'large pink rug'\\ndoes the query above return item_no 90492550 as a result. yes or no?\",\n",
       " \"query:'textured rugs'\\nthe query above returns item_no 90492550 as a result. what is its rank? 3 or 1\",\n",
       " \"query:'large play mat'\\nthe query above returns item_no 90492550 as a result. what is its rank? 8, 8, 9, 2 or 5\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 90492550. yes or no?',\n",
       " \"query:'rugs blue '\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"query:'antislip mat'\\nthe query above returns item_no 10492549 as a result. what is its rank? 2 or 9\",\n",
       " \"query:'rugs large blue'\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"query:'large rug high pile'\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"query:'rugs 300 x 300'\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"query:'160x230 rug'\\nthe query above returns item_no 10492549 as a result. what is its rank? 5, 7, 6, 4 or 1\",\n",
       " \"query:'carpet 200'\\nthe query above returns item_no 10492549 as a result. what is its rank? 3, 5 or 10\",\n",
       " 'extra thick and comfy under your feet.,different yarn thicknesses create a varied texture.,the high pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 10492549. yes or no?',\n",
       " 'throw. is the previous sentence a summary of item_no 10492549. yes or no?',\n",
       " \"query:'dark blue rugs'\\nthe query above returns item_no 10492549 as a result. what is its rank? 7, 4, 3, 5 or 7\",\n",
       " \"if name is vollerslev rug high pile dark blue, what item_no does it refer to? '80430534', '40292902', '50489842' or '10492549'?\",\n",
       " \"query:'large shaggy rugs'\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"if item_no is 10492549, which of the following is the correct name: 'utrusta shelf wood effect black', 'trattviva bedspread dark pink', 'malm desk with pull out panel white' or 'vollerslev rug high pile dark blue'?\",\n",
       " \"query:'carpet  large'\\nthe query above returns item_no 10492549 as a result. what is its rank? 6, 3 or 1\",\n",
       " 'cover panel. is the previous sentence a summary of item_no 10492549. yes or no?',\n",
       " \"query:'soft rugs'\\ndoes the query above return item_no 10492549 as a result. yes or no?\",\n",
       " \"if item_no is 40512386, which of the following is the correct name: 'metod wall cab horizontal w glass door white jutis smoked glass', 'besta tv storage combination glass doors black brown selsviken high gloss beige clear glass' or 'vodskov rug flatwoven natural light grey'?\",\n",
       " \"query:'large jute rugs '\\ndoes the query above return item_no 40512386 as a result. yes or no?\",\n",
       " \"query:'vodskov '\\ndoes the query above return item_no 40512386 as a result. yes or no?\",\n",
       " \"query:'carpet living room'\\nthe query above returns item_no 40512386 as a result. what is its rank? 3, 3 or 9\",\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,a perfect companion for all types of flooring, even those with underfloor heating. ,the anti-skid backing protects the floor, keeps the rug in place and thereby reduces the risk of falling or tripping. is the previous sentence a description of item_no 40512386. yes or no?',\n",
       " \"the drawers close slowly, quietly and softly thanks to the built-in dampers.,you can view and access what's inside, as the drawers can be pulled out all the way .,smooth-running drawers with stop.,drawers are self-closing the last few centimetres.,sturdy frame construction, 18 mm thick. is the previous sentence a description of item_no 40512386. yes or no?\",\n",
       " \"if name is vodskov rug flatwoven natural light grey, what item_no does it refer to? '40512386' or '10191264'?\",\n",
       " \"if name is vodskov rug flatwoven natural light grey, what item_no does it refer to? '19266033' or '40512386'?\",\n",
       " \"query:'flat woven natural'\\nthe query above returns item_no 40512386 as a result. what is its rank? 7 or 2\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 40512386. yes or no?',\n",
       " \"query:'cream rug large'\\ndoes the query above return item_no 40512386 as a result. yes or no?\",\n",
       " \"query:'sissal rugs'\\ndoes the query above return item_no 40512386 as a result. yes or no?\",\n",
       " 'bamboo is a hardwearing natural material.,perfect for your jewellery and hair clips. is the previous sentence a description of item_no 40512386. yes or no?',\n",
       " \"query:'170 x 120cm rug'\\nthe query above returns item_no 40512386 as a result. what is its rank? 1, 6, 5 or 9\",\n",
       " \"if name is vodskov rug flatwoven natural light grey, what item_no does it refer to? '69406719', '40512386', '69440855' or '20456664'?\",\n",
       " 'the tabletop has pre-drilled holes to make it easier to attach to the underframe.,the table top is made of board-on-frame, a strong and lightweight material – and pre-drilled holes make it easier to attach the legs.,plenty of room on the shelf under the trestle for your printer, books or papers. that keeps your table top clear so you have more room to work. is the previous sentence a description of item_no 40512386. yes or no?',\n",
       " 'riksviken front has a subtle sheen that adds depth and luster to your home.,it’s easy to keep the cables from your tv and other devices out of sight but close at hand, as there are several cable outlets at the back of the tv bench.,the cable outlet at the top lets cables run down smoothly into the tv bench.,the legs raise your bestå combination from the floor, giving it a light airy look and making it easy to clean the floor underneath.,the two drawers make it easy to keep remote controls, game controllers and other tv accessories organised.,the drawers close silently and softly, thanks to the integrated soft-closing function. is the previous sentence a description of item_no 50512376. yes or no?',\n",
       " \"query:'sisal jute rugs'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'rugs large vodskov'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'jute natural rugs'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'vodskov 133x195'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'ourdoor rug'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'matting'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'sisak rug'\\nthe query above returns item_no 50512376 as a result. what is its rank? 9, 9, 1, 2 or 9\",\n",
       " \"query:'sisal rugs '\\nthe query above returns item_no 50512376 as a result. what is its rank? 3 or 9\",\n",
       " \"query:'jute rugs large'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"query:'vodskov'\\ndoes the query above return item_no 50512376 as a result. yes or no?\",\n",
       " \"if item_no is 50512376, which of the following is the correct name: 'lagkapten adils desk white black', 'vodskov rug flatwoven natural light grey' or 'metod high cabinet w shelves wire basket white askersund light ash effect'?\",\n",
       " \"if item_no is 50512376, which of the following is the correct name: 'satsumas plant stand bamboo white' or 'vodskov rug flatwoven natural light grey'?\",\n",
       " \"query:'jute natural rugs'\\nthe query above returns item_no 50512376 as a result. what is its rank? 9, 5, 10 or 2\",\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,a perfect companion for all types of flooring, even those with underfloor heating. ,the anti-skid backing protects the floor, keeps the rug in place and thereby reduces the risk of falling or tripping. is the previous sentence a description of item_no 50512376. yes or no?',\n",
       " \"if name is vodskov rug flatwoven natural light grey, what item_no does it refer to? '89388106', '29419441' or '50512376'?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 50495409. yes or no?',\n",
       " \"query:'200x300 rug'\\nthe query above returns item_no 50495409 as a result. what is its rank? 2, 6 or 7\",\n",
       " \"if item_no is 50495409, which of the following is the correct name: 'kafferep oat biscuits', 'pax wardrobe frame white' or 'lydersholm rug flatwoven in outdoor medium brown'?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 50495409. yes or no?',\n",
       " \"query:'large rug '\\nthe query above returns item_no 50495409 as a result. what is its rank? 4 or 8\",\n",
       " \"query:'wooden easy chairs'\\ndoes the query above return item_no 50495409 as a result. yes or no?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 50495409. yes or no?',\n",
       " \"you sit comfortably thanks to the restful flexibility of the scooped seat and shaped back.,the self-adjusting plastic feet adds stability to the chair.,a special surface treatment on the seat prevents you from sliding.,you can stack the chairs, so they take less space when you're not using them. is the previous sentence a description of item_no 50495409. yes or no?\",\n",
       " \"if item_no is 50495409, which of the following is the correct name: 'metod wall cabinet frame wood effect black' or 'lydersholm rug flatwoven in outdoor medium brown'?\",\n",
       " \"query:'large ryg'\\nthe query above returns item_no 50495409 as a result. what is its rank? 5, 2 or 9\",\n",
       " \"query:'lage rugs'\\ndoes the query above return item_no 50495409 as a result. yes or no?\",\n",
       " 'perfect for long dinners since the length and height of the armrests, the angle of the backrest and the extra thick seat make the chair comfortable to sit on.,the armrests are long enough to provide support when you lean back and short enough to allow you to sit as close to the table as you like when eating.,the chair can be pushed under the table so that it takes up less space when not in use.,you don’t have to worry about spills since the cover can be removed and machine washed.,the backrest has the same shape as the legs of ingatorp table and gives the modern chair a traditional touch. is the previous sentence a description of item_no 50495409. yes or no?',\n",
       " \"if item_no is 50495409, which of the following is the correct name: 'lydersholm rug flatwoven in outdoor medium brown', 'metod maximera hc w p o func 4drw 1dr 2shlv white axstad matt blue' or 'lerhyttan door light grey'?\",\n",
       " \"query:'indoor outdoor rug'\\ndoes the query above return item_no 50495409 as a result. yes or no?\",\n",
       " \"query:'new ikea lydersholm rug'\\nthe query above returns item_no 50495409 as a result. what is its rank? 4, 1 or 4\",\n",
       " \"query:'rugs. 200x300. '\\ndoes the query above return item_no 50495409 as a result. yes or no?\",\n",
       " \"if name is tiphede rug flatwoven black natural, what item_no does it refer to? '50409732' or '20470047'?\",\n",
       " 'lightweight and easy to move for airing or washing.,suitable for use in your living room or underneath your dining table, as the flat-woven surface makes it easy to pull out chairs and clean.,cotton is a soft and easy-care natural material that you can machine wash.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 20470047. yes or no?',\n",
       " \"if item_no is 20470047, which of the following is the correct name: 'metod maximera base cabinet with 2 drawers black axstad dark grey', 'uppspel cpu stand with castors dark grey' or 'tiphede rug flatwoven black natural'?\",\n",
       " \"if item_no is 20470047, which of the following is the correct name: 'tiphede rug flatwoven black natural', 'besta storage combination w glass doors oak effect selsviken high gloss white clear glass' or 'metod wall cab horizontal w 2 glass doors black jutis smoked glass'?\",\n",
       " \"query:'soft living room rug'\\ndoes the query above return item_no 20470047 as a result. yes or no?\",\n",
       " \"if name is tiphede rug flatwoven black natural, what item_no does it refer to? '20470047', '10504646' or '10310460'?\",\n",
       " \"query:'rug grey and white'\\nthe query above returns item_no 20470047 as a result. what is its rank? 4, 5, 3 or 4\",\n",
       " \"query:'rugs large flatwoven'\\nthe query above returns item_no 20470047 as a result. what is its rank? 1, 5, 4 or 9\",\n",
       " \"query:'flooring for living room'\\nthe query above returns item_no 20470047 as a result. what is its rank? 1 or 7\",\n",
       " \"query:'washable floor carpets '\\nthe query above returns item_no 20470047 as a result. what is its rank? 2, 9, 1, 8 or 4\",\n",
       " \"query:'washable floor carpets '\\ndoes the query above return item_no 20470047 as a result. yes or no?\",\n",
       " \"if item_no is 20470047, which of the following is the correct name: 'husaroe armchair outdoor dark grey', 'tiphede rug flatwoven black natural' or 'smycka artificial flower lily pink'?\",\n",
       " \"query:'rug room '\\nthe query above returns item_no 20470047 as a result. what is its rank? 2 or 10\",\n",
       " \"query:'soft rug'\\nthe query above returns item_no 20470047 as a result. what is its rank? 7, 5, 3 or 9\",\n",
       " \"query:'rugs carpet'\\nthe query above returns item_no 20470047 as a result. what is its rank? 3, 8 or 4\",\n",
       " \"query:'cotton rug washable'\\nthe query above returns item_no 20470047 as a result. what is its rank? 1 or 6\",\n",
       " \"query:'rugs 133 195'\\ndoes the query above return item_no 90495172 as a result. yes or no?\",\n",
       " \"query:'red rugs '\\ndoes the query above return item_no 90495172 as a result. yes or no?\",\n",
       " \"query:'bblue rug'\\nthe query above returns item_no 90495172 as a result. what is its rank? 7, 1, 7, 4 or 7\",\n",
       " \"query:'langsted 133 x 195'\\nthe query above returns item_no 90495172 as a result. what is its rank? 4, 1 or 5\",\n",
       " \"if name is langsted rug low pile light blue, what item_no does it refer to? '69285208', '90495172' or '39046808'?\",\n",
       " \"query:'rugs all'\\ndoes the query above return item_no 90495172 as a result. yes or no?\",\n",
       " 'open shelving unit. is the previous sentence a summary of item_no 90495172. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90495172. yes or no?',\n",
       " \"if name is langsted rug low pile light blue, what item_no does it refer to? '00208236', '80461479', '90495172' or '09290953'?\",\n",
       " 'lantern for block candle. is the previous sentence a summary of item_no 90495172. yes or no?',\n",
       " \"query:'polypropylene rug'\\nthe query above returns item_no 90495172 as a result. what is its rank? 8, 3, 2 or 9\",\n",
       " 'the cut edges makes it easy to join several rugs to create a bigger rug.,place several rugs of your favorite color together – or mix colors.,durable and will not shed since the rug is made of polypropylene.,the rug will last a long time since it is stain-resistant and easy to care for.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 90495172. yes or no?',\n",
       " \"if name is langsted rug low pile light blue, what item_no does it refer to? '40321155', '90495172', '30328534' or '70289286'?\",\n",
       " \"if name is langsted rug low pile light blue, what item_no does it refer to? '30263844', '90495172', '69409077' or '09331519'?\",\n",
       " \"query:'kitvhen unit'\\nthe query above returns item_no 90495172 as a result. what is its rank? 10, 4 or 3\",\n",
       " \"query:'kitvhen unit'\\nthe query above returns item_no 90495172 as a result. what is its rank? 10, 8, 5, 9 or 4\",\n",
       " \"query:'stoense 170 240'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " 'corner wall cab w carousel/glass dr. is the previous sentence a summary of item_no 10500182. yes or no?',\n",
       " \"query:'stoense olive'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " \"query:'stoense rug sizes'\\nthe query above returns item_no 10500182 as a result. what is its rank? 3, 1 or 2\",\n",
       " 'ringhult kitchen drawer front has clean, straight lines and a glossy surface which is durable and easy to keep clean. with ringhult you get a light kitchen with a modern feeling.,covered with high-gloss foil; gives an easy care finish.,25 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 10500182. yes or no?',\n",
       " \"query:'large rugs'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " \"query:'light olive'\\nthe query above returns item_no 10500182 as a result. what is its rank? 8 or 9\",\n",
       " \"query:'rugs large oliv'\\nthe query above returns item_no 10500182 as a result. what is its rank? 9, 2, 1 or 8\",\n",
       " \"if item_no is 10500182, which of the following is the correct name: 'stoense rug low pile light olive green' or 'moerbylanga odger table and 4 chairs brown white beige'?\",\n",
       " \"if name is stoense rug low pile light olive green, what item_no does it refer to? '10500182' or '99402932'?\",\n",
       " \"if name is stoense rug low pile light olive green, what item_no does it refer to? '10455896', '10500182', '00450031' or '80448954'?\",\n",
       " \"query:'steiner 200x300'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " \"query:'rug medium pile'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " \"query:'stain-resistant sofa'\\ndoes the query above return item_no 10500182 as a result. yes or no?\",\n",
       " \"query:'low pile olive green'\\nthe query above returns item_no 10500182 as a result. what is its rank? 1, 6 or 7\",\n",
       " \"query:'rugs large oliv'\\nthe query above returns item_no 10500182 as a result. what is its rank? 5, 1, 2, 6 or 7\",\n",
       " \"query:'stoense olive green rug'\\ndoes the query above return item_no 70500179 as a result. yes or no?\",\n",
       " \"if item_no is 70500179, which of the following is the correct name: 'auli sekken pair of sliding doors mirror glass frosted glass', 'stoense rug low pile light olive green', 'nordviken leifarne table and 6 chairs black light olive green black' or 'metod maximera hc w p o func 1dr 4drw white axstad dark grey'?\",\n",
       " \"query:'200cm x 300cm'\\ndoes the query above return item_no 70500179 as a result. yes or no?\",\n",
       " \"query:'rug sizes'\\nthe query above returns item_no 70500179 as a result. what is its rank? 1, 9, 5 or 1\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 70500179. yes or no?',\n",
       " \"if name is stoense rug low pile light olive green, what item_no does it refer to? '10428488', '70500179' or '30498927'?\",\n",
       " \"query:'low pile '\\nthe query above returns item_no 70500179 as a result. what is its rank? 1, 2 or 9\",\n",
       " \"query:'stonese green'\\ndoes the query above return item_no 70500179 as a result. yes or no?\",\n",
       " \"if item_no is 70500179, which of the following is the correct name: 'stoense rug low pile light olive green' or 'besta wall cabinet with 2 doors black brown lappviken light grey beige'?\",\n",
       " \"if name is stoense rug low pile light olive green, what item_no does it refer to? '70500179', '80260876', '70483274' or '10442917'?\",\n",
       " \"query:'olive rugs'\\ndoes the query above return item_no 70500179 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 70500179. yes or no?',\n",
       " \"if item_no is 70500179, which of the following is the correct name: 'galant storage combination with drawers white stained oak veneer', 'moppa jug with lid dark blue transparent', 'stoense rug low pile light olive green' or 'metod maximera base cab f hob int extractor w drw white axstad matt blue'?\",\n",
       " \"query:'stoence'\\ndoes the query above return item_no 70500179 as a result. yes or no?\",\n",
       " 'motif created by vincent van gogh.,create a picture that fits you and your walls by choosing size of the canvas, a motif you like, and the colour of the frame you prefer.,choose the look of your picture by either having the frame visible or folding the canvas around the frame.,if you want some variation, you can easily change motif or put textiles in the frame.,the picture and frame come in separate packages.,easy to take home since you buy the frame folded up in a handy package and the canvas comes as a roll.,no need to make holes in the wall – use 2 alfta self-adhesive hooks and the picture will hang securely in place. is the previous sentence a description of item_no 70500179. yes or no?',\n",
       " \"if item_no is 70500179, which of the following is the correct name: 'knoxhult base cabinet with doors and drawer wood effect grey' or 'stoense rug low pile light olive green'?\",\n",
       " \"query:'250x350 rug'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'rug 250x 350'\\nthe query above returns item_no 60229033 as a result. what is its rank? 2 or 1\",\n",
       " \"query:'stockholm rugs '\\nthe query above returns item_no 60229033 as a result. what is its rank? 9, 1, 10 or 7\",\n",
       " \"query:'rugs wool'\\nthe query above returns item_no 60229033 as a result. what is its rank? 10 or 1\",\n",
       " \"if item_no is 60229033, which of the following is the correct name: 'tyvelse rug low pile dark red', 'stockholm rug flatwoven handmade chequered brown', 'godmorgon tolken bathroom vanity white bamboo' or 'besta storage combination with doors'?\",\n",
       " \"query:'x large rugs'\\nthe query above returns item_no 60229033 as a result. what is its rank? 1 or 6\",\n",
       " 'handwoven by skilled craftspeople, each one is unique. made in india in organised weaving centres with good working conditions and fair wages.,the durable, soil-resistant wool surface makes this rug perfect in your living room or under your dining table.,the rug has the same pattern on both sides, so you can turn it over and it will withstand more wear and last even longer. is the previous sentence a description of item_no 60229033. yes or no?',\n",
       " \"this firm sofa will have a long life since the seat cushions are filled with high resilience foam that gives good support for your body and quickly regains its original shape when you get up.  ,the sofa's sections can be combined in different ways to get a size and shape that suits you and your home. if you ever need a larger sofa, you can always add a section or two.,you can create your own perfect combination with the planning tool. assemble, take apart and put together again until it’s just right.,the chaise longue has storage under the seat. the lid automatically stops in the open position so that you can easily pick out and put back the things that you are storing.,you can place the chaise longue section to the left or right of the sofa, and switch whenever you like.,the headrest extends the backrest of the sofa so that you sit more comfortably with a nice support for your neck.,this cover is made of dope-dyed gunnared fabric in polyester. it is a durable fabric with a wool-like feel, a warm look and a two-toned melange effect.,the cover is easy to keep clean since it is removable and machine washable.,10 year guarantee. read about the terms in the guarantee brochure. . is the previous sentence a description of item_no 60229033. yes or no?\",\n",
       " \"query:'carpet stockholm'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'rug 250cm'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'large floor mats'\\nthe query above returns item_no 60229033 as a result. what is its rank? 6, 7, 5 or 8\",\n",
       " \"query:'adum'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'rug 250cm '\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'350cm rug'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'carpets %2f rugs'\\ndoes the query above return item_no 60229033 as a result. yes or no?\",\n",
       " \"query:'rug large '\\nthe query above returns item_no 60229033 as a result. what is its rank? 9 or 6\",\n",
       " \"if item_no is 50408049, which of the following is the correct name: 'besta wall mounted cabinet combination black brown glassvik smoked glass' or 'langsted rug low pile beige'?\",\n",
       " 'the cut edges makes it easy to join several rugs to create a bigger rug.,place several rugs of your favourite colour together – or mix colours.,durable and will not shed since the rug is made of polypropylene. ,the rug will last a long time since it is stain-resistant and easy to care for.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 50408049. yes or no?',\n",
       " \"query:'170x240 cm'\\nthe query above returns item_no 50408049 as a result. what is its rank? 10 or 5\",\n",
       " \"query:'low pile rug '\\nthe query above returns item_no 50408049 as a result. what is its rank? 3, 9, 2 or 1\",\n",
       " \"query:'shed '\\ndoes the query above return item_no 50408049 as a result. yes or no?\",\n",
       " 'box with lid, set of 2. is the previous sentence a summary of item_no 50408049. yes or no?',\n",
       " \"query:'170 x 240 rug langsted'\\ndoes the query above return item_no 50408049 as a result. yes or no?\",\n",
       " \"if name is langsted rug low pile beige, what item_no does it refer to? '50408049', '39466093' or '50273502'?\",\n",
       " \"query:'rugs rugs '\\ndoes the query above return item_no 50408049 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 50408049. yes or no?',\n",
       " \"query:'rugs 170 240'\\nthe query above returns item_no 50408049 as a result. what is its rank? 9, 4, 1, 2 or 5\",\n",
       " 'the cut edges makes it easy to join several rugs to create a bigger rug.,place several rugs of your favourite colour together – or mix colours.,durable and will not shed since the rug is made of polypropylene. ,the rug will last a long time since it is stain-resistant and easy to care for.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 50408049. yes or no?',\n",
       " \"if name is langsted rug low pile beige, what item_no does it refer to? '50408049' or '50506506'?\",\n",
       " \"query:'rugs large 170 x 240'\\nthe query above returns item_no 50408049 as a result. what is its rank? 9, 6, 8 or 7\",\n",
       " \"query:'langsted rugs'\\ndoes the query above return item_no 50408049 as a result. yes or no?\",\n",
       " 'with grey as a base, you create a warm and cosy feeling. perfect if you want a discreet colour that goes with everything. modern or classic – you decide the style.,the wall cabinet provides ample storage for food, towels and detergents.,the open wall frame gives a good overview and easy access to jars, bottles and things often used.,enhet swivel shelf gives you quick and easy access to smaller items like perfumes, keys, accessories, vitamins or spice jars. it fastens easily to any of the open frames in the enhet series.,enhet hooks put towels close at hand. they slide into the grooves under the open enhet frames. and best of all – you don’t have to drill.,the space under the open frame is a smart spot to hang towels, place a freestanding waste bin or why not use it to park your trolley?,this combination can also serve as a laundry station with storage for detergents and space under the open frame for your washing machine. ,you can easily add more enhet accessories to extend the use of your storage combination. and best of all – you don’t have to drill. sold separately.,10 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 50408049. yes or no?',\n",
       " \"query:'133x195 cm'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " \"query:'large rug green'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " 'artificial flower. is the previous sentence a summary of item_no 00453435. yes or no?',\n",
       " \"if item_no is 00453435, which of the following is the correct name: 'jubla unscented candle white', 'besta tv bench with drawers oak effect selsviken high gloss white' or 'sporup rug low pile dark green'?\",\n",
       " \"query:'green rug'\\nthe query above returns item_no 00453435 as a result. what is its rank? 10, 4 or 8\",\n",
       " 'this rug is made from about 320 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 00453435. yes or no?',\n",
       " \"query:'00453435'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '00453435', '30374564' or '79322451'?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '80396865', '00453435' or '39463476'?\",\n",
       " \"query:'rugs 133x195'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " \"query:'large patterned rugs'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " 'kallarp is a high-gloss drawer front with sleek, straight lines and brings a colourful, modern look to your kitchen.     ,covered with high-gloss foil; gives an easy care finish.,25 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 00453435. yes or no?',\n",
       " \"query:'carpet low pile'\\ndoes the query above return item_no 00453435 as a result. yes or no?\",\n",
       " 'this rug is made from about 320 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 00453435. yes or no?',\n",
       " 'this rug is made from about 320 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 00453435. yes or no?',\n",
       " \"query:'green placemat'\\nthe query above returns item_no 00453435 as a result. what is its rank? 3, 7, 1 or 2\",\n",
       " 'you can customise spacing as you need, because the shelf is adjustable.,you can choose to mount the door on the right or left side.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 90453426. yes or no?',\n",
       " \"query:'filt'\\nthe query above returns item_no 90453426 as a result. what is its rank? 1, 1, 3, 2 or 7\",\n",
       " \"query:'ikea sporup'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '29398684', '90453426' or '30512669'?\",\n",
       " \"query:'green large rug'\\nthe query above returns item_no 90453426 as a result. what is its rank? 5, 4, 3, 2 or 3\",\n",
       " \"query:'rugs large 170'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"query:'green rug '\\nthe query above returns item_no 90453426 as a result. what is its rank? 2 or 7\",\n",
       " \"query:'rug 300'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '89323657', '90453426', '49325286' or '90477192'?\",\n",
       " \"query:'sw17 8ub'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '90453426' or '80435188'?\",\n",
       " \"query:'sporrup'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"query:'green mat'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " \"if item_no is 90453426, which of the following is the correct name: 'sporup rug low pile dark green' or 'vinter 2021 apron star pattern red beige'?\",\n",
       " \"query:'sporup rug low pile dark green 90453426'\\ndoes the query above return item_no 90453426 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90453426. yes or no?',\n",
       " \"query:'large rugs 200%2f300'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " \"query:'rug black'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " \"query:'200cmx 300cm rug'\\nthe query above returns item_no 30487660 as a result. what is its rank? 2, 3, 6, 6 or 5\",\n",
       " \"query:'carpet 200 x 300'\\nthe query above returns item_no 30487660 as a result. what is its rank? 5, 9, 8 or 6\",\n",
       " \"query:'black rug 200x300'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " \"query:'rug 300 low pile'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " \"if item_no is 30487660, which of the following is the correct name: 'leifarne swivel chair olive green balsberget white', 'eneryda cup handle chrome plated', 'smastad wardrobe white white with 2 clothes rails' or 'sporup rug low pile black'?\",\n",
       " \"query:'200cm x 300cm rug'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " 'cabinet combination for tv. is the previous sentence a summary of item_no 30487660. yes or no?',\n",
       " \"query:'rugs 200 300'\\nthe query above returns item_no 30487660 as a result. what is its rank? 2, 6 or 4\",\n",
       " 'this rug is made from about 740 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30487660. yes or no?',\n",
       " \"query:'low pile rug, black'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " 'simple yet timeless tableware with a traditional style and soft round shapes with attention to details. is the previous sentence a description of item_no 30487660. yes or no?',\n",
       " \"query:'sporup black'\\nthe query above returns item_no 30487660 as a result. what is its rank? 8, 5, 8 or 3\",\n",
       " 'this rug is made from about 740 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 30487660. yes or no?',\n",
       " \"query:'black rugs large'\\ndoes the query above return item_no 30487660 as a result. yes or no?\",\n",
       " 'this rug is made from about 740 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 70453432. yes or no?',\n",
       " 'desk. is the previous sentence a summary of item_no 70453432. yes or no?',\n",
       " \"query:'green rugs'\\ndoes the query above return item_no 70453432 as a result. yes or no?\",\n",
       " \"query:'computer stnad'\\ndoes the query above return item_no 70453432 as a result. yes or no?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '10482512', '10442352' or '70453432'?\",\n",
       " 'this rug is made from about 740 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the fine yarn is densely tufted into a pile that’s durable and incredibly soft.,the pattern is both visual and tactile thanks to the rib construction.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 70453432. yes or no?',\n",
       " \"query:'kitchen washable rugs'\\nthe query above returns item_no 70453432 as a result. what is its rank? 1, 3 or 6\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 70453432. yes or no?',\n",
       " \"query:'the rug'\\ndoes the query above return item_no 70453432 as a result. yes or no?\",\n",
       " \"query:'rug low pile '\\nthe query above returns item_no 70453432 as a result. what is its rank? 8, 3 or 2\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '60261527', '80436965' or '70453432'?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '50409732', '70453432' or '60246693'?\",\n",
       " \"if name is sporup rug low pile dark green, what item_no does it refer to? '80480345', '70453432' or '99416726'?\",\n",
       " \"the curved edge of this bowl spatula in silicone allows easy access to corners.,the soft silicone moulds to the shape of the container, and with the spatula's tapered lip you scrape up every last drop of butter or piece chocolate or dough.,the shape fits nicely in your left or right hand and has a comfortable grip. is the previous sentence a description of item_no 70453432. yes or no?\",\n",
       " \"query:'200 x 300 rug'\\ndoes the query above return item_no 70453432 as a result. yes or no?\",\n",
       " \"if item_no is 70453432, which of the following is the correct name: 'pax wardrobe white stained oak effect forsand white stained oak effect', 'sporup rug low pile dark green' or 'bekant desk with screen white grey'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90429554. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90429554. yes or no?',\n",
       " \"if name is tyvelse rug low pile dark blue, what item_no does it refer to? '00527592' or '90429554'?\",\n",
       " \"query:'red rug '\\nthe query above returns item_no 90429554 as a result. what is its rank? 10 or 7\",\n",
       " 'this rug is made from about 540 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibers.,want a bigger rug? place several rugs of your favorite color together – or mix colors.,shade variations are created when the pile falls in different directions adding life and interest to the rug.,the warm, deep color tones enhance the luxury feeling.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 90429554. yes or no?',\n",
       " \"query:'tyvelse dark blue'\\nthe query above returns item_no 90429554 as a result. what is its rank? 1, 2 or 4\",\n",
       " \"if name is tyvelse rug low pile dark blue, what item_no does it refer to? '90429554', '20460128' or '09194112'?\",\n",
       " \"query:'tyvelse rug 170 x240'\\ndoes the query above return item_no 90429554 as a result. yes or no?\",\n",
       " \"query:'rug deep pile'\\nthe query above returns item_no 90429554 as a result. what is its rank? 1, 5, 5, 2 or 7\",\n",
       " \"query:'rectangular rug 170 x 240'\\nthe query above returns item_no 90429554 as a result. what is its rank? 1 or 4\",\n",
       " \"if item_no is 90429554, which of the following is the correct name: 'tyvelse rug low pile dark blue', 'fejka artificial potted plant in outdoor poinsettia red', 'sekken 4 panels for sliding door frame frosted glass' or 'svenbertil chair black broringe chrome plated'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90429554. yes or no?',\n",
       " \"query:'rug low pile blue'\\nthe query above returns item_no 90429554 as a result. what is its rank? 2, 2, 9 or 1\",\n",
       " \"query:'rugs navy blue'\\ndoes the query above return item_no 90429554 as a result. yes or no?\",\n",
       " \"query:'rug low pile blue'\\nthe query above returns item_no 90429554 as a result. what is its rank? 4, 5, 8, 2 or 1\",\n",
       " \"query:'tyvelse blue'\\ndoes the query above return item_no 90429554 as a result. yes or no?\",\n",
       " \"query:'floor rug wool'\\ndoes the query above return item_no 30495976 as a result. yes or no?\",\n",
       " \"query:'carpet '\\ndoes the query above return item_no 30495976 as a result. yes or no?\",\n",
       " \"query:'rug pattern'\\nthe query above returns item_no 30495976 as a result. what is its rank? 3, 3, 5, 4 or 2\",\n",
       " \"query:'304.959.76'\\ndoes the query above return item_no 30495976 as a result. yes or no?\",\n",
       " \"if name is dejret rug high pile handmade beige, what item_no does it refer to? '69385528', '30495976', '40506804' or '70257313'?\",\n",
       " \"if item_no is 30495976, which of the following is the correct name: 'dejret rug high pile handmade beige' or 'besta tv storage combination glass doors black brown selsviken high gloss black smoked glass'?\",\n",
       " \"query:'rug tassel'\\ndoes the query above return item_no 30495976 as a result. yes or no?\",\n",
       " \"query:'ugs'\\nthe query above returns item_no 30495976 as a result. what is its rank? 2, 2, 1, 10 or 7\",\n",
       " \"query:'rug tassels'\\nthe query above returns item_no 30495976 as a result. what is its rank? 1 or 8\",\n",
       " \"query:'carpets dejret'\\nthe query above returns item_no 30495976 as a result. what is its rank? 1 or 6\",\n",
       " \"if name is dejret rug high pile handmade beige, what item_no does it refer to? '30495976' or '50492222'?\",\n",
       " \"if name is dejret rug high pile handmade beige, what item_no does it refer to? '30495976', '20515456', '79301161' or '40513140'?\",\n",
       " \"query:'rug dejret'\\ndoes the query above return item_no 30495976 as a result. yes or no?\",\n",
       " \"if item_no is 30495976, which of the following is the correct name: 'dejret rug high pile handmade beige', 'starttid backpack gray' or 'besta eket cabinet combination for tv white'?\",\n",
       " \"query:'rugs high pile large'\\nthe query above returns item_no 30495976 as a result. what is its rank? 7, 1 or 3\",\n",
       " \"query:'rug tassel '\\nthe query above returns item_no 30495976 as a result. what is its rank? 4, 3, 1, 10 or 7\",\n",
       " \"query:'rug extra large'\\ndoes the query above return item_no 40508068 as a result. yes or no?\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 40508068. yes or no?',\n",
       " 'wall-mounted cabinet combination. is the previous sentence a summary of item_no 40508068. yes or no?',\n",
       " \"query:'vindebäk beige'\\nthe query above returns item_no 40508068 as a result. what is its rank? 9 or 4\",\n",
       " \"query:'200 x300 rug'\\ndoes the query above return item_no 40508068 as a result. yes or no?\",\n",
       " \"query:'200 cm rug'\\nthe query above returns item_no 40508068 as a result. what is its rank? 1 or 3\",\n",
       " \"if item_no is 40508068, which of the following is the correct name: 'besta storage combination with drawers white bergsviken stubbarp black marble effect', 'pax grimo wardrobe combination white white', 'vindebaek rug high pile light beige' or 'gurli cushion cover olive green'?\",\n",
       " \"query:'extra large mat'\\nthe query above returns item_no 40508068 as a result. what is its rank? 6, 3, 7, 2 or 1\",\n",
       " \"query:'high pile beige rug'\\ndoes the query above return item_no 40508068 as a result. yes or no?\",\n",
       " 'tv bench. is the previous sentence a summary of item_no 40508068. yes or no?',\n",
       " 'it’s easy to keep the cords from your tv and other devices out of sight but close at hand, as there are several cord outlets at the back of the tv bench.,the cable outlet at the top lets cords run down smoothly into the tv bench.,the drawers and doors close silently and softly, thanks to the integrated soft-closing function.,the drawers make it easy to keep your belongings organized. the shelves behind the doors give you even more storage space.,the shelves are adjustable so you can customize your storage as needed.,the space-saving wall cabinets make the most of the wall area above your tv.,optimise and organise your bestå storage with boxes and inserts that you like. is the previous sentence a description of item_no 40508068. yes or no?',\n",
       " \"query:'living room rugs'\\nthe query above returns item_no 40508068 as a result. what is its rank? 6, 3 or 9\",\n",
       " \"query:'200 x 300 rugs'\\nthe query above returns item_no 40508068 as a result. what is its rank? 4, 5, 1, 2 or 7\",\n",
       " \"query:'beige high pile rug'\\nthe query above returns item_no 40508068 as a result. what is its rank? 1 or 2\",\n",
       " 'the two-tone mélange yarn creates a lively expression.,the anti-skid backing protects the floor, keeps the rug in place and thereby reduces the risk of falling or tripping.,the high pile dampens sound and provides a soft surface to walk on.,this rug is made of recycled polyester from sources like pet bottles. using waste as a resource takes us one step closer to a more sustainable future. is the previous sentence a description of item_no 40508068. yes or no?',\n",
       " \"if name is vindebaek rug high pile light beige, what item_no does it refer to? '40508068', '20518493' or '70409793'?\",\n",
       " 'shelf unit with door. is the previous sentence a summary of item_no 40507889. yes or no?',\n",
       " \"query:'vinderbäk'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'vindebak rugs'\\nthe query above returns item_no 40507889 as a result. what is its rank? 1, 5, 6 or 8\",\n",
       " \"query:'beige high pile carpet'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'rugs \\\\'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"if name is vindebaek rug high pile light beige, what item_no does it refer to? '40507889', '80457774' or '29419441'?\",\n",
       " \"query:'vindebäk rug high pile'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'rugs bedroom'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'bedroom rugs'\\nthe query above returns item_no 40507889 as a result. what is its rank? 10, 3, 1 or 5\",\n",
       " \"query:'carpet vindebak'\\nthe query above returns item_no 40507889 as a result. what is its rank? 2, 9, 7, 10 or 3\",\n",
       " \"query:'bedroom rugs'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'rugs 230 x 160'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'vindebäk rug high pile'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'high pile rug 230x'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'200x300 vindebat'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " \"query:'rugs 230 x 160'\\ndoes the query above return item_no 40507889 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90425315. yes or no?',\n",
       " \"if item_no is 90425315, which of the following is the correct name: 'hembjuden paper napkin multicolour' or 'tyvelse rug low pile dark red'?\",\n",
       " \"if item_no is 90425315, which of the following is the correct name: 'tyvelse rug low pile dark red' or 'besta frame oak effect'?\",\n",
       " \"lifelike, artificial plants that stay fresh year after year.,perfect if you can't have a live plant, but still want to enjoy the beauty of nature. is the previous sentence a description of item_no 90425315. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '90366839' or '90425315'?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '80472878', '90425315' or '80502922'?\",\n",
       " \"query:'red rug low pile'\\nthe query above returns item_no 90425315 as a result. what is its rank? 9, 2, 1 or 2\",\n",
       " \"query:'tyvelse 133x195'\\ndoes the query above return item_no 90425315 as a result. yes or no?\",\n",
       " 'this rug is made from about 345 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 90425315. yes or no?',\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '90425315', '29396086' or '00397227'?\",\n",
       " 'storage combination with drawers. is the previous sentence a summary of item_no 90425315. yes or no?',\n",
       " 'wardrobe. is the previous sentence a summary of item_no 90425315. yes or no?',\n",
       " \"query:'tyvelse 133x195'\\ndoes the query above return item_no 90425315 as a result. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '59408587', '90425315' or '00212554'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 90425315. yes or no?',\n",
       " \"query:'large rugs low pile'\\ndoes the query above return item_no 90425315 as a result. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '40425327', '69329410' or '80241816'?\",\n",
       " \"query:'red mats'\\ndoes the query above return item_no 40425327 as a result. yes or no?\",\n",
       " \"you can easily adapt this ready-made pax/komplement combination to suit your needs and taste using the pax planning tool.,adjustable feet make it possible to compensate any irregularities in the floor.,hinges with integrated dampers catch the door and close it slowly, silently and softly.,with the pax planner, you can easily complete your combination with integrated lighting. after choosing the light source, the planner will work out which accessories you need to complete the solution.,the mesh basket allows air to circulate and is perfect for storing your folded clothes, socks or accessories.,do you know it's easy as pie to move around the inside organisers after taste, or change them later on? with small changes you can alter the storage solution to fit your clothes, instead of the other way around. is the previous sentence a description of item_no 40425327. yes or no?\",\n",
       " \"query:'large deep pile rug'\\nthe query above returns item_no 40425327 as a result. what is its rank? 1 or 7\",\n",
       " \"query:'603 194 .96adum'\\ndoes the query above return item_no 40425327 as a result. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '20419389' or '40425327'?\",\n",
       " \"query:'clearance grey rug'\\nthe query above returns item_no 40425327 as a result. what is its rank? 5 or 1\",\n",
       " \"query:'large rug 170 x 240'\\ndoes the query above return item_no 40425327 as a result. yes or no?\",\n",
       " 'adjustable hinges allow you to adjust the door horizontally and vertically.,the drawers have integrated push-openers so that you can open them with just a light push.,the shelves are adjustable so you can customise your storage as needed.,the two drawers make it easy to keep your belongings organised. the shelves behind the doors give you even more storage space.,optimise and organise your bestå storage with boxes and inserts that you like. is the previous sentence a description of item_no 40425327. yes or no?',\n",
       " \"query:'rug 170 240'\\nthe query above returns item_no 40425327 as a result. what is its rank? 2, 6, 7 or 3\",\n",
       " \"if item_no is 40425327, which of the following is the correct name: 'metod maximera base cb 3 frnts 2 low 1 md 1 hi drw black askersund dark brown ash effect', 'vikfjaerd bath towel light pink' or 'tyvelse rug low pile dark red'?\",\n",
       " 'this rug is made from about 540 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 40425327. yes or no?',\n",
       " \"query:'large mat'\\ndoes the query above return item_no 40425327 as a result. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '00337380', '40476779' or '40425327'?\",\n",
       " \"query:'rug red'\\ndoes the query above return item_no 40425327 as a result. yes or no?\",\n",
       " \"query:'tyvelse 40425327'\\nthe query above returns item_no 40425327 as a result. what is its rank? 1 or 4\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '60426854', '59430871', '30331089' or '99301141'?\",\n",
       " \"query:'dark red rugs'\\ndoes the query above return item_no 60426854 as a result. yes or no?\",\n",
       " 'this rug is made from about 780 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 60426854. yes or no?',\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '39272641', '49457272' or '60426854'?\",\n",
       " \"query:'60426854'\\nthe query above returns item_no 60426854 as a result. what is its rank? 1 or 10\",\n",
       " \"query:'roll of rug'\\ndoes the query above return item_no 60426854 as a result. yes or no?\",\n",
       " \"if name is tyvelse rug low pile dark red, what item_no does it refer to? '50474770', '60426854', '79393783' or '60492448'?\",\n",
       " \"query:'200 x 300 rug'\\nthe query above returns item_no 60426854 as a result. what is its rank? 6, 7, 2, 7 or 1\",\n",
       " 'this rug is made from about 780 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 60426854. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 60426854. yes or no?',\n",
       " \"query:'tyvelse'\\nthe query above returns item_no 60426854 as a result. what is its rank? 2, 5 or 5\",\n",
       " \"if item_no is 60426854, which of the following is the correct name: 'enhet door concrete effect', 'haverud table with storage ladder black', 'tyvelse rug low pile dark red' or 'insvep basket plastic rattan brown'?\",\n",
       " 'block-out roller blind. is the previous sentence a summary of item_no 60426854. yes or no?',\n",
       " 'this rug is made from about 780 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 60426854. yes or no?',\n",
       " \"query:'tyvlse'\\ndoes the query above return item_no 60426854 as a result. yes or no?\",\n",
       " \"query:'red rug low pile'\\ndoes the query above return item_no 60426854 as a result. yes or no?\",\n",
       " \"query:'200 x 300cm'\\nthe query above returns item_no 20429557 as a result. what is its rank? 7, 1 or 6\",\n",
       " \"if name is tyvelse rug low pile dark blue, what item_no does it refer to? '40432479', '20429557' or '20326804'?\",\n",
       " \"query:'large blue rugs'\\ndoes the query above return item_no 20429557 as a result. yes or no?\",\n",
       " 'lounger, outdoor. is the previous sentence a summary of item_no 20429557. yes or no?',\n",
       " \"query:'200cm x 300cm rug'\\nthe query above returns item_no 20429557 as a result. what is its rank? 1, 4 or 8\",\n",
       " \"query:'rug sale'\\nthe query above returns item_no 20429557 as a result. what is its rank? 8, 1, 5 or 3\",\n",
       " \"query:'stoense '\\nthe query above returns item_no 20429557 as a result. what is its rank? 10, 2 or 9\",\n",
       " \"query:'all sale'\\ndoes the query above return item_no 20429557 as a result. yes or no?\",\n",
       " \"if item_no is 20429557, which of the following is the correct name: 'smastad platsa storage combination white with frame', 'knipen detergent dispenser stainless steel', 'ivar 3 sections corner pine gray mesh' or 'tyvelse rug low pile dark blue'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 20429557. yes or no?',\n",
       " 'this rug is made from about 780 recycled 0.5 l pet bottles. using waste as a resource brings us one step closer to a more sustainable future.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,want a bigger rug? place several rugs of your favourite colour together – or mix colours.,shade variations are created when the pile falls in different directions adding life and interest to the rug. ,the warm, deep colour tones enhance the luxury feeling,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 20429557. yes or no?',\n",
       " \"query:'carpetes'\\nthe query above returns item_no 20429557 as a result. what is its rank? 4 or 8\",\n",
       " \"query:'dark blue mat'\\ndoes the query above return item_no 20429557 as a result. yes or no?\",\n",
       " \"query:'tyvelse 200x300'\\ndoes the query above return item_no 20429557 as a result. yes or no?\",\n",
       " \"query:'tyvelese'\\nthe query above returns item_no 20429557 as a result. what is its rank? 9, 9 or 3\",\n",
       " \"query:'navy blue rugs'\\nthe query above returns item_no 20429557 as a result. what is its rank? 2, 5, 3 or 8\",\n",
       " \"if item_no is 50348244, which of the following is the correct name: 'filskov rug flatwoven handmade grey white' or 'auli mehamn pair of sliding doors mirror glass black brown stained ash effect'?\",\n",
       " \"query:'carpets white '\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " \"query:'flat rug'\\nthe query above returns item_no 50348244 as a result. what is its rank? 1 or 7\",\n",
       " \"if name is filskov rug flatwoven handmade grey white, what item_no does it refer to? '69047806' or '50348244'?\",\n",
       " \"query:'woven mat'\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " \"query:'grey large rug'\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " 'bookcase. is the previous sentence a summary of item_no 50348244. yes or no?',\n",
       " \"query:'large rug chevron'\\nthe query above returns item_no 50348244 as a result. what is its rank? 4, 9 or 1\",\n",
       " 'adjustable shelves can be arranged according to your needs.,a simple unit can be enough storage for a limited space or the foundation for a larger storage solution if your needs change.,surface made from natural wood veneer. is the previous sentence a description of item_no 50348244. yes or no?',\n",
       " \"query:'rug, flatwoven, handmade'\\nthe query above returns item_no 50348244 as a result. what is its rank? 5 or 2\",\n",
       " \"query:'rug 100 x 100'\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " 'bookcase. is the previous sentence a summary of item_no 50348244. yes or no?',\n",
       " \"if item_no is 50348244, which of the following is the correct name: 'besta wall mounted cabinet combination white glassvik white frosted glass' or 'filskov rug flatwoven handmade grey white'?\",\n",
       " \"query:'medium rugs'\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " \"query:'artificial bouquet'\\ndoes the query above return item_no 50348244 as a result. yes or no?\",\n",
       " \"query:'rugs 170x240'\\nthe query above returns item_no 50348244 as a result. what is its rank? 5, 10 or 6\",\n",
       " \"query:'stoense rugs beige'\\nthe query above returns item_no 20426808 as a result. what is its rank? 2 or 3\",\n",
       " \"query:'carpets white'\\ndoes the query above return item_no 20426808 as a result. yes or no?\",\n",
       " \"if item_no is 20426808, which of the following is the correct name: 'komplement shoe shelf dark grey', 'stoense rug low pile off white', 'kungsbacka door anthracite' or 'besta shelf unit with door white smeviken white'?\",\n",
       " \"query:'stoensa'\\ndoes the query above return item_no 20426808 as a result. yes or no?\",\n",
       " \"if item_no is 20426808, which of the following is the correct name: 'fredriksjoen bath sheet dark blue', 'galant storage combination black stained ash veneer', 'vinarn hand towel light grey beige' or 'stoense rug low pile off white'?\",\n",
       " 'the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibers.,the cut edges makes it easy to join several rugs to create a bigger rug, without a visible seam.,the light sheen creates variations in the surface.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 20426808. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 20426808. yes or no?',\n",
       " \"query:'ruga'\\ndoes the query above return item_no 20426808 as a result. yes or no?\",\n",
       " \"query:'rugs 200'\\ndoes the query above return item_no 20426808 as a result. yes or no?\",\n",
       " \"query:'white%2fcream rugs'\\nthe query above returns item_no 20426808 as a result. what is its rank? 6 or 1\",\n",
       " \"if name is stoense rug low pile off white, what item_no does it refer to? '49329374', '29331764', '60214159' or '20426808'?\",\n",
       " \"if item_no is 20426808, which of the following is the correct name: 'voxtorp door high gloss white', 'stoense rug low pile off white' or 'besta tv storage combination glass doors white selsviken high gloss white frosted glass'?\",\n",
       " \"query:'large rugs 200x300 beige'\\nthe query above returns item_no 20426808 as a result. what is its rank? 8, 10, 1, 9 or 4\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 20426808. yes or no?',\n",
       " \"if name is stoense rug low pile off white, what item_no does it refer to? '20426808' or '09429610'?\",\n",
       " \"query:'rug 200 300'\\ndoes the query above return item_no 20426808 as a result. yes or no?\",\n",
       " \"if name is stoense rug low pile off white, what item_no does it refer to? '89469640', '39401596', '10425526' or '39285568'?\",\n",
       " \"query:'stoenes rug'\\nthe query above returns item_no 10425526 as a result. what is its rank? 6, 9, 10, 5 or 1\",\n",
       " \"query:'low pile rug 200x300'\\ndoes the query above return item_no 10425526 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 10425526. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 10425526. yes or no?',\n",
       " \"query:'stroense 157 inch x94 inch carpet'\\nthe query above returns item_no 10425526 as a result. what is its rank? 4 or 1\",\n",
       " \"query:'indoor and outdoor rugs'\\nthe query above returns item_no 10425526 as a result. what is its rank? 4, 3, 2, 6 or 2\",\n",
       " \"query:'carpet bigger'\\ndoes the query above return item_no 10425526 as a result. yes or no?\",\n",
       " \"query:'thick rugs'\\nthe query above returns item_no 10425526 as a result. what is its rank? 5, 6, 9 or 10\",\n",
       " \"query:'srtoense'\\ndoes the query above return item_no 10425526 as a result. yes or no?\",\n",
       " 'you can customise spacing as you need, because the shelf is adjustable.,you can choose to mount the door on the right or left side.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 10425526. yes or no?',\n",
       " \"query:'rugs flatwoven'\\ndoes the query above return item_no 10425526 as a result. yes or no?\",\n",
       " \"if item_no is 10425526, which of the following is the correct name: 'stoense rug low pile off white', 'gurli floor cushion grey' or 'brimnes day bed frame with 2 drawers white'?\",\n",
       " \"if item_no is 10425526, which of the following is the correct name: 'stoense rug low pile off white', 'metod base cabinet for sink 2 doors black askersund dark brown ash effect' or 'knoxhult base cabinet with doors and drawer wood effect grey'?\",\n",
       " \"if name is stoense rug low pile off white, what item_no does it refer to? '49466484', '10425526', '20457673' or '39456065'?\",\n",
       " \"query:'104.255.26'\\nthe query above returns item_no 10425526 as a result. what is its rank? 10, 1, 3 or 3\",\n",
       " \"if item_no is 70429545, which of the following is the correct name: 'stoense rug low pile purple' or 'bergmund chair with long cover black inseros white'?\",\n",
       " \"query:'carpet purple'\\ndoes the query above return item_no 70429545 as a result. yes or no?\",\n",
       " \"query:'medium rugs'\\nthe query above returns item_no 70429545 as a result. what is its rank? 4, 8, 2 or 6\",\n",
       " \"query:'mat purple '\\nthe query above returns item_no 70429545 as a result. what is its rank? 4, 3, 7 or 1\",\n",
       " \"query:'240 cm rug'\\ndoes the query above return item_no 70429545 as a result. yes or no?\",\n",
       " \"query:'purple rug 120'\\nthe query above returns item_no 70429545 as a result. what is its rank? 10, 6, 2 or 1\",\n",
       " \"if name is stoense rug low pile purple, what item_no does it refer to? '29396091', '19443173', '70429545' or '30456531'?\",\n",
       " \"query:'lilac placemats'\\ndoes the query above return item_no 70429545 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 70429545. yes or no?',\n",
       " \"if name is stoense rug low pile purple, what item_no does it refer to? '20472918', '60333063', '50430535' or '70429545'?\",\n",
       " 'this tv-bench has large drawers that make it easy to keep remote controls, game controllers and other tv accessories organised.,cable outlets make it easy to lead cables out the back, so they’re hidden from view but close at hand when you need them. is the previous sentence a description of item_no 70429545. yes or no?',\n",
       " \"if item_no is 70429545, which of the following is the correct name: 'besta tv bench with doors and drawers white sutterviken kabbarp grey beige' or 'stoense rug low pile purple'?\",\n",
       " \"if name is stoense rug low pile purple, what item_no does it refer to? '00453063', '70429545', '50343802' or '10363632'?\",\n",
       " \"query:'rugs medium '\\ndoes the query above return item_no 70429545 as a result. yes or no?\",\n",
       " \"query:'stoens'\\nthe query above returns item_no 70429545 as a result. what is its rank? 1, 3, 4, 10 or 5\",\n",
       " \"query:'rug pirple'\\ndoes the query above return item_no 70429545 as a result. yes or no?\",\n",
       " \"if name is marstrup rug low pile beige, what item_no does it refer to? '70427122' or '80482132'?\",\n",
       " \"query:'rug, low pile'\\nthe query above returns item_no 80482132 as a result. what is its rank? 6, 7 or 1\",\n",
       " \"query:'rugs beige'\\nthe query above returns item_no 80482132 as a result. what is its rank? 1, 7, 10, 7 or 8\",\n",
       " 'the pattern and colors are easy to coordinate with many different styles.,durable and stain resistant since the rug is made of synthetic fibers. the material does not shed and is very easy to care for.,the thick pile dampens sound and provides a soft surface to walk on.,nice and soft for your feet and comfy to sit on.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 80482132. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 80482132. yes or no?',\n",
       " \"query:'large beige rug'\\ndoes the query above return item_no 80482132 as a result. yes or no?\",\n",
       " \"query:'large cream rug'\\ndoes the query above return item_no 80482132 as a result. yes or no?\",\n",
       " \"if item_no is 80482132, which of the following is the correct name: 'marstrup rug low pile beige' or 'besta storage combination w glass doors white selsviken high gloss white frosted glass'?\",\n",
       " \"query:'marstrup rug'\\nthe query above returns item_no 80482132 as a result. what is its rank? 3, 8, 3, 8 or 1\",\n",
       " \"query:'berber rug'\\nthe query above returns item_no 80482132 as a result. what is its rank? 2 or 10\",\n",
       " \"query:'berber cream rugs'\\ndoes the query above return item_no 80482132 as a result. yes or no?\",\n",
       " \"if item_no is 80482132, which of the following is the correct name: 'marstrup rug low pile beige', 'lagkapten table top white stained oak effect' or 'besta storage combination with doors black brown kallviken stubbarp dark grey concrete effect'?\",\n",
       " \"query:'berber cream rugs'\\nthe query above returns item_no 80482132 as a result. what is its rank? 8, 10 or 1\",\n",
       " \"if name is marstrup rug low pile beige, what item_no does it refer to? '80482132' or '59455994'?\",\n",
       " \"if name is marstrup rug low pile beige, what item_no does it refer to? '40111322', '80482132' or '00431151'?\",\n",
       " \"query:'cream rug '\\ndoes the query above return item_no 80482132 as a result. yes or no?\",\n",
       " 'you can also charge your mobile phone when it’s in the holder. is the previous sentence a description of item_no 40408021. yes or no?',\n",
       " 'worktop with a thick walnut veneer, a hardwearing natural material that can be sanded and surface treated when required.,layer construction adds stability and makes the worktop less sensitive to humidity, thus, less likely to bend, split or crack than solid wood.,for quick installation and easy maintenance the worktop is pre-treated with hard wax oil.,the worktop has a chevron design that gives it a sophisticated look that supports all kitchen styles.,you can cut the worktop to the length you want and cover the edges with the 2 included edging strips.,good environmental choice, because the method of using a top layer of wood on particleboard is resource-efficient.,25 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 40408021. yes or no?',\n",
       " \"query:'rugs large woven'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"query:'natural rug large'\\nthe query above returns item_no 40408021 as a result. what is its rank? 10 or 3\",\n",
       " 'tv bench with drawers. is the previous sentence a summary of item_no 40408021. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 40408021. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 40408021. yes or no?',\n",
       " 'in this storage combination, there’s room for your tv and for displaying your favourite things on the open, adjustable shelves.,adjustable shelves, so you can customise your storage as needed.,cable outlets make it easy to lead cables out the back, so they’re hidden from view but close at hand when you need them.,smooth-running drawers with drawer stops to keep them in place. is the previous sentence a description of item_no 40408021. yes or no?',\n",
       " \"query:'baige and blue rug'\\nthe query above returns item_no 40408021 as a result. what is its rank? 1, 8, 5 or 8\",\n",
       " \"query:'rug jute '\\nthe query above returns item_no 40408021 as a result. what is its rank? 3, 9, 6, 4 or 10\",\n",
       " 'jute is a durable and recyclable material with natural colour variations.,the jute has natural colour variations that make each rug distinct and unique. is the previous sentence a description of item_no 40408021. yes or no?',\n",
       " \"query:'weave rugs'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"query:'large rug natural material'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"query:'133x195 cm'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"query:'rug £59'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"query:'40408021'\\ndoes the query above return item_no 40408021 as a result. yes or no?\",\n",
       " \"if name is stockholm 2017 rug flatwoven handmade striped grey, what item_no does it refer to? '90485031', '80345239' or '30435398'?\",\n",
       " \"query:'grey stripe rug'\\ndoes the query above return item_no 80345239 as a result. yes or no?\",\n",
       " \"query:'stockholm 2017 rug flatwoven'\\nthe query above returns item_no 80345239 as a result. what is its rank? 1, 8, 2, 3 or 8\",\n",
       " \"if name is stockholm 2017 rug flatwoven handmade striped grey, what item_no does it refer to? '90291288' or '80345239'?\",\n",
       " \"if name is stockholm 2017 rug flatwoven handmade striped grey, what item_no does it refer to? '30328534' or '80345239'?\",\n",
       " \"query:'flat weave rug grey'\\nthe query above returns item_no 80345239 as a result. what is its rank? 2, 5 or 4\",\n",
       " \"query:'handwoven rugs'\\ndoes the query above return item_no 80345239 as a result. yes or no?\",\n",
       " \"query:'black white rug'\\ndoes the query above return item_no 80345239 as a result. yes or no?\",\n",
       " 'tv storage combination/glass doors. is the previous sentence a summary of item_no 80345239. yes or no?',\n",
       " 'a smooth-running, full-extension pull-out interior fitting with push-to-open feature and built-in dampers so that it closes slowly, softly and quietly.,push to open; no handles needed.,handle-free doors create a streamlined look; push to open.,the smooth-running drawer with drawer stop provides a good overview of the contents and makes things easily accessible.,25 year guarantee. read about the terms in the guarantee brochure. is the previous sentence a description of item_no 80345239. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 80345239. yes or no?',\n",
       " \"if name is stockholm 2017 rug flatwoven handmade striped grey, what item_no does it refer to? '69904237', '80345239', '39230712' or '99403012'?\",\n",
       " \"query:'kitchen rugs '\\nthe query above returns item_no 80345239 as a result. what is its rank? 5, 6 or 2\",\n",
       " \"query:'stockholm striped grey '\\nthe query above returns item_no 80345239 as a result. what is its rank? 3, 10, 2 or 1\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 80345239. yes or no?',\n",
       " \"if name is stockholm 2017 rug flatwoven handmade striped grey, what item_no does it refer to? '80457646' or '80345239'?\",\n",
       " \"query:'stockholm carpet'\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"if name is stockholm rug flatwoven handmade striped black off white, what item_no does it refer to? '90103254' or '90469984'?\",\n",
       " \"query:'stocholm'\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"query:'rug handmade'\\nthe query above returns item_no 90103254 as a result. what is its rank? 5, 4, 6, 2 or 1\",\n",
       " \"if item_no is 90103254, which of the following is the correct name: 'stockholm rug flatwoven handmade striped black off white', 'groenlid cover 3 seat sofa w chaise longue inseros white' or 'metod maximera base cabinet with 2 drawers black axstad dark grey'?\",\n",
       " \"if item_no is 90103254, which of the following is the correct name: 'radgraes fabric white beige striped' or 'stockholm rug flatwoven handmade striped black off white'?\",\n",
       " \"query:'large low pile rugs'\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"if name is stockholm rug flatwoven handmade striped black off white, what item_no does it refer to? '49429694', '10420562', '90103254' or '59455994'?\",\n",
       " \"query:'large dining room rug'\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"query:'rugs 250 x 350'\\nthe query above returns item_no 90103254 as a result. what is its rank? 5 or 1\",\n",
       " \"query:'centre carpet'\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"query:'carpet black and white '\\ndoes the query above return item_no 90103254 as a result. yes or no?\",\n",
       " \"query:'rug grey and white stripe'\\nthe query above returns item_no 90103254 as a result. what is its rank? 4, 4, 1, 3 or 2\",\n",
       " 'tabletop. is the previous sentence a summary of item_no 90103254. yes or no?',\n",
       " \"query:'stockholm rug'\\nthe query above returns item_no 90103254 as a result. what is its rank? 10, 7, 7, 10 or 1\",\n",
       " \"if item_no is 90103254, which of the following is the correct name: 'trofast storage combination light white stained pine turquoise', 'gracioes rug pink blue' or 'stockholm rug flatwoven handmade striped black off white'?\",\n",
       " \"query:'soft rugs large'\\ndoes the query above return item_no 30390861 as a result. yes or no?\",\n",
       " \"if name is torrild rug low pile multicolour, what item_no does it refer to? '30390861', '59409110', '80407835' or '69316498'?\",\n",
       " \"query:'carpets '\\nthe query above returns item_no 30390861 as a result. what is its rank? 9, 9, 8 or 1\",\n",
       " \"query:'rugs low pile'\\ndoes the query above return item_no 30390861 as a result. yes or no?\",\n",
       " \"query:'large colourful rug'\\nthe query above returns item_no 30390861 as a result. what is its rank? 2 or 8\",\n",
       " \"if item_no is 30390861, which of the following is the correct name: 'torrild rug low pile multicolour' or 'platsa wardrobe with 3 doors white fonnes white'?\",\n",
       " \"query:'mat '\\nthe query above returns item_no 30390861 as a result. what is its rank? 9 or 4\",\n",
       " 'shelving unit. is the previous sentence a summary of item_no 30390861. yes or no?',\n",
       " \"query:'torrild rug'\\nthe query above returns item_no 30390861 as a result. what is its rank? 1, 10 or 2\",\n",
       " \"query:'carpet large'\\nthe query above returns item_no 30390861 as a result. what is its rank? 1 or 4\",\n",
       " \"if name is torrild rug low pile multicolour, what item_no does it refer to? '30390861', '79329297' or '19402950'?\",\n",
       " \"query:'torrild rug '\\ndoes the query above return item_no 30390861 as a result. yes or no?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 30390861. yes or no?',\n",
       " 'the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres. is the previous sentence a description of item_no 30390861. yes or no?',\n",
       " \"query:'colourful rug'\\ndoes the query above return item_no 30390861 as a result. yes or no?\",\n",
       " 'the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres. is the previous sentence a description of item_no 30390861. yes or no?',\n",
       " \"if item_no is 00345846, which of the following is the correct name: 'groenlid cvr crnr sofa bed 5 seat w chs lng tallmyra light red', 'soenderoed rug high pile blue' or 'hemnes chest of 6 drawers grey stained'?\",\n",
       " \"if item_no is 00345846, which of the following is the correct name: 'soenderoed rug high pile blue', 'brimnes storage combination w glass doors black', 'varsta front for dishwasher stainless steel' or 'platsa tv bench white'?\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 00345846. yes or no?',\n",
       " \"query:'rugs large blue'\\nthe query above returns item_no 00345846 as a result. what is its rank? 4, 6, 9 or 3\",\n",
       " 'rug, high pile. is the previous sentence a summary of item_no 00345846. yes or no?',\n",
       " \"query:'thick pile rug'\\ndoes the query above return item_no 00345846 as a result. yes or no?\",\n",
       " \"query:'teal and grey carpet'\\nthe query above returns item_no 00345846 as a result. what is its rank? 2, 3, 3, 8 or 1\",\n",
       " \"query:'rug thick pile large'\\ndoes the query above return item_no 00345846 as a result. yes or no?\",\n",
       " 'you can choose to sit or stand - whatever is most comfortable while you eat, work, study, prepare food, tend to your herbs, chat with friends or anything else you like to do around the table.,add hooks, hanging storage and containers on the storage ladder to store the things you need close by.,sturdy, durable and easy to keep clean with storage ladder in strong steel and smooth melamine table top with a dark wood look. is the previous sentence a description of item_no 00345846. yes or no?',\n",
       " \"query:'00345846 '\\ndoes the query above return item_no 00345846 as a result. yes or no?\",\n",
       " \"query:'sale rug'\\ndoes the query above return item_no 00345846 as a result. yes or no?\",\n",
       " \"query:'large blue rugs'\\ndoes the query above return item_no 00345846 as a result. yes or no?\",\n",
       " \"if item_no is 00345846, which of the following is the correct name: 'soenderoed rug high pile blue' or 'elloven padlock white'?\",\n",
       " \"if item_no is 00345846, which of the following is the correct name: 'soenderoed rug high pile blue', 'foerbaettra cover panel high gloss white', 'langdans roller blind grey' or 'besta storage combination with drawers laxviken black selsviken high gloss black'?\",\n",
       " \"if item_no is 00345846, which of the following is the correct name: 'voxtorp cover panel walnut effect', 'laetthet handle white' or 'soenderoed rug high pile blue'?\",\n",
       " \"if name is soenderoed rug high pile blue, what item_no does it refer to? '50492726' or '00345846'?\",\n",
       " \"query:'knardup'\\nthe query above returns item_no 40492595 as a result. what is its rank? 2, 10 or 8\",\n",
       " \"if name is knardrup rug low pile light gray, what item_no does it refer to? '69284987' or '40492595'?\",\n",
       " \"if name is knardrup rug low pile light gray, what item_no does it refer to? '19321591', '40492595', '30493350' or '19322449'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 40492595. yes or no?',\n",
       " \"query:'carpet large'\\ndoes the query above return item_no 40492595 as a result. yes or no?\",\n",
       " \"if name is knardrup rug low pile light gray, what item_no does it refer to? '40321141', '40492595', '89325171' or '20476167'?\",\n",
       " 'wardrobe. is the previous sentence a summary of item_no 40492595. yes or no?',\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 40492595. yes or no?',\n",
       " \"query:'grey soft rug'\\ndoes the query above return item_no 40492595 as a result. yes or no?\",\n",
       " \"query:'grey thick rug'\\nthe query above returns item_no 40492595 as a result. what is its rank? 2 or 6\",\n",
       " \"query:'large grey rug 200x300'\\nthe query above returns item_no 40492595 as a result. what is its rank? 7, 3, 1 or 7\",\n",
       " \"query:'carpets grey'\\ndoes the query above return item_no 40492595 as a result. yes or no?\",\n",
       " \"query:'knardrup '\\nthe query above returns item_no 40492595 as a result. what is its rank? 6 or 5\",\n",
       " \"query:'large grey carpet'\\ndoes the query above return item_no 40492595 as a result. yes or no?\",\n",
       " \"if name is knardrup rug low pile light gray, what item_no does it refer to? '40492595' or '50517859'?\",\n",
       " 'the soft color blends easily with other textiles and home furnishings.,a timeless design with a sheen effect.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibers.,this rug is made of recycled polyester from sources like pet bottles.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 40492595. yes or no?',\n",
       " \"query:'billy bookcase lighting'\\ndoes the query above return item_no 80492616 as a result. yes or no?\",\n",
       " \"if name is knardrup rug low pile white, what item_no does it refer to? '70359991', '80492616', '10456693' or '30160407'?\",\n",
       " \"if item_no is 80492616, which of the following is the correct name: 'knardrup rug low pile white' or 'besta tv bench with doors and drawers white lappviken stubbarp sindvik'?\",\n",
       " 'tv bench with doors and drawers. is the previous sentence a summary of item_no 80492616. yes or no?',\n",
       " \"query:'rug white'\\nthe query above returns item_no 80492616 as a result. what is its rank? 8, 6 or 10\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 80492616. yes or no?',\n",
       " \"query:'rugs knardrup'\\nthe query above returns item_no 80492616 as a result. what is its rank? 4, 3, 6, 1 or 4\",\n",
       " \"query:'rugs white'\\ndoes the query above return item_no 80492616 as a result. yes or no?\",\n",
       " \"query:'low rug'\\nthe query above returns item_no 80492616 as a result. what is its rank? 3 or 5\",\n",
       " 'the drawer has an integrated push-opener, so you don’t need handles or knobs and can open it with just a light push.,the drawer runners are adjustable so you can adjust the front both horizontally and in depth.,it’s easy to keep the cables from your tv and other devices out of sight but close at hand, as there are several cable outlets at the back of the tv bench.,the two drawers make it easy to keep remote controls, game controllers and other tv accessories organised.,adjustable feet make the tv bench stand steady also on an uneven floor.,optimise and organise your bestå storage with boxes and inserts that you like. is the previous sentence a description of item_no 80492616. yes or no?',\n",
       " \"if item_no is 80492616, which of the following is the correct name: 'dimforsen bath towel turquoise', 'knardrup rug low pile white', 'metod maximera bc w pull out work surface 3drw white bodarp grey green' or 'snaepp pedal bin white'?\",\n",
       " \"if name is knardrup rug low pile white, what item_no does it refer to? '70273092' or '80492616'?\",\n",
       " \"query:'cream rugs'\\ndoes the query above return item_no 80492616 as a result. yes or no?\",\n",
       " \"if item_no is 80492616, which of the following is the correct name: 'besta tv bench white hanviken stubbarp white clear glass', 'knardrup rug low pile white', 'besta wall mounted cabinet combination black brown kallviken dark grey concrete effect' or 'hemnes wash stand with 2 drawers white'?\",\n",
       " \"if item_no is 80492616, which of the following is the correct name: 'besta storage combination with doors white stained oak effect glassvik stubbarp white frosted glass', 'ikea 365 serving plate white', 'knardrup rug low pile white' or 'sinnlig scented block candle red garden berries red'?\",\n",
       " 'the soft colour blends easily with other textiles and home furnishings. ,a timeless design with a sheen effect.,the dense, thick pile dampens sound and provides a soft surface to walk on.,durable, stain resistant and easy to care for since the rug is made of synthetic fibres.,this rug is made of recycled polyester from sources like pet bottles.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 80492616. yes or no?',\n",
       " 'you can customise spacing as you need, because the shelf is adjustable.,sturdy frame construction, 18 mm thick.,snap-on hinges can be mounted on the door without screws, and you can easily remove the door for cleaning. is the previous sentence a description of item_no 00277395. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 00277395. yes or no?',\n",
       " \"query:'wooden floors'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " \"if name is lohals rug flatwoven natural, what item_no does it refer to? '29436978', '30419973', '00277395' or '89248371'?\",\n",
       " \"query:'jute rug 300'\\nthe query above returns item_no 00277395 as a result. what is its rank? 1 or 9\",\n",
       " \"query:'woven rug natural'\\nthe query above returns item_no 00277395 as a result. what is its rank? 4 or 1\",\n",
       " \"query:'flatwoven natural'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " \"query:'rug 200x 300'\\nthe query above returns item_no 00277395 as a result. what is its rank? 5, 2, 7 or 8\",\n",
       " \"query:'jute carpets'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 00277395. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 00277395. yes or no?',\n",
       " \"query:'lohals 200x300 natural'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " \"query:'lohall'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " \"query:'large carpets rugs'\\ndoes the query above return item_no 00277395 as a result. yes or no?\",\n",
       " \"if name is lohals rug flatwoven natural, what item_no does it refer to? '00277395' or '80378338'?\",\n",
       " \"if item_no is 00277395, which of the following is the correct name: 'platsa frame white', 'langaryd 3 seat sofa w chaise longue right lejde grey black wood', 'hedeviken drawer front dark brown stained oak veneer' or 'lohals rug flatwoven natural'?\",\n",
       " \"query:'mat dining table'\\nthe query above returns item_no 10433753 as a result. what is its rank? 4, 2 or 8\",\n",
       " 'rug. is the previous sentence a summary of item_no 10433753. yes or no?',\n",
       " \"query:'carpet for bedroom '\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " \"query:'mat dining table'\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " 'swivel chair. is the previous sentence a summary of item_no 10433753. yes or no?',\n",
       " \"query:'grey soft rug '\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " \"if name is hjorthede rug handmade grey, what item_no does it refer to? '10433753', '19284640', '49399215' or '60511338'?\",\n",
       " \"query:'rug for bedroom'\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " 'handwoven by skilled craftspeople, and therefore unique.,the rug is made of wool so it’s naturally soil-repellent and very durable.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 10433753. yes or no?',\n",
       " 'fitted sheet. is the previous sentence a summary of item_no 10433753. yes or no?',\n",
       " \"query:'thick rugs'\\nthe query above returns item_no 10433753 as a result. what is its rank? 2 or 10\",\n",
       " \"query:'bedroom grey rug'\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " 'handwoven by skilled craftspeople, and therefore unique.,the rug is made of wool so it’s naturally soil-repellent and very durable.,a perfect companion for all types of flooring, even those with underfloor heating. . is the previous sentence a description of item_no 10433753. yes or no?',\n",
       " \"query:'rug 240 '\\nthe query above returns item_no 10433753 as a result. what is its rank? 3 or 10\",\n",
       " \"query:'fully rug'\\ndoes the query above return item_no 10433753 as a result. yes or no?\",\n",
       " \"if name is hjorthede rug handmade grey, what item_no does it refer to? '79197584' or '10433753'?\",\n",
       " \"query:'rug large low'\\nthe query above returns item_no 30374564 as a result. what is its rank? 9, 1, 2, 5 or 6\",\n",
       " \"query:'low pile large rug'\\ndoes the query above return item_no 30374564 as a result. yes or no?\",\n",
       " \"if name is broenden rug low pile handmade multicolour, what item_no does it refer to? '50396923', '50430535' or '30374564'?\",\n",
       " \"query:'rugs pattern'\\nthe query above returns item_no 30374564 as a result. what is its rank? 7, 4, 8, 1 or 5\",\n",
       " \"query:'stripes rug'\\ndoes the query above return item_no 30374564 as a result. yes or no?\",\n",
       " \"query:'rug wool'\\nthe query above returns item_no 30374564 as a result. what is its rank? 2, 3, 5 or 6\",\n",
       " \"query:'eye mask'\\nthe query above returns item_no 30374564 as a result. what is its rank? 3, 8, 5, 1 or 2\",\n",
       " \"if name is broenden rug low pile handmade multicolour, what item_no does it refer to? '10417055', '09414736' or '30374564'?\",\n",
       " 'rug, low pile. is the previous sentence a summary of item_no 30374564. yes or no?',\n",
       " \"query:'lounge rugs'\\ndoes the query above return item_no 30374564 as a result. yes or no?\",\n",
       " \"if name is broenden rug low pile handmade multicolour, what item_no does it refer to? '09275113', '59157541', '00450031' or '30374564'?\",\n",
       " 'this frame is ideal to use with alfta adhesive hook. with the hook you can easily hang the frame without nails or screws and decorate your walls with pictures.,can be used hanging or standing to fit in the space available.,ph-neutral mount; will not discolour the picture.,front protection in plastic makes the frame safer to use. is the previous sentence a description of item_no 30374564. yes or no?',\n",
       " 'handwoven by skilled craftspeople, each one is unique.,the rug is made of wool so it’s naturally soil-repellent and very durable.,the rug is hand-woven by skilled craftspeople and adds a personal touch to your room. is the previous sentence a description of item_no 30374564. yes or no?',\n",
       " \"if name is broenden rug low pile handmade multicolour, what item_no does it refer to? '40456899' or '30374564'?\",\n",
       " \"query:'rugs in brown and light brown '\\ndoes the query above return item_no 30374564 as a result. yes or no?\",\n",
       " \"query:'handmade rug'\\ndoes the query above return item_no 30374564 as a result. yes or no?\",\n",
       " \"query:'240 x 100'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " 'hand-woven by skilled craftspeople, each one is unique.,made in india in organized weaving centers with good working conditions and fair wages.,the rug is made of wool so it’s naturally soil-repellent and very durable. is the previous sentence a description of item_no 20374569. yes or no?',\n",
       " \"query:'geometric rugs'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'rugs 170'\\nthe query above returns item_no 20374569 as a result. what is its rank? 5, 4, 3, 2 or 8\",\n",
       " \"query:'living room carpet '\\nthe query above returns item_no 20374569 as a result. what is its rank? 6, 6, 6, 4 or 2\",\n",
       " 'picture with frame. is the previous sentence a summary of item_no 20374569. yes or no?',\n",
       " \"query:'handmade rugs'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'kollund'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'large wool rug'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'collund'\\nthe query above returns item_no 20374569 as a result. what is its rank? 10, 1, 5 or 10\",\n",
       " \"query:'rug kollund'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"if item_no is 20374569, which of the following is the correct name: 'kollund rug flatwoven handmade gray', 'havsta nest of tables set of 2 grey', 'askersund drawer front light ash effect' or 'satsumas plant stand bamboo white'?\",\n",
       " \"query:'rugs grey 240 '\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'20374569'\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " \"query:'living room carpet '\\ndoes the query above return item_no 20374569 as a result. yes or no?\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 20374569. yes or no?',\n",
       " \"query:'160x 230 rug'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"if name is morum rug flatwoven in outdoor dark grey, what item_no does it refer to? '19408424' or '40203557'?\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 40203557. yes or no?',\n",
       " \"query:'large grey rug'\\nthe query above returns item_no 40203557 as a result. what is its rank? 10, 8, 1 or 9\",\n",
       " 'rug flatwoven, in/outdoor. is the previous sentence a summary of item_no 40203557. yes or no?',\n",
       " \"if item_no is 40203557, which of the following is the correct name: 'uppdatera utensil tray white', 'morum rug flatwoven in outdoor dark grey' or 'idasen shelving unit beige'?\",\n",
       " \"query:'navy rug'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"query:'artificisl plant'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"query:'rug dining '\\nthe query above returns item_no 40203557 as a result. what is its rank? 1, 2, 6 or 8\",\n",
       " \"query:'rugs large grey'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"if item_no is 40203557, which of the following is the correct name: 'morum rug flatwoven in outdoor dark grey' or 'begripa handle yellow half round'?\",\n",
       " \"query:'grey carpets '\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"query:'160x230 rugs'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"query:'rug 160 x 230'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"if name is morum rug flatwoven in outdoor dark grey, what item_no does it refer to? '99465321' or '40203557'?\",\n",
       " \"query:'ruga'\\ndoes the query above return item_no 40203557 as a result. yes or no?\",\n",
       " \"query:'stripe rug'\\ndoes the query above return item_no 20500153 as a result. yes or no?\",\n",
       " 'unique detail with hand-cut fringe pattern that reveals the darker yarn underneath.,the wool is undyed and therefore has a natural white color.,a perfect companion for all types of flooring, even those with underfloor heating. is the previous sentence a description of item_no 20500153. yes or no?',\n",
       " \"query:'flatwoven ruh'\\nthe query above returns item_no 20500153 as a result. what is its rank? 1, 8, 7 or 10\",\n",
       " \"query:'large white wool rug'\\nthe query above returns item_no 20500153 as a result. what is its rank? 4, 2, 5, 1 or 10\",\n",
       " \"if item_no is 20500153, which of the following is the correct name: 'kinnared decoration stickers pink peony', 'enhet drawer front grey frame', 'pedersborg rug flatwoven natural off white' or 'stabben pedal bin stainless steel'?\",\n",
       " \"if item_no is 20500153, which of the following is the correct name: 'metod corner base cab w pull out fitting white jaersta high gloss light turquoise', 'metod wall cabinet with 2 glass doors black axstad dark grey', 'pedersborg rug flatwoven natural off white' or 'kungsfors dish drainer'?\",\n",
       " \"query:'large woven mat'\\ndoes the query above return item_no 20500153 as a result. yes or no?\",\n",
       " \"query:'flatwoven natural'\\nthe query above returns item_no 20500153 as a result. what is its rank? 4, 5, 5 or 6\",\n",
       " \"query:'woven natural rugs'\\ndoes the query above return item_no 20500153 as a result. yes or no?\",\n",
       " 'desk. is the previous sentence a summary of item_no 20500153. yes or no?',\n",
       " 'headboard. is the previous sentence a summary of item_no 20500153. yes or no?',\n",
       " \"query:'wool rugs '\\ndoes the query above return item_no 20500153 as a result. yes or no?\",\n",
       " 'base cab f hob/int extractor w drw. is the previous sentence a summary of item_no 20500153. yes or no?',\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 20500153. yes or no?',\n",
       " \"query:'medium rugs'\\ndoes the query above return item_no 20500153 as a result. yes or no?\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 20500153. yes or no?',\n",
       " \"if item_no is 00345243, which of the following is the correct name: 'besta tv storage combination glass doors black brown selsviken high gloss black clear glass', 'saeljan worktop light grey mineral effect laminate', 'vessla storage crate with castors white' or 'stockholm 2017 rug flatwoven handmade zigzag pattern orange'?\",\n",
       " \"query:'rugs orange'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"query:'red orange'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"if name is stockholm 2017 rug flatwoven handmade zigzag pattern orange, what item_no does it refer to? '40291667', '10511741' or '00345243'?\",\n",
       " \"query:'rugs for living room'\\nthe query above returns item_no 00345243 as a result. what is its rank? 6 or 1\",\n",
       " \"query:'carpet living room'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"query:'stockholm 2017 rug flatwoven'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"query:'india'\\nthe query above returns item_no 00345243 as a result. what is its rank? 5, 4, 9 or 1\",\n",
       " \"query:'rugs large wool'\\nthe query above returns item_no 00345243 as a result. what is its rank? 2 or 6\",\n",
       " \"query:'medium hand made rug'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"if item_no is 00345243, which of the following is the correct name: 'stockholm 2017 rug flatwoven handmade zigzag pattern orange', 'bergshult krokshult wall shelf white anthracite', 'musik wall lamp brass colour' or 'ekedalen table and 6 chairs dark brown hakebo dark grey'?\",\n",
       " 'handwoven by skilled craftspeople, each one is unique. made in india in organised weaving centres with good working conditions and fair wages.,the durable, soil-resistant wool surface makes this rug perfect in your living room or under your dining table.,the rug has the same pattern on both sides, so you can turn it over and it will withstand more wear and last even longer.,ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum. is the previous sentence a description of item_no 00345243. yes or no?',\n",
       " 'smooth running drawer with pull-out stop.,made of solid wood, which is a hardwearing and warm natural material. is the previous sentence a description of item_no 00345243. yes or no?',\n",
       " \"query:'00345243'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"query:'wool rugs large'\\ndoes the query above return item_no 00345243 as a result. yes or no?\",\n",
       " \"if item_no is 00345243, which of the following is the correct name: 'ivar shelving unit pine', 'stockholm 2017 rug flatwoven handmade zigzag pattern orange' or 'kivik cover for chaise longue kelinge grey turquoise'?\",\n",
       " \"query:'rugs large blue'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " 'ideal in your living room or under your dining table since the flat-woven surface makes it easy to pull out the chairs and vacuum.,the rug has the same pattern on both sides, so you can turn it over and it will withstand more wear and last even longer. is the previous sentence a description of item_no 10348241. yes or no?',\n",
       " \"if item_no is 10348241, which of the following is the correct name: 'hydrangea potted plant hydrangea assorted colours', 'bodbyn door grey', 'skatval door dark gray' or 'fasterholt rug flatwoven dark blue'?\",\n",
       " \"query:'blue carpet'\\nthe query above returns item_no 10348241 as a result. what is its rank? 5, 9, 2 or 8\",\n",
       " \"if name is fasterholt rug flatwoven dark blue, what item_no does it refer to? '10348241', '79419448' or '20321142'?\",\n",
       " \"query:'rug %c2%a355'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"query:'fasterhalt'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"query:'blue chevron '\\nthe query above returns item_no 10348241 as a result. what is its rank? 1 or 9\",\n",
       " \"query:'rug dining'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"query:'flat weave rug '\\nthe query above returns item_no 10348241 as a result. what is its rank? 4, 2, 4 or 6\",\n",
       " \"query:'rugs #'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"query:'dinning room rug'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"if name is fasterholt rug flatwoven dark blue, what item_no does it refer to? '30384619' or '10348241'?\",\n",
       " \"query:'flat wooven'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " \"query:'blue flatwoven rug'\\ndoes the query above return item_no 10348241 as a result. yes or no?\",\n",
       " 'storage combination with doors. is the previous sentence a summary of item_no 10348241. yes or no?',\n",
       " \"query:'rug living room'\\ndoes the query above return item_no 20470518 as a result. yes or no?\",\n",
       " \"query:'living room rug '\\nthe query above returns item_no 20470518 as a result. what is its rank? 6, 7 or 9\",\n",
       " \"if name is spangsbro rug flatwoven handmade multicolour, what item_no does it refer to? '20470518', '90332255', '09424274' or '09324284'?\",\n",
       " \"query:'ibsker rug'\\ndoes the query above return item_no 20470518 as a result. yes or no?\",\n",
       " \"query:'medium small rug'\\nthe query above returns item_no 20470518 as a result. what is its rank? 1, 3, 3 or 10\",\n",
       " \"query:'dining room inspiration '\\nthe query above returns item_no 20470518 as a result. what is its rank? 9, 1 or 8\",\n",
       " \"query:'bauhaus rug'\\nthe query above returns item_no 20470518 as a result. what is its rank? 6, 6 or 1\",\n",
       " 'rug, flatwoven. is the previous sentence a summary of item_no 20470518. yes or no?',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  0.1436,   3.8750,   0.5352,  ...,  30.8750,   1.3281, -21.5000],\n",
       "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
       "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
       "        ...,\n",
       "        [ -7.5457,  -5.5364,   1.9197,  ...,   3.3695,  -2.7438, -18.4989],\n",
       "        [  3.6785,  -2.3788, -10.6894,  ..., -11.4406, -17.3757,  -8.8708],\n",
       "        [ -7.4763,  -3.4569,  -4.4757,  ...,  -1.4042,  -0.7912,  -9.8190]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/generation_utils.py:1207: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a spokesman for the british government said tuesday'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"20470518\"\n",
    "inputs = tokenizer.encode(\n",
    "    inp, return_tensors='pt')\n",
    "inputs = inputs.to(\"cuda:0\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6036 [00:01<3:02:18,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2796, device='cuda:0')\n",
      "lm_grad tensor(6531968, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3785, device='cuda:0')\n",
      "lm_grad tensor(2733, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3813, device='cuda:0')\n",
      "lm_grad tensor(2839, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3758, device='cuda:0')\n",
      "lm_grad tensor(7540, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3770, device='cuda:0')\n",
      "lm_grad tensor(8485, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6561, device='cuda:0')\n",
      "lm_grad tensor(8699, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6561, device='cuda:0')\n",
      "lm_grad tensor(10501, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(6563, device='cuda:0')\n",
      "lm_grad tensor(11384, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6036 [00:09<8:39:52,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7562, device='cuda:0')\n",
      "lm_grad tensor(12464, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1000, device='cuda:0')\n",
      "lm_grad tensor(1017, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(997, device='cuda:0')\n",
      "lm_grad tensor(1050, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2874, device='cuda:0')\n",
      "lm_grad tensor(1942, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2846, device='cuda:0')\n",
      "lm_grad tensor(1960, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2856, device='cuda:0')\n",
      "lm_grad tensor(1952, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2830, device='cuda:0')\n",
      "lm_grad tensor(1941, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2831, device='cuda:0')\n",
      "lm_grad tensor(1951, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6036 [00:16<10:27:12,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4750, device='cuda:0')\n",
      "lm_grad tensor(2837, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1029, device='cuda:0')\n",
      "lm_grad tensor(1016, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1023, device='cuda:0')\n",
      "lm_grad tensor(1887, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1055, device='cuda:0')\n",
      "lm_grad tensor(2017, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1041, device='cuda:0')\n",
      "lm_grad tensor(3112, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(3858, device='cuda:0')\n",
      "lm_grad tensor(4035, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(5728, device='cuda:0')\n",
      "lm_grad tensor(4122, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(7619, device='cuda:0')\n",
      "lm_grad tensor(4146, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/6036 [00:24<11:17:44,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11318, device='cuda:0')\n",
      "lm_grad tensor(5116, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1056, device='cuda:0')\n",
      "lm_grad tensor(960, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1991, device='cuda:0')\n",
      "lm_grad tensor(1940, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2008, device='cuda:0')\n",
      "lm_grad tensor(1926, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1998, device='cuda:0')\n",
      "lm_grad tensor(1934, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(2006, device='cuda:0')\n",
      "lm_grad tensor(1919, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1968, device='cuda:0')\n",
      "lm_grad tensor(1912, device='cuda:0')\n",
      "here3\n",
      "emb_grad tensor(1947, device='cuda:0')\n",
      "lm_grad tensor(1920, device='cuda:0')\n",
      "here3\n",
      "emb_grad "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/6036 [00:31<11:45:36,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4721, device='cuda:0')\n",
      "lm_grad tensor(2833, device='cuda:0')\n",
      "INSIDE emb_grad tensor(0, device='cuda:0')\n",
      "INSIDE lm_grad tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embeddings = list(model.named_parameters())[0][1]\n",
    "lm_head = list(model.named_parameters())[-1][1]\n",
    "init_len = len(tokenizer)-len(items)\n",
    "for epoch in range(1, args['num_train_epochs'] + 1):\n",
    "    model.train()\n",
    "\n",
    "    # freeze encoder updates if specified\n",
    "    if args['freeze_encoder']:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "                param.requires_grad = False\n",
    "            if name.startswith('shared') or name.startswith(\"lm_head\"):\n",
    "                grad_mask = torch.ones_like(param)\n",
    "                grad_mask[:init_len,:] = 0\n",
    "                h = param.register_hook(lambda grad: grad * grad_mask)\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / 8\n",
    "        accelerator.backward(loss)\n",
    "        print(\"emb_grad\", (embeddings.grad>0).sum())\n",
    "        print(\"lm_grad\", (lm_head.grad>0).sum())\n",
    "        if (\n",
    "                step % 8 == 0\n",
    "                or step == len(train_dataloader) - 1\n",
    "        ):\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            loss = loss.item()\n",
    "\n",
    "            item_ind = int(global_steps/2) #two batches per item_id\n",
    "            item_no = items[item_ind]\n",
    "\n",
    "            # get gradients for embedding matrix and lm_head\n",
    "            # embedding_grad_item = torch.norm(emb_grad[init_len+item_ind,:], p=2, dim=-1)\n",
    "            # embedding_grad_rest = torch.norm(torch.cat((emb_grad[init_len:init_len+item_ind,:], \n",
    "            #                                             emb_grad[init_len+item_ind+1:,:])),\n",
    "            #                                 p=2,\n",
    "            #                                 dim=-1).mean()\n",
    "            # lm_grad_item = torch.norm(lm_grad[init_len+item_ind,:], p=2, dim=-1)\n",
    "            # lm_grad_rest = torch.norm(torch.cat((lm_grad[init_len:init_len+item_ind,:], \n",
    "            #                                             lm_grad[init_len+item_ind+1:,:])),\n",
    "            #                                 p=2,\n",
    "            #                                 dim=-1).mean()\n",
    "            # wandb.log({\"loss\": loss, 'item_no':item_no, 'embedding_grad_item':embedding_grad_item,\n",
    "            #  'embedding_grad_rest':embedding_grad_rest, 'lm_grad_item':lm_grad_item,\n",
    "            #  'lm_grad_rest':lm_grad_rest}, step=global_steps)\n",
    "            # global_steps += 1\n",
    "        if step>=32:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.7607e-02, -6.2256e-02, -9.7656e-02,  ...,  2.6245e-02,\n",
       "          4.2725e-02, -2.9541e-02],\n",
       "        [-6.7383e-02, -1.2878e-02, -1.5234e-01,  ..., -1.4355e-01,\n",
       "         -2.8564e-02,  6.4453e-02],\n",
       "        [-3.1006e-02, -1.5820e-01,  2.6758e-01,  ...,  4.6143e-02,\n",
       "         -7.4707e-02,  1.6992e-01],\n",
       "        ...,\n",
       "        [ 5.1124e+05, -6.2280e+04, -7.8284e+04,  ..., -2.1716e+05,\n",
       "         -6.6607e+04, -3.2049e+04],\n",
       "        [ 7.0905e+03, -6.2312e+02, -1.0533e+03,  ..., -2.8763e+03,\n",
       "         -6.3681e+02, -4.1033e+02],\n",
       "        [ 1.4768e+03, -1.1156e+02, -2.2923e+02,  ..., -5.1655e+02,\n",
       "         -1.7873e+02, -6.2893e+01]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23757/3196268026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "v = torch.ones((emb.shape)).to('cuda:0')\n",
    "v2 = next(iter(lm._backward_hooks))(v)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lm._backward_hooks.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(grad)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb._backward_hooks[list(emb._backward_hooks.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 32128]), torch.Size([2048, 32128]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_n = torch.norm(emb, p=2, dim=-1)\n",
    "lm_n = torch.norm(lm, p=2, dim=-1 )\n",
    "emb_ = emb.T/emb_n\n",
    "lm_ = lm.T/lm_n\n",
    "lm_.shape, emb_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    heat_map = torch.matmul(emb_.cpu(), lm_.T.cpu())\n",
    "    ax = sns.heatmap(heat_map, norm=LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 125,   19,    3,    9, 9321,   58,    1]], device='cuda:0')\n",
      "torch.Size([1, 7, 2048])\n",
      "torch.Size([1, 7, 32128])\n",
      "tensor([  773, 18391, 22154,  9631,  7531, 25900, 11001], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kind Standortalbeitrésboygefordertpermalink'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tokenizer.encode('what is a cow?', return_tensors='pt').to('cuda:0')\n",
    "#l = l[0][0]\n",
    "print(l)\n",
    "l_e = embedding_module(l)\n",
    "print(l_e.shape)\n",
    "l_lm = lm_head(l_e)\n",
    "print(l_lm.shape)\n",
    "m = nn.Softmax(dim=-1)\n",
    "d = m(l_lm).max(axis=-1)[1][0]\n",
    "print(d)\n",
    "tokenizer.decode(d, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/06/2022 11:55:13 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgikok\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gikok/t0code2/training/wandb/run-20220706_115513-39wegzjz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8081/gikok/grad_test/runs/39wegzjz\" target=\"_blank\">glad-serenity-5</a></strong> to <a href=\"http://localhost:8081/gikok/grad_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"http://localhost:8081/gikok/grad_test/runs/39wegzjz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ffa7bb1b290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='grad_test', config={'a':'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/24144 [04:19<55:59:29,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "3.676435708999634"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = emb[init_len+item_ind,:]\n",
    "b = torch.cat((emb[init_len:init_len+item_ind,:], emb[init_len+item_ind+1:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_grad_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45.2586, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForSeq2Seq(tokenizer=PreTrainedTokenizerFast(name_or_path='bigscience/T0_3B', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}), model=T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.max(\n",
       " values=tensor([ 46.5000,  65.5000,  32.0000,  58.5000,  20.7500,  61.0000,  67.5000,\n",
       "         112.0000, 122.5000,  86.5000], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " indices=tensor([ 757,  190, 1051,  711,  303,  793,  190,  843,  843,  843],\n",
       "        device='cuda:0')),\n",
       " torch.return_types.max(\n",
       " values=tensor([0.2393, 1.4375, 0.5039, 0.3613, 0.6602, 0.2559, 0.2520, 0.3145, 0.2852,\n",
       "         0.4180], device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " indices=tensor([1506, 1035,  161,  351,  431, 1809,  351,  679,  710,  161],\n",
       "        device='cuda:0')))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0:10,:].max(axis=-1), lm[0:10,:].max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n",
      "lol\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/3758117431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;32m/tmp/ipykernel_20577/1684194404.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;31m#param.detach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('shared') or name.startswith(\"lm_head\"):\n",
    "        t = torch.zeros(param.shape).to('cuda:0')\n",
    "        t[-len(items):,:] = 1\n",
    "        param.register_hook(lambda grad: grad*t)\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "global_steps += 1\n",
    "loss = loss.item()\n",
    "if accelerator.is_main_process:\n",
    "    tqdm.write(f\"epoch = {1}, step = {global_steps}, loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/1934986589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'last' is not defined"
     ]
    }
   ],
   "source": [
    "last(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values[0][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items):,:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Embedding(38136, 2048)\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "38136\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l +=1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l,mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(38136, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=38136, bias=False)\n",
      ")\n",
      "************** 1\n",
      "Embedding(38136, 2048)\n",
      "************** 2\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 3\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 4\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 5\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 6\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 7\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 8\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 9\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 10\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 11\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 12\n",
      "Embedding(32, 32)\n",
      "************** 13\n",
      "T5LayerNorm()\n",
      "************** 14\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 15\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 16\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 17\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 18\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 19\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 20\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 21\n",
      "NewGELUActivation()\n",
      "************** 22\n",
      "T5LayerNorm()\n",
      "************** 23\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 24\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 25\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 26\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 27\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 28\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 29\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 30\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 31\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 32\n",
      "T5LayerNorm()\n",
      "************** 33\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 34\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 35\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 36\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 37\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 38\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 39\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 40\n",
      "T5LayerNorm()\n",
      "************** 41\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 42\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 43\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 44\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 45\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 46\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 47\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 48\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 49\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 50\n",
      "T5LayerNorm()\n",
      "************** 51\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 52\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 53\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 54\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 55\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 56\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 57\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 58\n",
      "T5LayerNorm()\n",
      "************** 59\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 60\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 61\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 62\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 63\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 64\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 65\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 66\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 67\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 68\n",
      "T5LayerNorm()\n",
      "************** 69\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 70\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 71\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 72\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 73\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 74\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 75\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 76\n",
      "T5LayerNorm()\n",
      "************** 77\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 78\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 79\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 80\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 81\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 82\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 83\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 84\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 85\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 86\n",
      "T5LayerNorm()\n",
      "************** 87\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 88\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 89\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 90\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 91\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 92\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 93\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 94\n",
      "T5LayerNorm()\n",
      "************** 95\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 96\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 97\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 98\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 99\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 100\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 101\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 102\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 103\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 104\n",
      "T5LayerNorm()\n",
      "************** 105\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 106\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 107\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 108\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 109\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 110\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 111\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 112\n",
      "T5LayerNorm()\n",
      "************** 113\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 114\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 115\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 116\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 117\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 118\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 119\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 120\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 121\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 122\n",
      "T5LayerNorm()\n",
      "************** 123\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 124\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 125\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 126\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 127\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 128\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 129\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 130\n",
      "T5LayerNorm()\n",
      "************** 131\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 132\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 133\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 134\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 135\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 136\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 137\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 138\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 139\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 140\n",
      "T5LayerNorm()\n",
      "************** 141\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 142\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 143\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 144\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 145\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 146\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 147\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 148\n",
      "T5LayerNorm()\n",
      "************** 149\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 150\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 151\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 152\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 153\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 154\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 155\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 156\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 157\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 158\n",
      "T5LayerNorm()\n",
      "************** 159\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 160\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 161\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 162\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 163\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 164\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 165\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 166\n",
      "T5LayerNorm()\n",
      "************** 167\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 168\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 169\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 170\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 171\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 172\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 173\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 174\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 175\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 176\n",
      "T5LayerNorm()\n",
      "************** 177\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 178\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 179\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 180\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 181\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 182\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 183\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 184\n",
      "T5LayerNorm()\n",
      "************** 185\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 186\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 187\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 188\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 189\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 190\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 191\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 192\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 193\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 194\n",
      "T5LayerNorm()\n",
      "************** 195\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 196\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 197\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 198\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 199\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 200\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 201\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 202\n",
      "T5LayerNorm()\n",
      "************** 203\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 204\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 205\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 206\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 207\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 208\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 209\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 210\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 211\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 212\n",
      "T5LayerNorm()\n",
      "************** 213\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 214\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 215\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 216\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 217\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 218\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 219\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 220\n",
      "T5LayerNorm()\n",
      "************** 221\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 222\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 223\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 224\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 225\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 226\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 227\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 228\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 229\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 230\n",
      "T5LayerNorm()\n",
      "************** 231\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 232\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 233\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 234\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 235\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 236\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 237\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 238\n",
      "T5LayerNorm()\n",
      "************** 239\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 240\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 241\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 242\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 243\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 244\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 245\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 246\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 247\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 248\n",
      "T5LayerNorm()\n",
      "************** 249\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 250\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 251\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 252\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 253\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 254\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 255\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 256\n",
      "T5LayerNorm()\n",
      "************** 257\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 258\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 259\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 260\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 261\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 262\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 263\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 264\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 265\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 266\n",
      "T5LayerNorm()\n",
      "************** 267\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 268\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 269\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 270\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 271\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 272\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 273\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 274\n",
      "T5LayerNorm()\n",
      "************** 275\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 276\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 277\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 278\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 279\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 280\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 281\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 282\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 283\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 284\n",
      "T5LayerNorm()\n",
      "************** 285\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 286\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 287\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 288\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 289\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 290\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 291\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 292\n",
      "T5LayerNorm()\n",
      "************** 293\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 294\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 295\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 296\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 297\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 298\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 299\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 300\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 301\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 302\n",
      "T5LayerNorm()\n",
      "************** 303\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 304\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 305\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 306\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 307\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 308\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 309\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 310\n",
      "T5LayerNorm()\n",
      "************** 311\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 312\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 313\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 314\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 315\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 316\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 317\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 318\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 319\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 320\n",
      "T5LayerNorm()\n",
      "************** 321\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 322\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 323\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 324\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 325\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 326\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 327\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 328\n",
      "T5LayerNorm()\n",
      "************** 329\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 330\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 331\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 332\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 333\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 334\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 335\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 336\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 337\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 338\n",
      "T5LayerNorm()\n",
      "************** 339\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 340\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 341\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 342\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 343\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 344\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 345\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 346\n",
      "T5LayerNorm()\n",
      "************** 347\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 348\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 349\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 350\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 351\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 352\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 353\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 354\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 355\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 356\n",
      "T5LayerNorm()\n",
      "************** 357\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 358\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 359\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 360\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 361\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 362\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 363\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 364\n",
      "T5LayerNorm()\n",
      "************** 365\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 366\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 367\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 368\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 369\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 370\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 371\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 372\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 373\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 374\n",
      "T5LayerNorm()\n",
      "************** 375\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 376\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 377\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 378\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 379\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 380\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 381\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 382\n",
      "T5LayerNorm()\n",
      "************** 383\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 384\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 385\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 386\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 387\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 388\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 389\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 390\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 391\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 392\n",
      "T5LayerNorm()\n",
      "************** 393\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 394\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 395\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 396\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 397\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 398\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 399\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 400\n",
      "T5LayerNorm()\n",
      "************** 401\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 402\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 403\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 404\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 405\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 406\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 407\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 408\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 409\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 410\n",
      "T5LayerNorm()\n",
      "************** 411\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 412\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 413\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 414\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 415\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 416\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 417\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 418\n",
      "T5LayerNorm()\n",
      "************** 419\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 420\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 421\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 422\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 423\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 424\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 425\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 426\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 427\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 428\n",
      "T5LayerNorm()\n",
      "************** 429\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 430\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 431\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 432\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 433\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 434\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 435\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 436\n",
      "T5LayerNorm()\n",
      "************** 437\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 438\n",
      "T5LayerNorm()\n",
      "************** 439\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 440\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 441\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 442\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 443\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 444\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 445\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 446\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 447\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 448\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 449\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 450\n",
      "Embedding(32, 32)\n",
      "************** 451\n",
      "T5LayerNorm()\n",
      "************** 452\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 453\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 454\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 455\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 456\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 457\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 458\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 459\n",
      "T5LayerNorm()\n",
      "************** 460\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 461\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 462\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 463\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 464\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 465\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 466\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 467\n",
      "T5LayerNorm()\n",
      "************** 468\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 469\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 470\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 471\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 472\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 473\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 474\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 475\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 476\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 477\n",
      "T5LayerNorm()\n",
      "************** 478\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 479\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 480\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 481\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 482\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 483\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 484\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 485\n",
      "T5LayerNorm()\n",
      "************** 486\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 487\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 488\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 489\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 490\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 491\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 492\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 493\n",
      "T5LayerNorm()\n",
      "************** 494\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 495\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 496\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 497\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 498\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 499\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 500\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 501\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 502\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 503\n",
      "T5LayerNorm()\n",
      "************** 504\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 505\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 506\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 507\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 508\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 509\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 510\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 511\n",
      "T5LayerNorm()\n",
      "************** 512\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 513\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 514\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 515\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 516\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 517\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 518\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 519\n",
      "T5LayerNorm()\n",
      "************** 520\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 521\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 522\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 523\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 524\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 525\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 526\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 527\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 528\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 529\n",
      "T5LayerNorm()\n",
      "************** 530\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 531\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 532\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 533\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 534\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 535\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 536\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 537\n",
      "T5LayerNorm()\n",
      "************** 538\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 539\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 540\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 541\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 542\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 543\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 544\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 545\n",
      "T5LayerNorm()\n",
      "************** 546\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 547\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 548\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 549\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 550\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 551\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 552\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 553\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 554\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 555\n",
      "T5LayerNorm()\n",
      "************** 556\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 557\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 558\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 559\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 560\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 561\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 562\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 563\n",
      "T5LayerNorm()\n",
      "************** 564\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 565\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 566\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 567\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 568\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 569\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 570\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 571\n",
      "T5LayerNorm()\n",
      "************** 572\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 573\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 574\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 575\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 576\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 577\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 578\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 579\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 580\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 581\n",
      "T5LayerNorm()\n",
      "************** 582\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 583\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 584\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 585\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 586\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 587\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 588\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 589\n",
      "T5LayerNorm()\n",
      "************** 590\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 591\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 592\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 593\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 594\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 595\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 596\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 597\n",
      "T5LayerNorm()\n",
      "************** 598\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 599\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 600\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 601\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 602\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 603\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 604\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 605\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 606\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 607\n",
      "T5LayerNorm()\n",
      "************** 608\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 609\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 610\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 611\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 612\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 613\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 614\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 615\n",
      "T5LayerNorm()\n",
      "************** 616\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 617\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 618\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 619\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 620\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 621\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 622\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 623\n",
      "T5LayerNorm()\n",
      "************** 624\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 625\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 626\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 627\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 628\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 629\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 630\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 631\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 632\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 633\n",
      "T5LayerNorm()\n",
      "************** 634\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 635\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 636\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 637\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 638\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 639\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 640\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 641\n",
      "T5LayerNorm()\n",
      "************** 642\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 643\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 644\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 645\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 646\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 647\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 648\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 649\n",
      "T5LayerNorm()\n",
      "************** 650\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 651\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 652\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 653\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 654\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 655\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 656\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 657\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 658\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 659\n",
      "T5LayerNorm()\n",
      "************** 660\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 661\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 662\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 663\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 664\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 665\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 666\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 667\n",
      "T5LayerNorm()\n",
      "************** 668\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 669\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 670\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 671\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 672\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 673\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 674\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 675\n",
      "T5LayerNorm()\n",
      "************** 676\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 677\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 678\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 679\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 680\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 681\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 682\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 683\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 684\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 685\n",
      "T5LayerNorm()\n",
      "************** 686\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 687\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 688\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 689\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 690\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 691\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 692\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 693\n",
      "T5LayerNorm()\n",
      "************** 694\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 695\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 696\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 697\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 698\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 699\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 700\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 701\n",
      "T5LayerNorm()\n",
      "************** 702\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 703\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 704\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 705\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 706\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 707\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 708\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 709\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 710\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 711\n",
      "T5LayerNorm()\n",
      "************** 712\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 713\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 714\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 715\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 716\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 717\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 718\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 719\n",
      "T5LayerNorm()\n",
      "************** 720\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 721\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 722\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 723\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 724\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 725\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 726\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 727\n",
      "T5LayerNorm()\n",
      "************** 728\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 729\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 730\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 731\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 732\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 733\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 734\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 735\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 736\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 737\n",
      "T5LayerNorm()\n",
      "************** 738\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 739\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 740\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 741\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 742\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 743\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 744\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 745\n",
      "T5LayerNorm()\n",
      "************** 746\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 747\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 748\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 749\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 750\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 751\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 752\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 753\n",
      "T5LayerNorm()\n",
      "************** 754\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 755\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 756\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 757\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 758\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 759\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 760\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 761\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 762\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 763\n",
      "T5LayerNorm()\n",
      "************** 764\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 765\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 766\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 767\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 768\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 769\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 770\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 771\n",
      "T5LayerNorm()\n",
      "************** 772\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 773\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 774\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 775\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 776\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 777\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 778\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 779\n",
      "T5LayerNorm()\n",
      "************** 780\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 781\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 782\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 783\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 784\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 785\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 786\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 787\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 788\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 789\n",
      "T5LayerNorm()\n",
      "************** 790\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 791\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 792\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 793\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 794\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 795\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 796\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 797\n",
      "T5LayerNorm()\n",
      "************** 798\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 799\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 800\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 801\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 802\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 803\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 804\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 805\n",
      "T5LayerNorm()\n",
      "************** 806\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 807\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 808\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 809\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 810\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 811\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 812\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 813\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 814\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 815\n",
      "T5LayerNorm()\n",
      "************** 816\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 817\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 818\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 819\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 820\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 821\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 822\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 823\n",
      "T5LayerNorm()\n",
      "************** 824\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 825\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 826\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 827\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 828\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 829\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 830\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 831\n",
      "T5LayerNorm()\n",
      "************** 832\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 833\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 834\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 835\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 836\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 837\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 838\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 839\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 840\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 841\n",
      "T5LayerNorm()\n",
      "************** 842\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 843\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 844\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 845\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 846\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 847\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 848\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 849\n",
      "T5LayerNorm()\n",
      "************** 850\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 851\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 852\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 853\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 854\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 855\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 856\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 857\n",
      "T5LayerNorm()\n",
      "************** 858\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 859\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 860\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 861\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 862\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 863\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 864\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 865\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 866\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 867\n",
      "T5LayerNorm()\n",
      "************** 868\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 869\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 870\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 871\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 872\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 873\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 874\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 875\n",
      "T5LayerNorm()\n",
      "************** 876\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 877\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 878\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 879\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 880\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 881\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 882\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 883\n",
      "T5LayerNorm()\n",
      "************** 884\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 885\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 886\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 887\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 888\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 889\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 890\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 891\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 892\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 893\n",
      "T5LayerNorm()\n",
      "************** 894\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 895\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 896\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 897\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 898\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 899\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 900\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 901\n",
      "T5LayerNorm()\n",
      "************** 902\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 903\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 904\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 905\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 906\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 907\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 908\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 909\n",
      "T5LayerNorm()\n",
      "************** 910\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 911\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 912\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 913\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 914\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 915\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 916\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 917\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 918\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 919\n",
      "T5LayerNorm()\n",
      "************** 920\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 921\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 922\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 923\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 924\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 925\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 926\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 927\n",
      "T5LayerNorm()\n",
      "************** 928\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 929\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 930\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 931\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 932\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 933\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 934\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 935\n",
      "T5LayerNorm()\n",
      "************** 936\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 937\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 938\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 939\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 940\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 941\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 942\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 943\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 944\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 945\n",
      "T5LayerNorm()\n",
      "************** 946\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 947\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 948\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 949\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 950\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 951\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 952\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 953\n",
      "T5LayerNorm()\n",
      "************** 954\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 955\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 956\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 957\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 958\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 959\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 960\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 961\n",
      "T5LayerNorm()\n",
      "************** 962\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 963\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 964\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 965\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 966\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 967\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 968\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 969\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 970\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 971\n",
      "T5LayerNorm()\n",
      "************** 972\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 973\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 974\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 975\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 976\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 977\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 978\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 979\n",
      "T5LayerNorm()\n",
      "************** 980\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 981\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 982\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 983\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 984\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 985\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 986\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 987\n",
      "T5LayerNorm()\n",
      "************** 988\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 989\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 990\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 991\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 992\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 993\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 994\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 995\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 996\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 997\n",
      "T5LayerNorm()\n",
      "************** 998\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 999\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1000\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1001\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1002\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1003\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1004\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1005\n",
      "T5LayerNorm()\n",
      "************** 1006\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1007\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1008\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1009\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1010\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1011\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1012\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1013\n",
      "T5LayerNorm()\n",
      "************** 1014\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1015\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1016\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1017\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1018\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1019\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1020\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1021\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1022\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1023\n",
      "T5LayerNorm()\n",
      "************** 1024\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1025\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1026\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1027\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1028\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1029\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1030\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1031\n",
      "T5LayerNorm()\n",
      "************** 1032\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1033\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1034\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1035\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1036\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1037\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1038\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1039\n",
      "T5LayerNorm()\n",
      "************** 1040\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1041\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1042\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1043\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1044\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1045\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1046\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1047\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1048\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1049\n",
      "T5LayerNorm()\n",
      "************** 1050\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1051\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1052\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1053\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1054\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1055\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1056\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1057\n",
      "T5LayerNorm()\n",
      "************** 1058\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1059\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1060\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1061\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1062\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1063\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1064\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1065\n",
      "T5LayerNorm()\n",
      "************** 1066\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1067\n",
      "T5LayerNorm()\n",
      "************** 1068\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1069\n",
      "Linear(in_features=2048, out_features=38136, bias=False)\n",
      "************** 1070\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (relative_attention_bias): Embedding(32, 32)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
