{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": 1,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                                    "  from .autonotebook import tqdm as notebook_tqdm\n"
                              ]
                        }
                  ],
                  "source": [
                        "import argparse\n",
                        "import logging\n",
                        "import os\n",
                        "import random\n",
                        "import pandas as pd\n",
                        "from dataclasses import dataclass\n",
                        "from itertools import chain\n",
                        "from typing import Optional, Union\n",
                        "import csv\n",
                        "import math\n",
                        "import numpy as np\n",
                        "import matplotlib.pyplot as plt\n",
                        "\n",
                        "import datasets\n",
                        "import torch\n",
                        "from datasets import load_dataset, load_metric, DatasetDict\n",
                        "from torch.utils.data import DataLoader\n",
                        "from tqdm.auto import tqdm\n",
                        "\n",
                        "import transformers\n",
                        "from accelerate import Accelerator\n",
                        "from transformers import (\n",
                        "    AutoConfig,\n",
                        "    AutoModelForSeq2SeqLM,\n",
                        "    AutoTokenizer,\n",
                        "    PreTrainedTokenizerBase,\n",
                        "    default_data_collator,\n",
                        "    DataCollatorForSeq2Seq,\n",
                        "    AdamW,\n",
                        "    SchedulerType,\n",
                        "    get_scheduler,\n",
                        "    set_seed,\n",
                        ")\n",
                        "from transformers.file_utils import PaddingStrategy\n",
                        "from promptsource.templates import DatasetTemplates\n",
                        "pd.set_option(\"display.max_rows\", 1200)\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "n = 1000\n",
                        "model_dir = \"output_1000_plus\"\n",
                        "df = pd.read_csv(f'/home/gikok/{model_dir}/stats_{n}.csv')\n",
                        "# accuracy = np.zeros(n-1)\n",
                        "# for i in range(1, n):\n",
                        "#     temp = pd.read_csv(f'/home/gikok/{model_dir}/stats_{i}.csv')\n",
                        "#     accuracy[i-1] = temp['accuracy'].iloc[0]\n",
                        "# model_dir = \"output\"\n",
                        "# df2 = pd.read_csv(f'/home/gikok/{model_dir}/stats_{n}.csv')\n",
                        "# accuracy2 = np.zeros(n-1)\n",
                        "# for i in range(1, n):\n",
                        "#     temp = pd.read_csv(f'/home/gikok/{model_dir}/stats_{i}.csv')\n",
                        "#     accuracy2[i-1] = temp['accuracy'].iloc[0]\n",
                        "# # model_dir = \"output2\"\n",
                        "# # df3 = pd.read_csv(f'/home/gikok/{model_dir}/stats_{n}.csv')\n",
                        "# # accuracy3 = np.zeros(n-1)\n",
                        "# # for i in range(1, n):\n",
                        "# #     temp = pd.read_csv(f'/home/gikok/{model_dir}/stats_{i}.csv')\n",
                        "# #     accuracy3[i-1] = temp['accuracy'].iloc[0]\n",
                        "    "
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "df.to_csv(f'/home/gikok/{model_dir}/stats.csv')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "n = 342\n",
                        "model_dir = \"output_no_GA\"\n",
                        "df = pd.read_csv(f'/home/gikok/{model_dir}/stats.csv')\n",
                        "even = np.arange(0, len(df)-1, 2)\n",
                        "even1 = np.arange(1, len(df), 2)\n",
                        "df1 = df.iloc[even]\n",
                        "df2 = df.iloc[even1]"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "e = df.groupby(df.index//200).median()\n",
                        "et =  df.groupby(df.index//200).max()\n",
                        "eb =  df.groupby(df.index//200).quantile(0.25)\n",
                        "e2 = df2.groupby(df2.index//200).max()\n",
                        "et2 =  df2.groupby(df2.index//200).quantile(0.75)\n",
                        "eb2 =  df2.groupby(df2.index//200).quantile(0.25)\n",
                        "l = np.arange(len(e))\n",
                        "plt.rcParams['font.size'] = 16\n",
                        "# plt.plot(l, e[\"training_loss\"], label = 'Task 1', c='green')\n",
                        "plt.plot(l, et[\"test_loss\"], c='green', ls='--')\n",
                        "# plt.plot(l, eb[\"training_loss\"], c='green', ls='--')\n",
                        "\n",
                        "plt.plot(l, e2[\"test_loss\"], label = 'Task 2', c='red', alpha=0.6)\n",
                        "# plt.plot(l, et2[\"training_loss\"], c='red', ls='--')\n",
                        "# plt.plot(l, eb2[\"training_loss\"], c='red', ls='--')\n",
                        "\n",
                        "plt.xlabel('Epoch')\n",
                        "plt.ylabel('Loss')\n",
                        "plt.title(\"2 options\")\n",
                        "#plt.ylim(3e-1, 10)\n",
                        "plt.legend()\n",
                        "# plt.yscale('log')\n",
                        "plt.show()\n",
                        "plt.plot(l, abs(e[\"lm_grad_pct\"]), label = 'Task 1')\n",
                        "plt.plot(l, abs(e2[\"lm_grad_pct\"]), label = 'Task 2')\n",
                        "plt.xlabel('Epoch')\n",
                        "plt.yscale('log')\n",
                        "plt.title(\"Output Vocabulary\")\n",
                        "plt.ylabel('<Gradient>')\n",
                        "plt.legend()\n",
                        "#plt.yscale('log')\n",
                        "plt.show()\n",
                        "plt.plot(l, abs(e[\"em_grad_pct\"]), label= 'Task 1')\n",
                        "plt.plot(l, abs(e2[\"em_grad_pct\"]), label='Task 2')\n",
                        "plt.xlabel('Epoch')\n",
                        "plt.ylabel('<Gradient>')\n",
                        "plt.title(\"Input Vocabulary\")\n",
                        "plt.legend()\n",
                        "plt.yscale('log')\n",
                        "plt.show()\n",
                        "plt.plot(l, e[\"lm_delta_pct\"], '.', label = 'LM delta (prompt 1)')\n",
                        "plt.plot(l, e2[\"lm_delta_pct\"], '.', label = 'LM delta (prompt 2)')\n",
                        "plt.xlabel('Epoch')\n",
                        "#plt.ylim([-0.2, 0.2])\n",
                        "plt.ylabel('<Delta> (%)')\n",
                        "plt.legend()\n",
                        "plt.yscale('log')\n",
                        "plt.show()\n",
                        "plt.plot(l, e[\"em_delta_pct\"], '.', label= 'EM delta (prompt 1)')\n",
                        "plt.plot(l, e2[\"em_delta_pct\"], '.', label=' EM delta (prompt 2)')\n",
                        "plt.xlabel('Epoch')\n",
                        "#plt.ylim([-2e-3, 2e-3])\n",
                        "plt.ylabel('<Delta> (%)')\n",
                        "plt.legend()\n",
                        "plt.yscale('log')\n",
                        "plt.show()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "torch.cuda.device_count()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": []
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": []
            },
            {
                  "cell_type": "code",
                  "execution_count": 2,
                  "metadata": {
                        "collapsed": false,
                        "pycharm": {
                              "name": "#%%\n"
                        }
                  },
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Model and tokenizer loaded\n",
                                    "Moved model to GPUs\n"
                              ]
                        }
                  ],
                  "source": [
                        "#Load model and tokenizer\n",
                        "\n",
                        "model_name = \"bigscience/T0_3B\"\n",
                        "\n",
                        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
                        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                        "print(\"Model and tokenizer loaded\")\n",
                        "\n",
                        "#model.to('cuda:0')\n",
                        "print(\"Moved model to GPUs\")\n",
                        "\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 44,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import itertools\n",
                        "o = ['is red', 'vechicle', 'is Nissan']\n",
                        "li = list(itertools.permutations(o))\n",
                        "lis = [f\"{', '.join(v)}\" for v in li]\n",
                        "b = '; '.join(lis)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 76,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Q: list: car, dog, cat. in the previous list, how many items that are similar to animals?   1, 2, 3, 4, 5, 6, 7, 8, 9, 10?\n",
                                    "A: 10\n"
                              ]
                        }
                  ],
                  "source": [
                        "inp = f\"list: car, dog, cat. in the previous list, how many items that are similar to animals?   1, 2, 3, 4, 5, 6, 7, 8, 9, 10?\"\n",
                        "inputs = tokenizer.encode(\n",
                        "    inp, return_tensors='pt')\n",
                        "inputs = inputs#.to(\"cuda:0\")\n",
                        "with torch.no_grad():\n",
                        "    outputs = model.generate(inputs)\n",
                        "print(\"Q: \" + inp)\n",
                        "print(\"A: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 43,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[19026, 1]"
                                    ]
                              },
                              "execution_count": 43,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokenizer.encode('Nissan')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "inp = \"What is the top speed of a leopard?\"\n",
                        "inputs = tokenizer.encode(\n",
                        "    inp, return_tensors='pt')\n",
                        "inputs = inputs.to(\"cuda:0\")\n",
                        "with torch.no_grad():\n",
                        "    outputs = model.generate(inputs)\n",
                        "print(\"Q: \" + inp)\n",
                        "print(\"A: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "embeddings = list(model.named_parameters())[0][1]\n",
                        "lm_head = list(model.named_parameters())[-1][1]\n",
                        "n_new = 100\n",
                        "og = embeddings[:-n_new,:].detach().cpu()\n",
                        "new = embeddings[-n_new:,:].detach().cpu()\n",
                        "og_vals = np.zeros((2048, 30))\n",
                        "og_bins = np.zeros((2048, 30))\n",
                        "for i in range(2048):\n",
                        "    val, bin = np.histogram(og[:,i], 30, normed=True)\n",
                        "    og_vals[i] = val\n",
                        "    og_bins[i] = (bin[1:]+bin[:-1])/2\n",
                        "new_vals = np.zeros((2048, 30))\n",
                        "new_bins = np.zeros((2048, 30))\n",
                        "for i in range(2048):\n",
                        "    val, bin = np.histogram(new[:,i], 30, normed=True)\n",
                        "    new_vals[i] = val\n",
                        "    new_bins[i] = (bin[1:]+bin[:-1])/2\n",
                        "for i in range(2048):\n",
                        "    plt.plot(og_bins[i], og_vals[i], c='blue', alpha=0.1)\n",
                        "plt.xlabel('weight value')\n",
                        "plt.ylabel('PDF')\n",
                        "plt.title(\"Embedding Layer\")\n",
                        "plt.show()\n",
                        "for i in range(2048):\n",
                        "    plt.plot(new_bins[i], new_vals[i], c='blue', alpha=0.1)\n",
                        "plt.xlabel('weight value')\n",
                        "plt.ylabel('PDF')\n",
                        "plt.title(\"New EM\")\n",
                        "plt.show()\n",
                        "n_new = 100\n",
                        "og = lm_head[:-n_new,:].detach().cpu()\n",
                        "new = lm_head[-n_new:,:].detach().cpu()\n",
                        "og_vals = np.zeros((2048, 30))\n",
                        "og_bins = np.zeros((2048, 30))\n",
                        "for i in range(2048):\n",
                        "    val, bin = np.histogram(og[:,i], 30, normed=True)\n",
                        "    og_vals[i] = val\n",
                        "    og_bins[i] = (bin[1:]+bin[:-1])/2\n",
                        "new_vals = np.zeros((2048, 30))\n",
                        "new_bins = np.zeros((2048, 30))\n",
                        "for i in range(2048):\n",
                        "    val, bin = np.histogram(new[:,i], 30, normed=True)\n",
                        "    new_vals[i] = val\n",
                        "    new_bins[i] = (bin[1:]+bin[:-1])/2\n",
                        "for i in range(2048):\n",
                        "    plt.plot(og_bins[i], og_vals[i], c='blue', alpha=0.1)\n",
                        "plt.xlabel('weight value')\n",
                        "plt.ylabel('PDF')\n",
                        "#plt.ylim(0, 15)\n",
                        "plt.title(\"Language Model Head\")\n",
                        "plt.show()\n",
                        "for i in range(2048):\n",
                        "    plt.plot(new_bins[i], new_vals[i], c='blue', alpha=0.1)\n",
                        "plt.xlabel('weight value')\n",
                        "plt.ylabel('PDF')\n",
                        "plt.ylim(0, 10)\n",
                        "plt.title(\"New LM\")\n",
                        "plt.show()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "df = pd.read_parquet('data/miniprompts005.parquet.gzip')\n",
                        "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
                        "w = 1123\n",
                        "print(df['input'].iloc[w][:-12] + \", \" + ', '.join(items))\n",
                        "print(df['target'].iloc[w])\n",
                        "print(items.remove(items[0]))\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import (\n",
                        "    AutoTokenizer,\n",
                        "    AutoModelForSeq2SeqLM,\n",
                        "    LogitsProcessorList,\n",
                        "    MinLengthLogitsProcessor,\n",
                        "    BeamSearchScorer,\n",
                        ")\n",
                        "# instantiate logits processors\n",
                        "logits_processor = LogitsProcessorList(\n",
                        "    [\n",
                        "        MinLengthLogitsProcessor(100, eos_token_id=model.config.eos_token_id),\n",
                        "    ]\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 57,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "Q: What are top 3 descriptions of a whale, most relevant first ? a. animal , mammal ,lives in ocean b. mammal, animal , lives in ocean , c. lives in ocean , mammal , animal\n",
                                    "A: b\n"
                              ]
                        }
                  ],
                  "source": [
                        "w = 1123\n",
                        "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
                        "#items.remove('30493350')\n",
                        "#inp = \"query: 'artificial potted plant'. for the above query what item_no is the best match?\"\n",
                        "inp = \"What are top 3 descriptions of a whale, most relevant first ? a. animal , mammal ,lives in ocean b. mammal, animal , lives in ocean , c. lives in ocean , mammal , animal\"\n",
                        "inp1 = inp  + ', '.join(items)\n",
                        "#inp = \"99429408 and 50401335 are dining tables. is the previous sentence correct? yes or no?\"\n",
                        "inputs = tokenizer.encode(\n",
                        "    inp, return_tensors='pt')\n",
                        "inputs = inputs.to(\"cuda:0\")\n",
                        "with torch.no_grad():\n",
                        "    outputs = model.generate(inputs)#, logits_processor=logits_processor)\n",
                        "print(\"Q: \" + inp)\n",
                        "print(\"A: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                        "#print(\"A: \" + ', '.join((dict.fromkeys((tokenizer.decode(outputs[0], skip_special_tokens=True).split(','))))))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "names = pd.read_parquet(\"data/item_names_100.parquet.gzip\").values.tolist()\n",
                        "for i, item in enumerate(names):\n",
                        "    inp = f\"describe what you know about {item[0]}\"\n",
                        "    inputs = tokenizer.encode(\n",
                        "        inp, return_tensors='pt')\n",
                        "    inputs = inputs.to(\"cuda:0\")\n",
                        "    with torch.no_grad():\n",
                        "        outputs = model.generate(inputs)\n",
                        "    print(item[1])\n",
                        "    print(\"Q: \" + inp)\n",
                        "    print(\"A: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
                        "    print(\"\")\n",
                        "    print(\"\")\n",
                        "    print(\"**************************************************************\")\n",
                        "    print(\"\")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 14,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
                        "cos2 = torch.nn.functional.cosine_similarity"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 37,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "torch.Size([32100, 2048])"
                                    ]
                              },
                              "execution_count": 37,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "v.shape"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 44,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                                    "  \n"
                              ]
                        },
                        {
                              "data": {
                                    "text/plain": [
                                          "['duplicate Yes Brown yes Pra Sage herbal woodland 2.233']"
                                    ]
                              },
                              "execution_count": 44,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "r = cos2(v, lm[-93], dim=-1)\n",
                        "[tokenizer.decode(torch.tensor(r).topk(10, largest=True)[1], skip_special_tokens=False)]"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 8,
                  "metadata": {},
                  "outputs": [
                        {
                              "name": "stderr",
                              "output_type": "stream",
                              "text": [
                                    "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
                                    "/opt/conda/envs/tz/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
                              ]
                        },
                        {
                              "name": "stdout",
                              "output_type": "stream",
                              "text": [
                                    "[['Prozent Verbesserung Erfolg gefördert Erwerb epidemicOrice Kosten Nikola lycée'], ['dimensiuni importante deci decorate significance decorated Decor sophisticationMuzeul criterii'], ['whimsical botanical Național incontournablepuneti poetic culminat informatii moarte rassembl'], ['Crăciun Mă Timişoara Vă Să prénom autorizat démo botezscrise'], ['Bevölkerunggründe presque Bewohnerdécouvrezinfamous Région inhabitants demeure parish'], ['resilience indicateitudine indication Erinnerung underestimate imperative superioaragorge desires'], ['scopul asfalt consacr Zweck aucun rassembl subsidiaries chemindincolo consili'], ['preciz cushion cushions assess erfülltécoute ceci inflammation domni souligne'], ['roastroasted conviction ferment crowdfundingconvicted proprietor Roastaction2.9'], ['serving Dominique joining începutul ehemalige Derekaiming hiring Anfang deadline'], ['Handlung Feier relatii Einkommen Myanmar Entspannung Ausland advertisements Tätigkeit administrativ'], ['conclusion sentence concluded niemandhalte anger equation Judge soluţi solutii'], ['Verarbeitung resurse Ursache Widerruf descarc Putin FAQ Recomand plagiarism Förderung'], ['Kilometer Pharmaceutical uncertainties minerals microscope merveille internationaux plentiful Investigator3,000'], ['indispensable Wurzel Ursache putere overcomeovercomingVielen solution Plumbing Solution'], ['revelation profund perennial genießen empirical masterpiece geniusproprietar intriguing Guatemala'], ['hospice exhaustive lavish Tätigkeit scripture thrive Einwilligung perennial sportive scăpa'], ['regard Procedure Verbesserung reputation Spracheeffet Rouge Körperehr Yi'], ['Proiectul Vermieterţi rejoin cumpara preturi Giurgiu Vă Büro empfi'], ['homework élabor lucrez déclaration diligence réalisation dragostescriere intrebare inteleg'], ['proximité consumator Übersetzung livelihood plage Dropbox repas Necklace schimbat Hochzeit'], ['Google bringt google click well Kendall Jupiter Collins online mind'], ['dependence addiction depression addicted dessert douce dépense doute dolarierfahrung'], ['brûl ihn vollkommen délicat chaudière Schl Außen periculoas contaminationbehält'], ['Bevölkerung Braşov jährlich merveille témoignévénement săptămâna facture lucrez incredibil'], ['Mitte Schmuck lemnbewegung porumb addictedglück Schaden erinnern printemps'], ['offiziell ausdrücklich sfarsitenfin aspectsност facile vinovat sovereignnigsten'], ['Somerset Wirkung nevoi Multumesc dissertation taiat chinez vinovatspannung Saskatchewan'], ['genre evolve phénomènevarying exceptional phenomenon métier region experienta tradition'], ['réalitéformulaireplaîtblatt Volume frustration fragrant poudre Content agréable'], ['prince quatre securitate hart prolific shipped volontaire Harrison march portion'], ['achat protège theoriesOlimp pouvoir Wahrheit sanitar faction assertionatorium'], ['DecorationAssemblée Digitalisierung VereinbarungDezvoltare Verpackung Herstellung Iohannis iuniePART'], ['eventuell intalni apărut Jahres September peisaj sequential Schweizer restoration Sept'], ['sfaturi supraveghere écologique dimineata lungime Românieiagentur legislature Scripture presiune'], ['Empire Germania rebellion Emperor economieidentité Kläger indépendant Exhibition territorial'], ['espagnol canton bracketétablissement colon propriété autoturism Salvador Lafayette Slovakia'], ['voudrai Parchet Konsequenz propuneri gata comport souhaitDecizi Proiect mitigation'], ['institu spezialisiert Russie Dumnezeuamerikanische Freund privée centru cerveau secteur'], ['vermeidenistik annul oversight preț prévention plomberie Erfolg Seller Investition'], ['fulfill Konsequenz niciodata frontier spezialisiert acele juillet fulfillment schwierig mondiale'], ['național aranjperçu precis protecţi întreag petrecut administered vollkommen getestet'], ['satisfaction wardrobe Crăciun complexionoutefois Anleitungépreuve zufrieden fashionable efficiency'], ['décide engagé dépend recession trilogy trackback délai définitive reluctantexposition'], ['pagini episcop privire calatori principii centaine Iaşi meciuri Crăciun asteapta'], ['Romaniei Timişoara României Regulament cumpara ministère congratulations Parlament Latvia Timisoara'], ['roulant génér temperatura celule réelle vigorous épaisfecți vorhandenenţă'], ['Artificialimply viitorul automatisch vizualiz vân aflat artificialiști integriert'], ['actuellement réalis traversintermédiaire termen Zubehör applicantsvețiextrêmement Politic'], ['Verwendung Telefonnummer Investition Vorstand preț Dienstleistungqualität Handlung Primări rezistent'], ['phenomenal réalis ressembl ambition scenarioenvisioned vizualiz goal marvel révél'], ['Förderung intrebarisendung supraveghere ministère réussite participer aprobare persecution Durchführung'], ['factura bill installment Bill summarysystematically 1/4 workflow grandfather tablespoon'], ['Proiectul numitintitulé vielleicht réduit stabilit cercetare urmarihésit Între'], ['Schlüsselexigencevețiţia serrure détour Bestandteil exposé élevé Herausforderung'], ['shelving backsplash Collaboration shelves Blockchaindéposerlipsa entspannt Sherlock Sortiment'], ['sfarsit devine investitii iunie dintre Vertrag privind parle Gründungenţă'], ['determine pinpoint correlatetreffdeterminingcerereaaéroport remplacement discover collapse'], ['Vergangenheit segments segment transformer témoignfollow mentioned Conclusion explained episode'], ['Leute envision descriptive outline summary interesting intellect individualspitfalls Ergebnisse'], ['iuniiTotulrespectarea arhitect păcatemystical unele obiectivventricular Asociaţi'], ['tribunal român Asociaţi psiholog câștig judiciaire erlebt intéressant situatii epidemi'], ['Bearbeitung scurt Processinglucrează exercitii resurseapprentissage Raum Printable mereu'], ['Persönlichkeitevenimentelesehen überhaupt niemand Handeln Gleich grundsätzlich fachlich asuma'], ['begrüß păcate rassembl Pouitorul rappel gonfl générale Orientierung voluntar'], ['atenti proast getrennt opus laissé doar interzis colaborare plâng Cerceta'], ['bewertet météo septembrie trăi tradiţiserait proximité Timişoara méta ungefähr'], ['kämpfeépreuve Ergebnis echipă décide erzielenWettbewerb compléter ceremonies obese'], ['Arriv Fehl Ende carcas Braşov strive maximize ramaneoutskirt Acum'], ['tradiţi réalitéprinzip définition maîtrise matièrepéri expérience démocratie Einwilligung'], ['maxim problema asuma Zweifelprinzip soluţi decis membru decrease sympa'], ['ultimatelyUltimatelygesetzlichen recens Versailles sommes Kunststoff remport soirée process'], ['investissementextrêmement continuu prote paradox Freiheit securitate următorexigence ceva'], ['lendemain etaj Perspective sophomore perspectivevarsity intuition Gegner Monday veille'], ['continuare fachlich Ersatzteile găzdui cercetareshortened Kazakhstan relationships Databasefostering'], ['ţie parfois portail specialitate tro débat rău endlich lurk oferte'], ['curtain curtains Curtain Candidates volet credibility Kreuz mărturi présidentielle wallpaper'], ['Pune Melanie Projekte verdienenished offline profit istoric Balkon Scotia'], ['Jusqu benefic kommekommenden Braşov fehlen mauvaisală résult Baltic'], ['umbrella precaution sponsorship Situation authorization ministère obligation courage candidature announcement'], ['stratégieţiile compétences prevede professeur colaborare crise comunictherapie Theorie'], ['ulei noapte dimineata pericolCartea Rö tăi Montréaleți Peel'], ['Eindruck darüber cercetare Publikum FAQCependant Trouve Wirtschafts Sfant niemand'], ['contient Umfang anuntat extrem Smokță despair magique Kilometer popcorn'], ['motivation operations evolution réalisation supervision consciousness resonate magnitude destiny exceeding'], ['hervorragend ressortAllerdings Cluj RecomandmasăCentrul empfohlen swamp Zambia'], ['abrupt canalisation Ausdruck Varianten Constanţa avantaj chinez studiuVoilà russisch'], ['kopf conseils fingertips unemployment collège tribal metabolism stool cuisine români'], ['prelucr ambience entsprechen representation compliment Lambertexplication sagen reprezentat Übersetzung'], ['cadou Entscheidung streak legătur vizualiz cliqu refroidi calatorispannung accesorii'], ['V Ju AD differdivdict Interpret 1 Timişoara 3'], ['Travail travail vizitat timpului aufmerksam criz émerg révolutionKonferenz très'], ['rencontré trouverrecevoir rencontrer außergewöhnlich întâmpin tânăr Grill répondre résist'], ['Gustav etwa Folk Fußball Füll folk Unterricht Perhaps komplexe Allgemein'], ['Lille offen clutter acestei utopiivism trustworthyéglise endless prevalent'], ['questionnaireêtre pielii Vorgehen untersuchtînt Ereignis porumbintestinErlebnis'], ['Funktionen functi ecosystem jurisdictionidentité Entscheidung essence Interface aspects implementation'], ['Konsequenz felicit oferte reign undergo überzeugt suprafata discrimin dort fapt'], ['molecules psiholog odata colonne paginifolg români practitioners descoperi românească'], ['supraveghere râ Timişoara Erinnerung wardrobe erlebenwirken Constanța attire mă']]\n",
                                    "[['Frame frame framesframeframed Foto Fotografi Photo Photography Lo'], ['Flower flower Pot Garden decorations garden gardens pot Gardenskal'], ['Throw throw throwingthrown blankethrew quilt BedtossedThr'], ['anime Fantasy fantasy Magic paper nicotine RedRED Fiction magic'], ['diningTable Dining table Table Din din farmtable tables'], ['dining Dining din Din extendcorn corner extensions define table'], ['cushion Cu Cushion cushions covers pillow pillowsCu cover Cover'], ['Cu Pra turmeric rice praying pray spices brown sustainable spus'], ['Bake baking Roast bake roast baked crust cook oven 19.'], ['baked Bake Gourmet He baking bake oven Heckhiver Cake'], ['black Black BLACKBlack glass black Glass dispositionschwarz books'], ['SH books book Book Sha advantageSh BooksBOOK Shi'], ['Plant gardening Garden122 soil Educational Gardens Alumni planted Grow'], ['Ceramic ceramic Flower Plant crusher porcelain plants plant Dentistry flower'], ['Water waterWater aquatic duplicate Red red sodawater plumbing'], ['Plant plants stood plant Flint farmers coffee Nebraska Farm Farmers'], ['Georgetown boxBox boxes Outdoorbox Box7% outdoorecluded'], ['plant Plant plants Hang hangingkap hangplanthang Hungary'], ['beds sofa Bed mattressslept Sofa bedVG Spain Gr'], ['TV ESPN television Televisionécrite UnitTVVideo Media televiziune'], ['desk tableskri Desk mix Kra gamers Kri identitate 27'], ['100 90 tables adjustTable Desk computers table adjusted adjustment'], ['Coffee tables coffee table LI Table Rica Georg176 caffeine'], ['Bin binbin garbage Bon sanitation junk rubbish cleanliness wastewater'], ['Office office organizationaldruck Draw offices gal GallCW workspace'], ['LED socket flashlight Smart glow sensor Porschetritt gatewayangebote'], ['SUPER mattress Bed Mattress bedding super mattresses SuperSuper stump'], ['Sink basin102103 sink piscine swimmer 101 rats 350'], ['changing Pediatric Nursery Table pediatric childcare Parents table nursery Children'], ['table Table runnersRunnerTabletable run tables Runningrunner'], ['BOOK display displayed units book Unit unit Bookunitschlagen'], ['Brown oak Danish Denmark 50 Peanut brown50 peintureTV'], ['BLACK BlackBlack Glass HIGH SE6030 SSD Training'], ['Wall wall piata Living 1962 NW Jocuri(6) 64wall'], ['30 Frame 30, revolution Publi 60 Lease Newsletter Practice Subaru'], ['wall Mount mount Wall Fish fish Cet179 aquarium walls'], ['TVTV television H Netflix 1901HF Sony Television HBO'], ['Top 42 topTop 41 TOPtop Panelpanel 36'], ['concrete Concrete quarry concret granite TV Netflix cryptocurrency HBO privé'], ['rail Rail shampoorail Hair salon Shampoo Allen train stations'], ['ME55 Welche 55ppe clash Conflict Med Me Economics'], ['Concrete concretedrywall bath wallshonda bathrooms béton Bath232'], ['9509545%750 mirror prze92 preparat Mirror Portsmouth'], ['200 sliding200300 300400301303 wax glide'], ['sliding mirror slide Panel panels me Mirror Slide slides oak'], ['wardrobeFIT closet gown wool Grey pa grey frontier Namibia'], ['12% 12 obedienceArt8% Plant7% robot costumes6%'], ['floristnatural plants Organic organic outdoors botanical landscaping outdoorpflanzen'], ['tiles fri Bathroom Lighting mirror Fri grout Mirror shine111'], ['mattress Bed Mattress mattresses Sleep bedslept headboard sleep beds'], ['Bed bedslept crib mattress sleeping loft Mattress sleep headboard'], ['basket Basket Deutsche din 28 wire boxes Nissandeutsche cables'], ['book Doors BookBOOK doors books booked textbookbookbooks'], ['glass cabinet Display Mill Michigan display Glassschalten Maryland Müller'], ['Hand39 Nigeriamy BLACK hand Wheel handle NJте'], ['Bedroom ADHD Airbnb Accommodation bedrooms contend Herbstbedroom Indian Mortgage'], ['Artificial vasevases Flip artificial Flower Kunst petals floristART'], ['mirror Mirrormirrored Bathroomdisplaying display pantry bathroom cupboardERN'], ['30 tall paragraph30 healthcare Bath Tall 29 Thir chapters'], ['base 50 forty Unit fifty frame 49 bases framesbase'], ['para Para protejeaz Carroll sunscreenbase Bra Sun solar SRL'], ['Kal Box boxes Under personnelle Syracuse strongkal box under'], ['white KalwhiteWhite White hardwoodkal headboard dabei Dakota'], ['56 51 47 46 Wichtig shelf BLACK 49 75 Ralph'], ['J Frame frame josframe framesJCJ JoshJS'], ['outdoors Garden garden Outdoor Rat backyard Environment seat juridic Up'], ['patio Patio setSET folding 1929 Fold 1959 fünffold'], ['rug Rug runnersrugsRunner Carpet carpet Teppich Run horses'], ['insomnia mattress Bed mattresses bed Mattress sleep crib preschool childcare'], ['37 3837 1936 1937 39 slides 1938 47 33'], ['augmentationMass Bottle Glass entitlement Mass pan Springfield Pan MA'], ['stainless shelving Edelstahl steel shelvesStainless Kun Steel K Boeing'], ['oak desk Desk table178 Table 160 Oak Monitor Support'], ['top table Table Black tables BLACKTop Top BlackBerryblack'], ['cover coverscover Holly greenhouse Cover territorial Green Plastic hydrogen'], ['Pediatric pediatric Pink preschool Parents Box Child Kids pink child'], ['Curtain curtain curtains curl drape cur Tur BlindBlue cord'], ['lamps Lamp lamp Al Shade shade Pendant shades pendant Lighting'], ['lamps Lamp Lighting lamp lighting Shade black Black shade Ceiling'], ['umbrella rain douche inflatable Garden Rain acknowledg discharge bladder shower'], ['shoes shoe footwear Shoes sneakers organisations shows 14 personalities show'], ['Ke kettle boiling boil boiler teaboiledMetallbrewedkett'], ['Plastic Chef duplicate plasticplastic Drawplate Smartphone Customers Restaurant'], ['saucepan cook skillet Pan Cook cooking pan Pot eligibility pana'], ['Ceiling ceiling Lighting lighting illumination LED Bathroom electrician electricity bathroom'], ['drawing preschool Front turquoise front Sofia Pediatric strawberry DinSM'], ['3030 DESIGN Paint-30Green paint thirty colors color'], ['childchildren Child stool child Children Pediatric childhood playground Kids'], ['walnut strip strips Wall Wal Wood Strip wall Decor VA'], ['29 azi29 ale Azapte zic Dean Vi 31'], ['glass Glass lens lenses Black frames Lens cornea Kitchenglass'], ['30% 80 proposer30 ESPN 30 1980 AviationAW fața'], ['Ly doorLY detectionWOOD Door KYHY connector aperture'], ['Glo Ring 200 GT HIGH Rock Höchst denseHigh High'], ['green GreenGreengreen Bob glow Blog eco Sustainability40'], ['Illinois VA duplicate PV calitate Veterans Dodge IRS drawer Iowa'], ['50 100 Brown 40 cabinet Cab fifty Mexico commun Cabinet'], ['Concrete concrete cement gravel side Quarry sides Riverside Dark limestone'], ['Grey gloss Grammar timeless primer Glo grey units Gray GRA'], ['box Boxbox Storage LaBox storage cupboard Netflixspa']]\n"
                              ]
                        }
                  ],
                  "source": [
                        "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
                        "l = [\"furniture\", \"couch\", \"sofa\", \"chair\", \"shelf\", \"kitchen\", \"table\", \"bed\"]\n",
                        "inputs = tokenizer.batch_encode_plus(\n",
                        "    l, return_tensors=\"pt\", padding=True\n",
                        ").to('cuda:0')\n",
                        "inds = torch.tensor([inputs['input_ids'][i].max() for i in range(len(l))])\n",
                        "cos = torch.nn.functional.cosine_similarity\n",
                        "emb = list(model.named_parameters())[0][1]\n",
                        "lm = list(model.named_parameters())[-1][1]\n",
                        "emb_old_tokens = emb[:-100]\n",
                        "lm_old_tokens = lm[:-100]\n",
                        "\n",
                        "#tt = emb[inds]\n",
                        "#tt = lm[inds]\n",
                        "\n",
                        "emb_new_tokens = emb[-100:]\n",
                        "lm_new_tokens = lm[-100:]\n",
                        "\n",
                        "emb_top10 = []\n",
                        "lm_top10 = []\n",
                        "for i in range(len(emb_new_tokens)):\n",
                        "    new_emb_token = emb_new_tokens[i]\n",
                        "    new_lm_token = lm_new_tokens[i]\n",
                        "\n",
                        "    emb_top10_similar = torch.tensor(cos(new_emb_token, emb_old_tokens, dim=-1)).topk(10, largest=True)\n",
                        "    lm_top10_similar = torch.tensor(cos(new_lm_token, lm_old_tokens, dim=-1)).topk(10, largest=True)\n",
                        "\n",
                        "    emb_top10 += [[tokenizer.decode(emb_top10_similar[1], skip_special_tokens=False)]]\n",
                        "    lm_top10 += [[tokenizer.decode(lm_top10_similar[1], skip_special_tokens=False)]]\n",
                        "print(emb_top10)\n",
                        "print(lm_top10)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 9,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "queries = pd.read_parquet(\"data/item_namequeries100.parquet.gzip\")\n",
                        "df = pd.DataFrame({'item_no':items,'top_10_emb':emb_top10, 'top_10_lm':lm_top10})\n",
                        "df = df.merge(queries, how='inner', on='item_no')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 10,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ddf = df[['item_no', 'name', 'top_10_emb', 'top_10_lm','top10 queries']]"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 6,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ddf_ptl = ddf.copy()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 11,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "d = pd.DataFrame()\n",
                        "c = 0\n",
                        "c1 = 0\n",
                        "c2 = 0\n",
                        "for i in range(300):\n",
                        "    if c == 0:\n",
                        "        d = d.append(ddf.iloc[c1], ignore_index=True)\n",
                        "        c1 += 1\n",
                        "    if c == 1:\n",
                        "        d = d.append(ddf_ptl.iloc[c2], ignore_index=True)\n",
                        "        c2 += 1\n",
                        "    if c == 2:\n",
                        "        d = d.append({'item_no':\" \", 'name':\" \", 'top_10_emb':\" \", 'top_10_lm':\" \",'top10 queries':\" \"}, ignore_index=True)\n",
                        "        c = -1\n",
                        "    c+=1"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 12,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "d.to_excel('old_vs_ptl_2000_epoch.xlsx')"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "top3 = []\n",
                        "top32n = 100*[[]]\n",
                        "for i, j in enumerate(top102):\n",
                        "    top3 = [j[0][0:8], j[0][8:16], j[0][16:]]\n",
                        "    top32n[i] = [queries[queries['item_no']==top3[0]]['name'].iloc[0], queries[queries['item_no']==top3[1]]['name'].iloc[0], queries[queries['item_no']==top3[2]]['name'].iloc[0]] "
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": 17,
                  "metadata": {},
                  "outputs": [
                        {
                              "data": {
                                    "text/plain": [
                                          "[200, 9, 1]"
                                    ]
                              },
                              "execution_count": 17,
                              "metadata": {},
                              "output_type": "execute_result"
                        }
                  ],
                  "source": [
                        "tokenizer.encode('besta')"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3.7.12 ('tz')",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.7.12"
            },
            "orig_nbformat": 4,
            "pycharm": {
                  "stem_cell": {
                        "cell_type": "raw",
                        "metadata": {
                              "collapsed": false
                        },
                        "source": []
                  }
            },
            "vscode": {
                  "interpreter": {
                        "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
                  }
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
