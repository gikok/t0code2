{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/z/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from promptsource.templates import DatasetTemplates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data_test-7d72f7f8f62e79b5\n",
      "Reusing dataset csv (/home/gikok/.cache/huggingface/datasets/csv/data_test-7d72f7f8f62e79b5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n",
      "Using custom data configuration data_test-7d72f7f8f62e79b5\n",
      "Reusing dataset csv (/home/gikok/.cache/huggingface/datasets/csv/data_test-7d72f7f8f62e79b5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.csv\"}\n",
    "raw_train_dataset = load_dataset('data_test', data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/gikok/.cache/huggingface/datasets/csv/data_test-7d72f7f8f62e79b5/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-2a01debe3f3c17d9.arrow\n"
     ]
    }
   ],
   "source": [
    "#Prepare training data\n",
    "column_names = raw_train_dataset.column_names\n",
    "padding='max_length'\n",
    "max_length=1024\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/transformers2/\", use_fast=not True)\n",
    "def preprocess_train(examples):\n",
    "    bs = len(examples)\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for i in range(bs):\n",
    "        ex = examples[column_names[1]][i].strip('][').split(',')[:10]\n",
    "        ex = [x.rsplit(' ', 1)[0] for x in ex]\n",
    "        input = f'With the following query: {examples[column_names[0]][i]}. Which is the best answer?'\n",
    "        target = ex[0]\n",
    "        input_texts.append(input)\n",
    "        target_texts.append(target)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs['labels'] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "train_dataset = raw_train_dataset.map(preprocess_train, batched=True, remove_columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load model\n",
    "accelerator = Accelerator()\n",
    "config = AutoConfig.from_pretrained('/home/transformers2/')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    '/home/transformers2/',\n",
    "    from_tf=bool(\".ckpt\" in '/home/transformers2/'),\n",
    "    config=config,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collator = default_data_collator\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=eval_collator, batch_size=1)\n",
    "eval_dataloader = accelerator.prepare(eval_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/z/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=10*math.ceil(len(train_dataloader)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallelize model and inputs\n",
    "model.parallelize()\n",
    "optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "            optimizer, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize first epoch\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do one training step\n",
    "train_features = next(iter(train_dataloader))\n",
    "outputs = model(**train_features)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "loss = loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate metrics\n",
    "metric = load_metric(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 2048)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 2048)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 32)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedGeluDense(\n",
       "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (gelu_act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initiate eval\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11866/3226762067.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmasked_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels_attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mseq_token_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1606\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m             )\n\u001b[1;32m   1610\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# required mask seq length can be calculated via length of past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#Do one eval timestep\n",
    "batch = next(iter(eval_dataloader))\n",
    "model_inputs = {\n",
    "    k: batch[k]\n",
    "    for k in [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "}\n",
    "with torch.no_grad():\n",
    "    logits = model(**model_inputs).logits\n",
    "masked_log_probs = batch[\"labels_attention_mask\"].unsqueeze(-1) * torch.log_softmax(logits, dim=-1)\n",
    "seq_token_log_probs = torch.gather(masked_log_probs, -1, batch[\"labels\"].unsqueeze(-1))\n",
    "seq_log_prob = seq_token_log_probs.squeeze(dim=-1).sum(dim=-1)\n",
    "seq_log_prob = seq_log_prob.view(batch[\"targets\"].size(0), -1) \n",
    "predictions = seq_log_prob.argmax(dim=-1)\n",
    "\n",
    "metric.add_batch(\n",
    "    predictions=accelerator.gather(predictions),\n",
    "    references=accelerator.gather(batch[\"targets\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "146e1d1b458fe2084fa64e5e1f61d8dc80cf176a66b2db74b494635d4cf7f021"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('z')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}