{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from promptsource.templates import DatasetTemplates\n",
    "pd.set_option(\"display.max_rows\", 1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-73d83a3e944add1f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/data to /home/gikok/.cache/huggingface/datasets/parquet/data-73d83a3e944add1f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 882.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/gikok/.cache/huggingface/datasets/parquet/data-73d83a3e944add1f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.69it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = load_dataset('data', data_files='prompts_001.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Moved model to GPUs\n"
     ]
    }
   ],
   "source": [
    "#Load model and tokenizer\n",
    "\n",
    "model_name = \"/home/transformers2\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Model and tokenizer loaded\")\n",
    "\n",
    "model.parallelize()\n",
    "print(\"Moved model to GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet('data/item_no_6k.parquet.gzip')['item_no'].values.tolist()\n",
    "tokenizer.add_tokens(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-73d83a3e944add1f\n",
      "Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-73d83a3e944add1f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "100%|██████████| 1/1 [00:00<00:00, 84.20it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('data', data_files='prompts_001.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6180864"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "max_length = 1024\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 932/6181 [21:27<2:01:53,  1.39s/ba]"
     ]
    }
   ],
   "source": [
    "train_dataset = data.map(\n",
    "    tokenize_train, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "146e1d1b458fe2084fa64e5e1f61d8dc80cf176a66b2db74b494635d4cf7f021"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('z')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
