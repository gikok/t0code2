{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from lib2to3.pgen2 import token\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from train import DataCollatorForMultipleChoice\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"dataset_name\"] = \"seven_prompts003.parquet.gzip\"\n",
    "args[\"eval_name\"] = \"miniprompts002_eval.parquet.gzip\"\n",
    "args[\"model_name_or_path\"] = \"bigscience/T0_3B\"\n",
    "args[\"output_dir\"] = \"/home/gikok/output\"\n",
    "args[\"num_train_epochs\"] = 1\n",
    "args[\"per_device_train_batch_size\"] = 16\n",
    "args[\"per_device_eval_batch_size\"] = 16\n",
    "args[\"freeze_encoder\"] = True\n",
    "args[\"learning_rate\"] = 1e30\n",
    "args[\"parallelize\"] = False\n",
    "args[\"seed\"] = 42\n",
    "args[\"pad_to_max_length\"] = False\n",
    "args[\"input_eos\"] = False\n",
    "args[\"target_max_length\"] = 256\n",
    "args[\"max_length\"] = 512\n",
    "args[\"num_warmup_steps\"] = 0\n",
    "args[\"debug\"] = False\n",
    "args[\"lr_scheduler_type\"] = \"linear\"\n",
    "args[\"num_shots\"] = None\n",
    "args[\"weight_decay\"] = 0.0\n",
    "args[\"gradient_checkpoint\"] = False\n",
    "args[\"gradient_accumulation_steps\"] = 1\n",
    "args[\"max_train_steps\"] = None\n",
    "\n",
    "\n",
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(2048):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new, i], 10000)\n",
    "        pdf = val / sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:] + bin[:-1]) / 2\n",
    "        new_tensor[-n_new:, i] = torch.tensor(\n",
    "            np.interp(np.random.random(n_new), cdf, b)\n",
    "        )\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:, :] = new_tensor\n",
    "# define the LightningModule\n",
    "# class plModelClass(pl.LightningModule):\n",
    "#     def __init__(self, model, args):\n",
    "#         super().__init__()\n",
    "#         self.model = model\n",
    "#         self.args = args\n",
    "#         self.learning_rate = args['learning_rate']\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         # training_step defines the train loop.\n",
    "#         # it is independent of forward\n",
    "#         outputs = self.model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         # Logging to TensorBoard by default\n",
    "#         self.log(\"train_loss\", loss)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "\n",
    "#         optimizer = AdamW(\n",
    "#             [\n",
    "#                 list(self.model.named_parameters())[0][1],\n",
    "#                 list(self.model.named_parameters())[-1][1],\n",
    "#             ],\n",
    "#             lr=self.learning_rate,\n",
    "#             weight_decay=0,\n",
    "#         )\n",
    "\n",
    "#         return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:01<00:00, 13.94ba/s]\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:348: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "10/27/2022 14:51:00 - INFO - __main__ - ***** Running training *****\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Num training = 25600\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Num Epochs = 1\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "10/27/2022 14:51:00 - INFO - __main__ -   Total optimization steps = 1600\n"
     ]
    }
   ],
   "source": [
    "set_seed(args[\"seed\"])\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "\n",
    "datasets.utils.logging.set_verbosity_error()\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Handle the output directory creation\n",
    "os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args[\"dataset_name\"] is not None:\n",
    "    data_files = {\"train\": args[\"dataset_name\"]}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "else:\n",
    "    raise ValueError(\"Please specify `args['dataset_name`.\")\n",
    "\n",
    "# Trim a number of evaluation training\n",
    "if args[\"debug\"]:\n",
    "    raw_train_dataset = raw_train_dataset.select(\n",
    "        range(min(100, len(raw_train_dataset)))\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args[\"model_name_or_path\"]).to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name_or_path\"])\n",
    "\n",
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "# tokenizer.add_tokens(items)\n",
    "\n",
    "# # then resize embeddings\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # resample shared embedding and lm_head layer\n",
    "# resample(model, 0, len(items))\n",
    "# resample(model, -1, len(items))\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args[\"pad_to_max_length\"] else False\n",
    "\n",
    "\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args[\"input_eos\"],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = raw_train_dataset.map(tokenize_train, batched=True)\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Log a few random training:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.debug(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "# for index in random.sample(range(len(eval_dataset)), 3):\n",
    "#     logger.debug(f\"Sample {index} of the evaluation set: {eval_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args[\"per_device_train_batch_size\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args[\"weight_decay\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args[\"learning_rate\"])\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "if args[\"max_train_steps\"] is None:\n",
    "    args[\"max_train_steps\"] = args[\"num_train_epochs\"] * num_update_steps_per_epoch\n",
    "else:\n",
    "    args[\"num_train_epochs\"] = math.ceil(\n",
    "        args[\"max_train_steps\"] / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args[\"lr_scheduler_type\"],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args[\"num_warmup_steps\"],\n",
    "    num_training_steps=args[\"max_train_steps\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "total_batch_size = (\n",
    "    args[\"per_device_train_batch_size\"]\n",
    "    * args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num training = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args['per_device_train_batch_size']}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {args['max_train_steps']}\")\n",
    "\n",
    "global_steps = 0\n",
    "\n",
    "# how often trained model should be saved\n",
    "r = int(args[\"max_train_steps\"] / 30)\n",
    "if args[\"gradient_checkpoint\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plModelClass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18330/3777226869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model using pytorch-lightning API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplModelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m checkpoint_callback = ModelCheckpoint(\n\u001b[1;32m      4\u001b[0m     \u001b[0mevery_n_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plModelClass' is not defined"
     ]
    }
   ],
   "source": [
    "# train model using pytorch-lightning API\n",
    "plmodel = plModelClass(model, args)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    every_n_epochs=100,\n",
    "    dirpath=args.output_dir,\n",
    "    filename=\"model_epoch_{epoch:02d}\",\n",
    "    save_last=True,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    logger=True,\n",
    "    accelerator=\"gpu\",\n",
    "    min_epochs=args.num_train_epochs,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    devices=-1,\n",
    "    auto_select_gpus=True,\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    strategy=\"deepspeed_stage_2\",\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(\n",
    "    model=plmodel,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    # ckpt_path=args.output_dir + \"/model_epoch_epoch=199.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='/home/gikok/pltest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', 'target', 'options']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/gikok/pltest/last.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21469/3591284823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/gikok/pltest/last.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                 \u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 \u001b[0mcache_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             )\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/gikok/pltest/last.ckpt'"
     ]
    }
   ],
   "source": [
    "plmodel.load_from_checkpoint(checkpoint_path=\"/home/gikok/pltest/last.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\" if args[\"pad_to_max_length\"] else False\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args[\"input_eos\"],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name_or_path\"])\n",
    "items = pd.read_parquet(\"data/seven_names.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "tokenizer.add_tokens(items)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args[\"model_name_or_path\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix initial values of new matrices\n",
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(len(new_tensor[-1, :])):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new, i], 10000)\n",
    "        pdf = val / sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:] + bin[:-1]) / 2\n",
    "        new_tensor[-n_new:, i] = torch.tensor(\n",
    "            np.interp(np.random.random(n_new), cdf, b)\n",
    "        )\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:, :] = new_tensor\n",
    "\n",
    "resample(model, 0, len(items))\n",
    "resample(model, -1, len(items))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-c4bfcee78d1cb2be\n",
      "Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-c4bfcee78d1cb2be/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/gikok/.cache/huggingface/datasets/parquet/data-c4bfcee78d1cb2be/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b00f0678a21da1bf.arrow\n"
     ]
    }
   ],
   "source": [
    "if args[\"dataset_name\"] is not None:\n",
    "    data_files = {\"train\":args[\"dataset_name\"]}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "\n",
    "train_dataset = raw_train_dataset.map(tokenize_train, batched=True)\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=train_collator,\n",
    "    num_workers=1,\n",
    "    batch_size=args['per_device_train_batch_size'],\n",
    ")\n",
    "accelerator = Accelerator()\n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00484375"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = np.array(b)\n",
    "inds = inds>150\n",
    "sum(inds)/len(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"] = 16\n",
    "fig,ax=plt.subplots(figsize=(16,9))\n",
    "plt.hist(b, bins=30)\n",
    "plt.xlabel(\"Input data n_tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"New data tensor size distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"font.size\"] = 16\n",
    "fig,ax=plt.subplots(figsize=(16,9))\n",
    "plt.hist(bold, bins=30)\n",
    "plt.xlabel(\"Input data n_tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Old data tensor size distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # Logging to TensorBoard by default\n",
    "        print(loss)\n",
    "        #self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plmodel = LitAutoEncoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto select gpus: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=1, auto_select_gpus=True, auto_lr_find=True, accumulate_grad_batches=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:348: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 2.8 B \n",
      "-----------------------------------------------------\n",
      "131 M     Trainable params\n",
      "2.7 B     Non-trainable params\n",
      "2.8 B     Total params\n",
      "11,399.029Total estimated model params size (MB)\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:241: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 0:   0%|          | 0/1600 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 39.59 GiB total capacity; 37.34 GiB already allocated; 10.19 MiB free; 37.76 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18330/3975660516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#lr_finder = trainer.tuner.lr_find(model=plmodel, train_dataloaders=train_dataloader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         )\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0;31m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         )\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m         )\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_idx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             )\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[0;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_block_parallel_sync_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \"\"\"\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18330/607309262.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# training_step defines the train loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# it is independent of forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Logging to TensorBoard by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m         )\n\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 )\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             )\n\u001b[1;32m    703\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         )\n\u001b[1;32m    617\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# get key/value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         key_states = project(\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_value_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         )\n\u001b[1;32m    504\u001b[0m         value_states = project(\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0;31m# cross-attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tz/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 39.59 GiB total capacity; 37.34 GiB already allocated; 10.19 MiB free; 37.76 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=plmodel, train_dataloaders=train_dataloader)\n",
    "#lr_finder = trainer.tuner.lr_find(model=plmodel, train_dataloaders=train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nvidia-smi` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        t = torch.zeros(param.shape).to(\"cuda:0\")\n",
    "        t[-len(items) :, :] = 1\n",
    "        param.register_hook(lambda grad: grad * t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(model.children())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'miniprompts002.parquet.gzip',\n",
       " 'eval_name': 'miniprompts002_eval.parquet.gzip',\n",
       " 'model_name_or_path': 'bigscience/T0_3B',\n",
       " 'output_dir': '/home/gikok/output',\n",
       " 'num_train_epochs': 1,\n",
       " 'per_device_train_batch_size': 16,\n",
       " 'per_device_eval_batch_size': 16,\n",
       " 'freeze_encoder': True,\n",
       " 'learning_rate': 1e+30,\n",
       " 'parallelize': False,\n",
       " 'seed': 42,\n",
       " 'pad_to_max_length': False,\n",
       " 'input_eos': False,\n",
       " 'target_max_length': 256,\n",
       " 'max_length': 512,\n",
       " 'num_warmup_steps': 0,\n",
       " 'debug': False,\n",
       " 'lr_scheduler_type': 'linear',\n",
       " 'num_shots': None,\n",
       " 'weight_decay': 0.0,\n",
       " 'gradient_checkpoint': False,\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'max_train_steps': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items) :, :] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l += 1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l, mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = list(model.named_parameters())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(emb.grad[-101:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
