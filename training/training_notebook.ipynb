{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright BigScience, The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning T0 in PyTorch, optionally few-shot.\n",
    "This script is adapted from\n",
    "https://github.com/huggingface/transformers/blob/master/examples/pytorch/multiple-choice/run_swag_no_trainer.py\n",
    "as well as\n",
    "https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run_summarization_no_trainer.py\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "accelerator = Accelerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dataset_name'] = 'super_glue'\n",
    "args['dataset_config_name'] = 'rte'\n",
    "args['template_name'] = 'does this imply'\n",
    "args['config_name'] = None\n",
    "args['model_name_or_path'] = 'bigscience/T0_3B'\n",
    "args['output_dir'] = '/home/gikok/output'\n",
    "args['num_train_epochs'] = 1\n",
    "args['per_device_train_batch_size'] = 4\n",
    "args['per_device_eval_batch_size'] = 4\n",
    "args['freeze_encoder'] = True\n",
    "args['learning_rate'] = 10\n",
    "args['parallelize'] = True\n",
    "args['seed'] = 42\n",
    "args['pad_to_max_length'] = True\n",
    "args['input_eos'] = False\n",
    "args['target_max_length'] = 64\n",
    "args['max_length'] = 128\n",
    "args['num_warmup_steps'] = 0\n",
    "args['debug'] = False\n",
    "args['lr_scheduler_type'] = 'linear'\n",
    "args['num_shots'] = None\n",
    "args['weight_decay'] = 0\n",
    "args['gradient_checkpoint'] = False\n",
    "args['gradient_accumulation_steps'] = 1\n",
    "args['max_train_steps'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/30/2022 10:07:54 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "08/30/2022 10:07:54 - WARNING - datasets.builder - Reusing dataset super_glue (/home/gikok/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "08/30/2022 10:07:55 - WARNING - datasets.builder - Reusing dataset super_glue (/home/gikok/.cache/huggingface/datasets/super_glue/rte/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /home/gikok/.cache/huggingface/transformers/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /home/gikok/.cache/huggingface/transformers/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /home/gikok/.cache/huggingface/transformers/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /home/gikok/.cache/huggingface/transformers/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.12ba/s]\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.63s/ba]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "\n",
    "raw_train_dataset = load_dataset(args['dataset_name'], args['dataset_config_name'], split=\"train\")\n",
    "raw_eval_dataset = load_dataset(args['dataset_name'], args['dataset_config_name'], split=\"validation\")\n",
    "\n",
    "column_names = raw_eval_dataset.column_names\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "if args['config_name']:\n",
    "    config = AutoConfig.from_pretrained(args['config_name'])\n",
    "elif args['model_name_or_path']:\n",
    "    config = AutoConfig.from_pretrained(args['model_name_or_path'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'])\n",
    "\n",
    "if args['model_name_or_path']:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        args['model_name_or_path'],\n",
    "        from_tf=bool(\".ckpt\" in args['model_name_or_path']),\n",
    "        config=config,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args['pad_to_max_length'] else False\n",
    "\n",
    "# Get the prompt to apply and the possible targets.\n",
    "# TODO(Victor): If pulling from pre-processed data, remove this logic.\n",
    "prompts = DatasetTemplates(\n",
    "    f\"{args['dataset_name']}\"\n",
    "    if args['dataset_config_name'] is None\n",
    "    else f\"{args['dataset_name']}/{args['dataset_config_name']}\"\n",
    "    )\n",
    "template = prompts[args['template_name']]\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    bs = len(examples[column_names[0]])\n",
    "\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for i in range(bs):\n",
    "        ex = {\n",
    "            k: examples[k][i]\n",
    "            for k in column_names\n",
    "        }\n",
    "        input, target = template.apply(ex)\n",
    "        ex_answer_choices = template.get_answer_choices_list(ex)\n",
    "        assert target in ex_answer_choices\n",
    "        input_texts.append(input)\n",
    "        target_texts.append(target)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args['input_eos'],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs['labels'] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    bs = len(examples[column_names[0]])\n",
    "\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    answer_choices_texts = []\n",
    "    for i in range(bs):\n",
    "        ex = {\n",
    "            k: examples[k][i]\n",
    "            for k in column_names\n",
    "        }\n",
    "        input, target = template.apply(ex)\n",
    "        ex_answer_choices = template.get_answer_choices_list(ex)\n",
    "        assert target in ex_answer_choices\n",
    "        input_texts.append(input)\n",
    "        target_texts.append(target)\n",
    "        answer_choices_texts.append(ex_answer_choices)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args['max_length'],\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = [\n",
    "        tokenizer(\n",
    "            ans_choi,\n",
    "            padding=True,\n",
    "            max_length=args['target_max_length'],\n",
    "            truncation=True,\n",
    "        )\n",
    "        for ans_choi in answer_choices_texts\n",
    "    ]\n",
    "\n",
    "    features = {\n",
    "        k: [\n",
    "            [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "            for idx, elem in enumerate(v)\n",
    "        ]\n",
    "        for k, v in tokenized_inputs.items()\n",
    "    }\n",
    "\n",
    "    features[\"labels\"] = [\n",
    "        tokenized_targets[idx][\"input_ids\"]\n",
    "        for idx in range(bs)\n",
    "    ]\n",
    "    features[\"labels_attention_mask\"] = [\n",
    "        tokenized_targets[idx][\"attention_mask\"]\n",
    "        for idx in range(bs)\n",
    "    ]\n",
    "    features[\"targets\"] = [\n",
    "        answer_choices_texts[idx].index(t)\n",
    "        for idx, t in enumerate(target_texts)\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = raw_eval_dataset.map(preprocess_eval, batched=True, remove_columns=column_names)\n",
    "    train_dataset = raw_train_dataset.map(preprocess_train, batched=True, remove_columns=column_names)\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args['per_device_train_batch_size']\n",
    ")\n",
    "\n",
    "if args['pad_to_max_length']:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    eval_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    eval_collator = DataCollatorForMultipleChoice(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=eval_collator, batch_size=args['per_device_eval_batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'labels_attention_mask', 'targets'],\n",
       "    num_rows: 277\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0476, -0.0623, -0.0977,  ...,  0.0262,  0.0427, -0.0295],\n",
       "        [-0.0674, -0.0129, -0.1523,  ..., -0.1436, -0.0286,  0.0645],\n",
       "        [-0.0310, -0.1582,  0.2676,  ...,  0.0461, -0.0747,  0.1699],\n",
       "        ...,\n",
       "        [ 0.0055,  0.0084, -0.0009,  ..., -0.0090,  0.0089,  0.0145],\n",
       "        [ 0.0079, -0.0142,  0.0160,  ..., -0.0086,  0.0213,  0.0113],\n",
       "        [ 0.0073,  0.0013,  0.0113,  ...,  0.0068,  0.0009,  0.0005]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    heat_map = torch.matmul(lm.cpu(), emb.T.cpu())\n",
    "    ax = sns.heatmap(heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embedding_module.register_full_backward_hook(big_hook)\n",
    "b = embedding_module.weight.register_hook(hook_fkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%which` not found.\n"
     ]
    }
   ],
   "source": [
    "a.remove()\n",
    "b.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/24144 [00:01<7:20:18,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP WITH ADAMW LOL\n",
      "Parameter containing:\n",
      "tensor([[  0.1436,   3.8750,   0.5352,  ...,  30.8750,   1.3281, -21.5000],\n",
      "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
      "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
      "        ...,\n",
      "        [ -0.3020,  -0.3723,   1.2172,  ...,   1.8101,   1.0143,  -0.7559],\n",
      "        [ -0.7375,   0.0380,   0.1171,  ...,   1.1145,  -0.9926,   1.0823],\n",
      "        [  0.4283,   0.9865,   0.6769,  ...,  -1.0338,  -0.2796,  -0.3376]],\n",
      "       device='cuda:0', requires_grad=True) PRINTING p\n",
      "tensor([[-1.3869e-04,  2.3134e-04, -1.8927e-05,  ...,  5.3890e-04,\n",
      "          4.5927e-04,  1.4674e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0') PRINTING grad\n",
      "-10.0 printing group LR\n",
      "0 PRINTING GROUP WEIGHT DECAY\n",
      "tensor([[  8.2868,  -4.9224,   4.2794,  ...,  21.4293,  -8.0277, -29.7270],\n",
      "        [ -4.7812,   7.3125,   3.3438,  ...,  10.3125,  -0.8711,  -1.3047],\n",
      "        [ -0.4902,   2.3906,  -5.1562,  ...,  -0.5430,   9.8750, -13.5625],\n",
      "        ...,\n",
      "        [ -0.3020,  -0.3723,   1.2172,  ...,   1.8101,   1.0143,  -0.7559],\n",
      "        [ -0.7375,   0.0380,   0.1171,  ...,   1.1145,  -0.9926,   1.0823],\n",
      "        [  0.4283,   0.9865,   0.6769,  ...,  -1.0338,  -0.2796,  -0.3376]],\n",
      "       device='cuda:0') pdata\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args['num_train_epochs'] + 1):\n",
    "    #model.train()\n",
    "\n",
    "    # freeze encoder updates if specified\n",
    "    if args['freeze_encoder']:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(\"encoder\") or name.startswith(\"decoder\") or name.startswith(\"lm_head\"):\n",
    "                param.requires_grad = False\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        global_steps += 1\n",
    "        loss = loss.item()\n",
    "        if step>=0:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.add_>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[1].data.add_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0780, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[-1][1][3,5]/p[0][1][3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\") or name.startswith(\"lm_head\"):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fhook(module: nn.Module, _inputs, _outputs):\n",
    "    print(\"FORWARD_HOOK\", module)\n",
    "    print(type(_inputs), len(_inputs), type(_outputs), len(_outputs))\n",
    "    print(\"FORWARD_HOOK INPUT SHAPE\",_inputs[0].shape)\n",
    "    print(\"FORWARD_HOOK OUTUT SHAPE\",_outputs[0].shape, _outputs[1].shape, _outputs[2].shape, _outputs[3].shape)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bhook(module: nn.Module, _inputs, _outputs):\n",
    "    print(\"BACKWARD\", module)\n",
    "    print(type(_inputs), len(_inputs), type(_outputs), len(_outputs))\n",
    "    if _inputs[0] is not None:\n",
    "        print(\"BINPUT SHAPE\",_inputs[0].shape)\n",
    "    if _outputs[0] is not None:\n",
    "        print(\"BOUTUT SHAPE\",_outputs[0].shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(38136, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=38136, bias=False)\n",
      ")\n",
      "******\n",
      "Embedding(38136, 2048)\n",
      "******\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Embedding(32, 32)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "NewGELUActivation()\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Embedding(32, 32)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "******\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "******\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "******\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "******\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "T5LayerNorm()\n",
      "******\n",
      "Dropout(p=0.1, inplace=False)\n",
      "******\n",
      "Linear(in_features=2048, out_features=38136, bias=False)\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for m in model.modules():\n",
    "    print(m)\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[T5ForConditionalGeneration(\n",
       "   (shared): Embedding(38136, 2048)\n",
       "   (encoder): T5Stack(\n",
       "     (embed_tokens): Embedding(38136, 2048)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 32)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (3): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (4): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (5): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (6): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (7): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (8): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (9): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (10): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (11): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (12): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (13): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (14): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (15): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (16): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (17): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (18): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (19): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (20): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (21): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (22): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (23): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (decoder): T5Stack(\n",
       "     (embed_tokens): Embedding(38136, 2048)\n",
       "     (block): ModuleList(\n",
       "       (0): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (relative_attention_bias): Embedding(32, 32)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (1): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (2): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (3): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (4): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (5): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (6): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (7): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (8): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (9): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (10): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (11): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (12): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (13): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (14): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (15): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (16): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (17): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (18): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (19): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (20): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (21): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (22): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (23): T5Block(\n",
       "         (layer): ModuleList(\n",
       "           (0): T5LayerSelfAttention(\n",
       "             (SelfAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (1): T5LayerCrossAttention(\n",
       "             (EncDecAttention): T5Attention(\n",
       "               (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "               (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (2): T5LayerFF(\n",
       "             (DenseReluDense): T5DenseGatedGeluDense(\n",
       "               (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "               (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "               (gelu_act): NewGELUActivation()\n",
       "             )\n",
       "             (layer_norm): T5LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=2048, out_features=38136, bias=False)\n",
       " ),\n",
       " Embedding(38136, 2048),\n",
       " T5Stack(\n",
       "   (embed_tokens): Embedding(38136, 2048)\n",
       "   (block): ModuleList(\n",
       "     (0): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (relative_attention_bias): Embedding(32, 32)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (3): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (4): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (5): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (6): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (7): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (8): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (9): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (10): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (11): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (12): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (13): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (14): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (15): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (16): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (17): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (18): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (19): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (20): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (21): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (22): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (23): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (relative_attention_bias): Embedding(32, 32)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (2): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (3): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (4): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (5): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (6): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (7): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (8): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (9): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (10): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (11): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (12): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (13): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (14): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (15): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (16): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (17): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (18): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (19): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (20): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (21): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (22): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (23): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (relative_attention_bias): Embedding(32, 32)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (relative_attention_bias): Embedding(32, 32)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (relative_attention_bias): Embedding(32, 32)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (relative_attention_bias): Embedding(32, 32)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Embedding(32, 32),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " NewGELUActivation(),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Stack(\n",
       "   (embed_tokens): Embedding(38136, 2048)\n",
       "   (block): ModuleList(\n",
       "     (0): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (relative_attention_bias): Embedding(32, 32)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (3): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (4): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (5): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (6): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (7): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (8): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (9): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (10): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (11): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (12): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (13): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (14): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (15): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (16): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (17): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (18): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (19): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (20): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (21): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (22): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (23): T5Block(\n",
       "       (layer): ModuleList(\n",
       "         (0): T5LayerSelfAttention(\n",
       "           (SelfAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (1): T5LayerCrossAttention(\n",
       "           (EncDecAttention): T5Attention(\n",
       "             (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "             (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (2): T5LayerFF(\n",
       "           (DenseReluDense): T5DenseGatedGeluDense(\n",
       "             (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "             (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "             (gelu_act): NewGELUActivation()\n",
       "           )\n",
       "           (layer_norm): T5LayerNorm()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (final_layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (relative_attention_bias): Embedding(32, 32)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (1): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (2): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (3): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (4): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (5): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (6): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (7): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (8): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (9): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (10): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (11): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (12): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (13): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (14): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (15): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (16): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (17): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (18): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (19): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (20): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (21): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (22): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (23): T5Block(\n",
       "     (layer): ModuleList(\n",
       "       (0): T5LayerSelfAttention(\n",
       "         (SelfAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): T5LayerCrossAttention(\n",
       "         (EncDecAttention): T5Attention(\n",
       "           (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "           (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (2): T5LayerFF(\n",
       "         (DenseReluDense): T5DenseGatedGeluDense(\n",
       "           (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "           (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "           (gelu_act): NewGELUActivation()\n",
       "         )\n",
       "         (layer_norm): T5LayerNorm()\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (relative_attention_bias): Embedding(32, 32)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (relative_attention_bias): Embedding(32, 32)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (relative_attention_bias): Embedding(32, 32)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (relative_attention_bias): Embedding(32, 32)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Embedding(32, 32),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerFF(\n",
       "   (DenseReluDense): T5DenseGatedGeluDense(\n",
       "     (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "     (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (gelu_act): NewGELUActivation()\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5DenseGatedGeluDense(\n",
       "   (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "   (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (gelu_act): NewGELUActivation()\n",
       " ),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=2048, out_features=5120, bias=False),\n",
       " Linear(in_features=5120, out_features=2048, bias=False),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5Block(\n",
       "   (layer): ModuleList(\n",
       "     (0): T5LayerSelfAttention(\n",
       "       (SelfAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (1): T5LayerCrossAttention(\n",
       "       (EncDecAttention): T5Attention(\n",
       "         (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "         (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (2): T5LayerFF(\n",
       "       (DenseReluDense): T5DenseGatedGeluDense(\n",
       "         (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "         (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "         (dropout): Dropout(p=0.1, inplace=False)\n",
       "         (gelu_act): NewGELUActivation()\n",
       "       )\n",
       "       (layer_norm): T5LayerNorm()\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " ModuleList(\n",
       "   (0): T5LayerSelfAttention(\n",
       "     (SelfAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (1): T5LayerCrossAttention(\n",
       "     (EncDecAttention): T5Attention(\n",
       "       (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "       (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (2): T5LayerFF(\n",
       "     (DenseReluDense): T5DenseGatedGeluDense(\n",
       "       (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
       "       (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "       (gelu_act): NewGELUActivation()\n",
       "     )\n",
       "     (layer_norm): T5LayerNorm()\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " T5LayerSelfAttention(\n",
       "   (SelfAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " T5Attention(\n",
       "   (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       " ),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " Linear(in_features=2048, out_features=2048, bias=False),\n",
       " T5LayerNorm(),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " T5LayerCrossAttention(\n",
       "   (EncDecAttention): T5Attention(\n",
       "     (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "     (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "   )\n",
       "   (layer_norm): T5LayerNorm()\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38136, 2048])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[-1].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n",
      "lol\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/3758117431.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/z/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;32m/tmp/ipykernel_20577/1684194404.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;31m#param.detach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith('shared') or name.startswith(\"lm_head\"):\n",
    "        print(\"lol\")\n",
    "        t = torch.tensor(np.zeros((param.shape[0], param.shape[1])))\n",
    "        t[-len(items):,:] = 1\n",
    "        param.register_hook(lambda grad: grad*t)\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "global_steps += 1\n",
    "loss = loss.item()\n",
    "if accelerator.is_main_process:\n",
    "    tqdm.write(f\"epoch = {1}, step = {global_steps}, loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20577/1934986589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'last' is not defined"
     ]
    }
   ],
   "source": [
    "last(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values[0][1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items):,:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Embedding(38136, 2048)\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "38136\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l +=1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l,mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(38136, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(38136, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedGeluDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (gelu_act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=38136, bias=False)\n",
      ")\n",
      "************** 1\n",
      "Embedding(38136, 2048)\n",
      "************** 2\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 3\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 4\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 5\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 6\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 7\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 8\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 9\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 10\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 11\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 12\n",
      "Embedding(32, 32)\n",
      "************** 13\n",
      "T5LayerNorm()\n",
      "************** 14\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 15\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 16\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 17\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 18\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 19\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 20\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 21\n",
      "NewGELUActivation()\n",
      "************** 22\n",
      "T5LayerNorm()\n",
      "************** 23\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 24\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 25\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 26\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 27\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 28\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 29\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 30\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 31\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 32\n",
      "T5LayerNorm()\n",
      "************** 33\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 34\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 35\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 36\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 37\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 38\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 39\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 40\n",
      "T5LayerNorm()\n",
      "************** 41\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 42\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 43\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 44\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 45\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 46\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 47\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 48\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 49\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 50\n",
      "T5LayerNorm()\n",
      "************** 51\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 52\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 53\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 54\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 55\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 56\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 57\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 58\n",
      "T5LayerNorm()\n",
      "************** 59\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 60\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 61\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 62\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 63\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 64\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 65\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 66\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 67\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 68\n",
      "T5LayerNorm()\n",
      "************** 69\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 70\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 71\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 72\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 73\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 74\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 75\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 76\n",
      "T5LayerNorm()\n",
      "************** 77\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 78\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 79\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 80\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 81\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 82\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 83\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 84\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 85\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 86\n",
      "T5LayerNorm()\n",
      "************** 87\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 88\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 89\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 90\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 91\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 92\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 93\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 94\n",
      "T5LayerNorm()\n",
      "************** 95\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 96\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 97\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 98\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 99\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 100\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 101\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 102\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 103\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 104\n",
      "T5LayerNorm()\n",
      "************** 105\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 106\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 107\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 108\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 109\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 110\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 111\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 112\n",
      "T5LayerNorm()\n",
      "************** 113\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 114\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 115\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 116\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 117\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 118\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 119\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 120\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 121\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 122\n",
      "T5LayerNorm()\n",
      "************** 123\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 124\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 125\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 126\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 127\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 128\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 129\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 130\n",
      "T5LayerNorm()\n",
      "************** 131\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 132\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 133\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 134\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 135\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 136\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 137\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 138\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 139\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 140\n",
      "T5LayerNorm()\n",
      "************** 141\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 142\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 143\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 144\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 145\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 146\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 147\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 148\n",
      "T5LayerNorm()\n",
      "************** 149\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 150\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 151\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 152\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 153\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 154\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 155\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 156\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 157\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 158\n",
      "T5LayerNorm()\n",
      "************** 159\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 160\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 161\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 162\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 163\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 164\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 165\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 166\n",
      "T5LayerNorm()\n",
      "************** 167\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 168\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 169\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 170\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 171\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 172\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 173\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 174\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 175\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 176\n",
      "T5LayerNorm()\n",
      "************** 177\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 178\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 179\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 180\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 181\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 182\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 183\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 184\n",
      "T5LayerNorm()\n",
      "************** 185\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 186\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 187\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 188\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 189\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 190\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 191\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 192\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 193\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 194\n",
      "T5LayerNorm()\n",
      "************** 195\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 196\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 197\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 198\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 199\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 200\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 201\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 202\n",
      "T5LayerNorm()\n",
      "************** 203\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 204\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 205\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 206\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 207\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 208\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 209\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 210\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 211\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 212\n",
      "T5LayerNorm()\n",
      "************** 213\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 214\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 215\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 216\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 217\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 218\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 219\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 220\n",
      "T5LayerNorm()\n",
      "************** 221\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 222\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 223\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 224\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 225\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 226\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 227\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 228\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 229\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 230\n",
      "T5LayerNorm()\n",
      "************** 231\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 232\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 233\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 234\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 235\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 236\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 237\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 238\n",
      "T5LayerNorm()\n",
      "************** 239\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 240\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 241\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 242\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 243\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 244\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 245\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 246\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 247\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 248\n",
      "T5LayerNorm()\n",
      "************** 249\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 250\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 251\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 252\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 253\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 254\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 255\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 256\n",
      "T5LayerNorm()\n",
      "************** 257\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 258\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 259\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 260\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 261\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 262\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 263\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 264\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 265\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 266\n",
      "T5LayerNorm()\n",
      "************** 267\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 268\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 269\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 270\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 271\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 272\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 273\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 274\n",
      "T5LayerNorm()\n",
      "************** 275\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 276\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 277\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 278\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 279\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 280\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 281\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 282\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 283\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 284\n",
      "T5LayerNorm()\n",
      "************** 285\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 286\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 287\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 288\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 289\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 290\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 291\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 292\n",
      "T5LayerNorm()\n",
      "************** 293\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 294\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 295\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 296\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 297\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 298\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 299\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 300\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 301\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 302\n",
      "T5LayerNorm()\n",
      "************** 303\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 304\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 305\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 306\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 307\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 308\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 309\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 310\n",
      "T5LayerNorm()\n",
      "************** 311\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 312\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 313\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 314\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 315\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 316\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 317\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 318\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 319\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 320\n",
      "T5LayerNorm()\n",
      "************** 321\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 322\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 323\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 324\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 325\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 326\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 327\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 328\n",
      "T5LayerNorm()\n",
      "************** 329\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 330\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 331\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 332\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 333\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 334\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 335\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 336\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 337\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 338\n",
      "T5LayerNorm()\n",
      "************** 339\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 340\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 341\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 342\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 343\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 344\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 345\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 346\n",
      "T5LayerNorm()\n",
      "************** 347\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 348\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 349\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 350\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 351\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 352\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 353\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 354\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 355\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 356\n",
      "T5LayerNorm()\n",
      "************** 357\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 358\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 359\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 360\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 361\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 362\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 363\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 364\n",
      "T5LayerNorm()\n",
      "************** 365\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 366\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 367\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 368\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 369\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 370\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 371\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 372\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 373\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 374\n",
      "T5LayerNorm()\n",
      "************** 375\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 376\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 377\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 378\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 379\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 380\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 381\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 382\n",
      "T5LayerNorm()\n",
      "************** 383\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 384\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 385\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 386\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 387\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 388\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 389\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 390\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 391\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 392\n",
      "T5LayerNorm()\n",
      "************** 393\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 394\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 395\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 396\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 397\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 398\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 399\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 400\n",
      "T5LayerNorm()\n",
      "************** 401\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 402\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 403\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 404\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 405\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 406\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 407\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 408\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 409\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 410\n",
      "T5LayerNorm()\n",
      "************** 411\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 412\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 413\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 414\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 415\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 416\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 417\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 418\n",
      "T5LayerNorm()\n",
      "************** 419\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 420\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 421\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 422\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 423\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 424\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 425\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 426\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 427\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 428\n",
      "T5LayerNorm()\n",
      "************** 429\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 430\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 431\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 432\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 433\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 434\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 435\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 436\n",
      "T5LayerNorm()\n",
      "************** 437\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 438\n",
      "T5LayerNorm()\n",
      "************** 439\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 440\n",
      "T5Stack(\n",
      "  (embed_tokens): Embedding(38136, 2048)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 32)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (21): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (22): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedGeluDense(\n",
      "            (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "            (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (gelu_act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 441\n",
      "ModuleList(\n",
      "  (0): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (17): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (18): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (19): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (20): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (21): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (22): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (23): T5Block(\n",
      "    (layer): ModuleList(\n",
      "      (0): T5LayerSelfAttention(\n",
      "        (SelfAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): T5LayerCrossAttention(\n",
      "        (EncDecAttention): T5Attention(\n",
      "          (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): T5LayerFF(\n",
      "        (DenseReluDense): T5DenseGatedGeluDense(\n",
      "          (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "          (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (gelu_act): NewGELUActivation()\n",
      "        )\n",
      "        (layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 442\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 32)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 443\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (relative_attention_bias): Embedding(32, 32)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 444\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (relative_attention_bias): Embedding(32, 32)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 445\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (relative_attention_bias): Embedding(32, 32)\n",
      ")\n",
      "************** 446\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 447\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 448\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 449\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 450\n",
      "Embedding(32, 32)\n",
      "************** 451\n",
      "T5LayerNorm()\n",
      "************** 452\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 453\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 454\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 455\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 456\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 457\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 458\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 459\n",
      "T5LayerNorm()\n",
      "************** 460\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 461\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 462\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 463\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 464\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 465\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 466\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 467\n",
      "T5LayerNorm()\n",
      "************** 468\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 469\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 470\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 471\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 472\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 473\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 474\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 475\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 476\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 477\n",
      "T5LayerNorm()\n",
      "************** 478\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 479\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 480\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 481\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 482\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 483\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 484\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 485\n",
      "T5LayerNorm()\n",
      "************** 486\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 487\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 488\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 489\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 490\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 491\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 492\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 493\n",
      "T5LayerNorm()\n",
      "************** 494\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 495\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 496\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 497\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 498\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 499\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 500\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 501\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 502\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 503\n",
      "T5LayerNorm()\n",
      "************** 504\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 505\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 506\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 507\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 508\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 509\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 510\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 511\n",
      "T5LayerNorm()\n",
      "************** 512\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 513\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 514\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 515\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 516\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 517\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 518\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 519\n",
      "T5LayerNorm()\n",
      "************** 520\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 521\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 522\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 523\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 524\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 525\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 526\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 527\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 528\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 529\n",
      "T5LayerNorm()\n",
      "************** 530\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 531\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 532\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 533\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 534\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 535\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 536\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 537\n",
      "T5LayerNorm()\n",
      "************** 538\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 539\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 540\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 541\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 542\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 543\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 544\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 545\n",
      "T5LayerNorm()\n",
      "************** 546\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 547\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 548\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 549\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 550\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 551\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 552\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 553\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 554\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 555\n",
      "T5LayerNorm()\n",
      "************** 556\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 557\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 558\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 559\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 560\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 561\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 562\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 563\n",
      "T5LayerNorm()\n",
      "************** 564\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 565\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 566\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 567\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 568\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 569\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 570\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 571\n",
      "T5LayerNorm()\n",
      "************** 572\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 573\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 574\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 575\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 576\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 577\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 578\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 579\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 580\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 581\n",
      "T5LayerNorm()\n",
      "************** 582\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 583\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 584\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 585\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 586\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 587\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 588\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 589\n",
      "T5LayerNorm()\n",
      "************** 590\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 591\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 592\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 593\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 594\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 595\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 596\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 597\n",
      "T5LayerNorm()\n",
      "************** 598\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 599\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 600\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 601\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 602\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 603\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 604\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 605\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 606\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 607\n",
      "T5LayerNorm()\n",
      "************** 608\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 609\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 610\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 611\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 612\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 613\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 614\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 615\n",
      "T5LayerNorm()\n",
      "************** 616\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 617\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 618\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 619\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 620\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 621\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 622\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 623\n",
      "T5LayerNorm()\n",
      "************** 624\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 625\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 626\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 627\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 628\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 629\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 630\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 631\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 632\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 633\n",
      "T5LayerNorm()\n",
      "************** 634\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 635\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 636\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 637\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 638\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 639\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 640\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 641\n",
      "T5LayerNorm()\n",
      "************** 642\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 643\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 644\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 645\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 646\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 647\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 648\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 649\n",
      "T5LayerNorm()\n",
      "************** 650\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 651\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 652\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 653\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 654\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 655\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 656\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 657\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 658\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 659\n",
      "T5LayerNorm()\n",
      "************** 660\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 661\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 662\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 663\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 664\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 665\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 666\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 667\n",
      "T5LayerNorm()\n",
      "************** 668\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 669\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 670\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 671\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 672\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 673\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 674\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 675\n",
      "T5LayerNorm()\n",
      "************** 676\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 677\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 678\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 679\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 680\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 681\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 682\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 683\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 684\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 685\n",
      "T5LayerNorm()\n",
      "************** 686\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 687\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 688\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 689\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 690\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 691\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 692\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 693\n",
      "T5LayerNorm()\n",
      "************** 694\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 695\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 696\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 697\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 698\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 699\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 700\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 701\n",
      "T5LayerNorm()\n",
      "************** 702\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 703\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 704\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 705\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 706\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 707\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 708\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 709\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 710\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 711\n",
      "T5LayerNorm()\n",
      "************** 712\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 713\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 714\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 715\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 716\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 717\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 718\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 719\n",
      "T5LayerNorm()\n",
      "************** 720\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 721\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 722\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 723\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 724\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 725\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 726\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 727\n",
      "T5LayerNorm()\n",
      "************** 728\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 729\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 730\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 731\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 732\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 733\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 734\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 735\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 736\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 737\n",
      "T5LayerNorm()\n",
      "************** 738\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 739\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 740\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 741\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 742\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 743\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 744\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 745\n",
      "T5LayerNorm()\n",
      "************** 746\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 747\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 748\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 749\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 750\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 751\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 752\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 753\n",
      "T5LayerNorm()\n",
      "************** 754\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 755\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 756\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 757\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 758\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 759\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 760\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 761\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 762\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 763\n",
      "T5LayerNorm()\n",
      "************** 764\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 765\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 766\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 767\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 768\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 769\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 770\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 771\n",
      "T5LayerNorm()\n",
      "************** 772\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 773\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 774\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 775\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 776\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 777\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 778\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 779\n",
      "T5LayerNorm()\n",
      "************** 780\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 781\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 782\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 783\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 784\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 785\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 786\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 787\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 788\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 789\n",
      "T5LayerNorm()\n",
      "************** 790\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 791\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 792\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 793\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 794\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 795\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 796\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 797\n",
      "T5LayerNorm()\n",
      "************** 798\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 799\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 800\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 801\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 802\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 803\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 804\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 805\n",
      "T5LayerNorm()\n",
      "************** 806\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 807\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 808\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 809\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 810\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 811\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 812\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 813\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 814\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 815\n",
      "T5LayerNorm()\n",
      "************** 816\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 817\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 818\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 819\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 820\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 821\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 822\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 823\n",
      "T5LayerNorm()\n",
      "************** 824\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 825\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 826\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 827\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 828\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 829\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 830\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 831\n",
      "T5LayerNorm()\n",
      "************** 832\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 833\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 834\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 835\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 836\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 837\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 838\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 839\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 840\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 841\n",
      "T5LayerNorm()\n",
      "************** 842\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 843\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 844\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 845\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 846\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 847\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 848\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 849\n",
      "T5LayerNorm()\n",
      "************** 850\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 851\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 852\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 853\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 854\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 855\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 856\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 857\n",
      "T5LayerNorm()\n",
      "************** 858\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 859\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 860\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 861\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 862\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 863\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 864\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 865\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 866\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 867\n",
      "T5LayerNorm()\n",
      "************** 868\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 869\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 870\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 871\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 872\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 873\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 874\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 875\n",
      "T5LayerNorm()\n",
      "************** 876\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 877\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 878\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 879\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 880\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 881\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 882\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 883\n",
      "T5LayerNorm()\n",
      "************** 884\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 885\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 886\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 887\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 888\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 889\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 890\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 891\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 892\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 893\n",
      "T5LayerNorm()\n",
      "************** 894\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 895\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 896\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 897\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 898\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 899\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 900\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 901\n",
      "T5LayerNorm()\n",
      "************** 902\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 903\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 904\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 905\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 906\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 907\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 908\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 909\n",
      "T5LayerNorm()\n",
      "************** 910\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 911\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 912\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 913\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 914\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 915\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 916\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 917\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 918\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 919\n",
      "T5LayerNorm()\n",
      "************** 920\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 921\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 922\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 923\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 924\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 925\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 926\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 927\n",
      "T5LayerNorm()\n",
      "************** 928\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 929\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 930\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 931\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 932\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 933\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 934\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 935\n",
      "T5LayerNorm()\n",
      "************** 936\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 937\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 938\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 939\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 940\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 941\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 942\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 943\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 944\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 945\n",
      "T5LayerNorm()\n",
      "************** 946\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 947\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 948\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 949\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 950\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 951\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 952\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 953\n",
      "T5LayerNorm()\n",
      "************** 954\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 955\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 956\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 957\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 958\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 959\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 960\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 961\n",
      "T5LayerNorm()\n",
      "************** 962\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 963\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 964\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 965\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 966\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 967\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 968\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 969\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 970\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 971\n",
      "T5LayerNorm()\n",
      "************** 972\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 973\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 974\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 975\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 976\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 977\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 978\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 979\n",
      "T5LayerNorm()\n",
      "************** 980\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 981\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 982\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 983\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 984\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 985\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 986\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 987\n",
      "T5LayerNorm()\n",
      "************** 988\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 989\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 990\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 991\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 992\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 993\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 994\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 995\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 996\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 997\n",
      "T5LayerNorm()\n",
      "************** 998\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 999\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1000\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1001\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1002\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1003\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1004\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1005\n",
      "T5LayerNorm()\n",
      "************** 1006\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1007\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1008\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1009\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1010\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1011\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1012\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1013\n",
      "T5LayerNorm()\n",
      "************** 1014\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1015\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1016\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1017\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1018\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1019\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1020\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1021\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1022\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1023\n",
      "T5LayerNorm()\n",
      "************** 1024\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1025\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1026\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1027\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1028\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1029\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1030\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1031\n",
      "T5LayerNorm()\n",
      "************** 1032\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1033\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1034\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1035\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1036\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1037\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1038\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1039\n",
      "T5LayerNorm()\n",
      "************** 1040\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1041\n",
      "T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedGeluDense(\n",
      "        (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "        (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (gelu_act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "************** 1042\n",
      "ModuleList(\n",
      "  (0): T5LayerSelfAttention(\n",
      "    (SelfAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): T5LayerCrossAttention(\n",
      "    (EncDecAttention): T5Attention(\n",
      "      (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): T5LayerFF(\n",
      "    (DenseReluDense): T5DenseGatedGeluDense(\n",
      "      (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "      (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (gelu_act): NewGELUActivation()\n",
      "    )\n",
      "    (layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "************** 1043\n",
      "T5LayerSelfAttention(\n",
      "  (SelfAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1044\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1045\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1046\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1047\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1048\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1049\n",
      "T5LayerNorm()\n",
      "************** 1050\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1051\n",
      "T5LayerCrossAttention(\n",
      "  (EncDecAttention): T5Attention(\n",
      "    (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1052\n",
      "T5Attention(\n",
      "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "************** 1053\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1054\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1055\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1056\n",
      "Linear(in_features=2048, out_features=2048, bias=False)\n",
      "************** 1057\n",
      "T5LayerNorm()\n",
      "************** 1058\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1059\n",
      "T5LayerFF(\n",
      "  (DenseReluDense): T5DenseGatedGeluDense(\n",
      "    (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "    (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gelu_act): NewGELUActivation()\n",
      "  )\n",
      "  (layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "************** 1060\n",
      "T5DenseGatedGeluDense(\n",
      "  (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu_act): NewGELUActivation()\n",
      ")\n",
      "************** 1061\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1062\n",
      "Linear(in_features=2048, out_features=5120, bias=False)\n",
      "************** 1063\n",
      "Linear(in_features=5120, out_features=2048, bias=False)\n",
      "************** 1064\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1065\n",
      "T5LayerNorm()\n",
      "************** 1066\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1067\n",
      "T5LayerNorm()\n",
      "************** 1068\n",
      "Dropout(p=0.1, inplace=False)\n",
      "************** 1069\n",
      "Linear(in_features=2048, out_features=38136, bias=False)\n",
      "************** 1070\n"
     ]
    }
   ],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (relative_attention_bias): Embedding(32, 32)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
