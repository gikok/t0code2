{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tz/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from lib2to3.pgen2 import token\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Optional, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.linear import Linear\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from train import DataCollatorForMultipleChoice\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    default_data_collator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"dataset_name\"] = \"miniprompts002.parquet.gzip\"\n",
    "args[\"eval_name\"] = \"miniprompts002_eval.parquet.gzip\"\n",
    "args[\"model_name_or_path\"] = \"bigscience/T0_3B\"\n",
    "args[\"output_dir\"] = \"/home/gikok/output\"\n",
    "args[\"num_train_epochs\"] = 1\n",
    "args[\"per_device_train_batch_size\"] = 16\n",
    "args[\"per_device_eval_batch_size\"] = 16\n",
    "args[\"freeze_encoder\"] = True\n",
    "args[\"learning_rate\"] = 1e30\n",
    "args[\"parallelize\"] = False\n",
    "args[\"seed\"] = 42\n",
    "args[\"pad_to_max_length\"] = False\n",
    "args[\"input_eos\"] = False\n",
    "args[\"target_max_length\"] = 256\n",
    "args[\"max_length\"] = 512\n",
    "args[\"num_warmup_steps\"] = 0\n",
    "args[\"debug\"] = False\n",
    "args[\"lr_scheduler_type\"] = \"linear\"\n",
    "args[\"num_shots\"] = None\n",
    "args[\"weight_decay\"] = 0.0\n",
    "args[\"gradient_checkpoint\"] = False\n",
    "args[\"gradient_accumulation_steps\"] = 1\n",
    "args[\"max_train_steps\"] = None\n",
    "\n",
    "\n",
    "def resample(model, layer, n_new):\n",
    "\n",
    "    new_tensor = list(model.named_parameters())[layer][1].detach().cpu()\n",
    "\n",
    "    for i in range(2048):\n",
    "        val, bin = np.histogram(new_tensor[:-n_new, i], 10000)\n",
    "        pdf = val / sum(val)\n",
    "        cdf = np.cumsum(pdf)\n",
    "        b = (bin[1:] + bin[:-1]) / 2\n",
    "        new_tensor[-n_new:, i] = torch.tensor(\n",
    "            np.interp(np.random.random(n_new), cdf, b)\n",
    "        )\n",
    "    data = list(model.named_parameters())[layer][1].data\n",
    "    data[:, :] = new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/18/2022 16:53:20 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "10/18/2022 16:53:20 - WARNING - datasets.builder - Using custom data configuration data-11314387c41b991e\n",
      "10/18/2022 16:53:20 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-11314387c41b991e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "10/18/2022 16:53:20 - WARNING - datasets.builder - Using custom data configuration data-11314387c41b991e\n",
      "10/18/2022 16:53:20 - WARNING - datasets.builder - Reusing dataset parquet (/home/gikok/.cache/huggingface/datasets/parquet/data-11314387c41b991e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /home/gikok/.cache/huggingface/transformers/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /home/gikok/.cache/huggingface/transformers/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /home/gikok/.cache/huggingface/transformers/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /home/gikok/.cache/huggingface/transformers/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /home/gikok/.cache/huggingface/transformers/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "10/18/2022 16:54:03 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/gikok/.cache/huggingface/datasets/parquet/data-11314387c41b991e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b1b3f56a8b163228.arrow\n",
      "10/18/2022 16:54:03 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/gikok/.cache/huggingface/datasets/parquet/data-11314387c41b991e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6d0bd5c8f8782ee9.arrow\n",
      "/opt/conda/envs/tz/lib/python3.7/site-packages/transformers/optimization.py:350: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "10/18/2022 16:54:04 - INFO - __main__ - ***** Running training *****\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Num training = 25600\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Num Epochs = 1\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "10/18/2022 16:54:04 - INFO - __main__ -   Total optimization steps = 1600\n",
      "  0%|          | 0/1600 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "set_seed(args[\"seed\"])\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# Handle the output directory creation\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(args[\"output_dir\"], exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args[\"dataset_name\"] is not None:\n",
    "    data_files = {\"train\": args[\"dataset_name\"], \"test\": args[\"eval_name\"]}\n",
    "    raw_train_dataset = load_dataset(\"data\", data_files=data_files, split=\"train\")\n",
    "    raw_eval_dataset = load_dataset(\"data\", data_files=data_files, split=\"test\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Please specify `args['dataset_name`.\")\n",
    "\n",
    "# Trim a number of evaluation training\n",
    "if args[\"debug\"]:\n",
    "    raw_train_dataset = raw_train_dataset.select(\n",
    "        range(min(100, len(raw_train_dataset)))\n",
    "    )\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(args[\"model_name_or_path\"]).to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name_or_path\"])\n",
    "\n",
    "# get all item_no and add as tokens\n",
    "items = pd.read_parquet(\"data/item_no_100.parquet.gzip\")[\"item_no\"].values.tolist()\n",
    "# tokenizer.add_tokens(items)\n",
    "\n",
    "# # then resize embeddings\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # resample shared embedding and lm_head layer\n",
    "# resample(model, 0, len(items))\n",
    "# resample(model, -1, len(items))\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if args[\"pad_to_max_length\"] else False\n",
    "\n",
    "\n",
    "def tokenize_train(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=args[\"input_eos\"],\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tokenized_targets = tokenizer(\n",
    "            target_texts,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        model_inputs[\"labels\"] = [\n",
    "            [(t if t != tokenizer.pad_token_id else -100) for t in targets]\n",
    "            for targets in tokenized_targets[\"input_ids\"]\n",
    "        ]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "column_names = raw_eval_dataset.column_names\n",
    "\n",
    "\n",
    "def preprocess_eval(examples):\n",
    "    input_texts = examples[\"input\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "    answer_choices_texts = examples[\"options\"]\n",
    "    bs = len(examples[column_names[0]])\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=padding,\n",
    "        max_length=args[\"max_length\"],\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    tokenized_targets = [\n",
    "        tokenizer(\n",
    "            ans_choi,\n",
    "            padding=padding,\n",
    "            max_length=args[\"target_max_length\"],\n",
    "            truncation=True,\n",
    "        )\n",
    "        for ans_choi in answer_choices_texts\n",
    "    ]\n",
    "\n",
    "    features = {\n",
    "        k: [\n",
    "            [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "            for idx, elem in enumerate(v)\n",
    "        ]\n",
    "        for k, v in tokenized_inputs.items()\n",
    "    }\n",
    "\n",
    "    features[\"labels\"] = [tokenized_targets[idx][\"input_ids\"] for idx in range(bs)]\n",
    "    features[\"labels_attention_mask\"] = [\n",
    "        tokenized_targets[idx][\"attention_mask\"] for idx in range(bs)\n",
    "    ]\n",
    "    features[\"targets\"] = [\n",
    "        answer_choices_texts[idx].index(t) for idx, t in enumerate(target_texts)\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = raw_eval_dataset.map(\n",
    "        preprocess_eval, batched=True, remove_columns=column_names\n",
    "    )\n",
    "\n",
    "    if args[\"num_shots\"] is not None:\n",
    "        sample_indices = random.sample(\n",
    "            range(0, len(raw_train_dataset)), k=args[\"num_shots\"]\n",
    "        )\n",
    "        raw_train_dataset = raw_train_dataset.select(sample_indices)\n",
    "    train_dataset = raw_train_dataset.map(tokenize_train, batched=True)\n",
    "    train_dataset.set_format(\n",
    "        type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Log a few random training:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.debug(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "# for index in random.sample(range(len(eval_dataset)), 3):\n",
    "#     logger.debug(f\"Sample {index} of the evaluation set: {eval_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collator,\n",
    "    batch_size=args[\"per_device_train_batch_size\"],\n",
    ")\n",
    "\n",
    "if args[\"pad_to_max_length\"]:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    eval_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    eval_collator = DataCollatorForMultipleChoice(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=eval_collator,\n",
    "    batch_size=args[\"per_device_eval_batch_size\"],\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args[\"weight_decay\"],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args[\"learning_rate\"])\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "if args[\"max_train_steps\"] is None:\n",
    "    args[\"max_train_steps\"] = args[\"num_train_epochs\"] * num_update_steps_per_epoch\n",
    "else:\n",
    "    args[\"num_train_epochs\"] = math.ceil(\n",
    "        args[\"max_train_steps\"] / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args[\"lr_scheduler_type\"],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args[\"num_warmup_steps\"],\n",
    "    num_training_steps=args[\"max_train_steps\"],\n",
    ")\n",
    "\n",
    "if args[\"parallelize\"]:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    assert num_gpus > 1, \"You need at least 2 GPUs to use `model.parallelize()`.\"\n",
    "    model.parallelize()\n",
    "    optimizer, train_dataloader = accelerator.prepare(optimizer, train_dataloader)\n",
    "else:\n",
    "    model, optimizer, train_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "total_batch_size = (\n",
    "    args[\"per_device_train_batch_size\"]\n",
    "    * accelerator.num_processes\n",
    "    * args[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num training = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args['num_train_epochs']}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {args['per_device_train_batch_size']}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    ")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args['gradient_accumulation_steps']}\")\n",
    "logger.info(f\"  Total optimization steps = {args['max_train_steps']}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(args[\"max_train_steps\"]), disable=not accelerator.is_local_main_process\n",
    ")\n",
    "global_steps = 0\n",
    "\n",
    "# how often trained model should be saved\n",
    "r = int(args[\"max_train_steps\"] / 30)\n",
    "if args[\"gradient_checkpoint\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "model_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_grad = []\n",
    "em_delta_pct = []\n",
    "lm_grad = []\n",
    "lm_delta_pct = []\n",
    "em = list(model.named_parameters())[0][1]\n",
    "lm = list(model.named_parameters())[-1][1]\n",
    "em_old = em * 1\n",
    "lm_old = lm * 1\n",
    "init_len = len(tokenizer) - len(items)\n",
    "def hook(grad):\n",
    "    grad_mask[:init_len, :] = 0\n",
    "    return grad*grad_mask\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        grad_mask = torch.ones_like(param)\n",
    "        grad_mask[:init_len, :] = 0\n",
    "        h = param.register_hook(hook)\n",
    "for epoch in range(1, 2):#args[\"num_train_epochs\"] + 1):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        if step % 1 == 0 or step == len(train_dataloader) - 1:\n",
    "            item_no = int(step/4)\n",
    "            ind = len(tokenizer) - 100 + item_no\n",
    "            em_grad += [\n",
    "                ((em.grad[ind, :]).mean()).cpu()*1\n",
    "            ]\n",
    "            lm_grad += [\n",
    "                ((lm.grad[ind, :]).mean()).cpu()*1\n",
    "            ]\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            loss = loss.item()\n",
    "            em = list(model.named_parameters())[0][1]\n",
    "            lm = list(model.named_parameters())[-1][1]\n",
    "            em_delta_pct += [\n",
    "                (((em[ind, :] - em_old[ind, :]) / em[ind, :]).mean()).cpu()\n",
    "            ]\n",
    "            lm_delta_pct += [\n",
    "                (((lm[ind, :] - lm_old[ind, :]) / lm[ind, :]).mean()).cpu()\n",
    "            ]\n",
    "            em_old = em * 1\n",
    "            lm_old = lm * 1\n",
    "            if accelerator.is_main_process:\n",
    "                tqdm.write(f\"epoch = {epoch}, step = {global_steps}, loss = {loss}\")\n",
    "        if step >= 25:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"encoder\") or name.startswith(\"decoder\"):\n",
    "        param.requires_grad = False\n",
    "    if name.startswith(\"shared\") or name.startswith(\"lm_head\"):\n",
    "        t = torch.zeros(param.shape).to(\"cuda:0\")\n",
    "        t[-len(items) :, :] = 1\n",
    "        param.register_hook(lambda grad: grad * t)\n",
    "outputs = model(**batch)\n",
    "loss = outputs.loss\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "lr_scheduler.step()\n",
    "optimizer.zero_grad()\n",
    "progress_bar.update(1)\n",
    "global_steps += 1\n",
    "loss = loss.item()\n",
    "if accelerator.is_main_process:\n",
    "    tqdm.write(f\"epoch = {1}, step = {global_steps}, loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last(model.children())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values[0][1].grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(np.zeros((new_values[0][1].shape[0], new_values[0][1].shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[-len(items) :, :] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelCheckpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21023/70247625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'my/path/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    l += 1\n",
    "    if isinstance(mod, Embedding):\n",
    "        if mod.num_embeddings == 38136:\n",
    "            print(l, mod)\n",
    "            print(type(mod))\n",
    "            print(mod.num_embeddings)\n",
    "            print(\"****************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for mod in model.modules():\n",
    "    print(mod)\n",
    "    print(\"**************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dca363fb86486d0b8c2e90488082decabafb37e80f3496c319a1970ca82f2e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
